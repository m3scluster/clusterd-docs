<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Apache Mesos</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Fundamentals</li><li class="chapter-item expanded "><a href="architecture.html"><strong aria-hidden="true">1.</strong> Mesos Architecture providing an overview of Mesos concepts</a></li><li class="chapter-item expanded "><a href="presentations.html"><strong aria-hidden="true">2.</strong> Video and Slides of Mesos Presentations</a></li><li class="chapter-item expanded "><a href="versioning.html"><strong aria-hidden="true">3.</strong> Mesos Release and Support Policy</a></li><li class="chapter-item expanded affix "><li class="part-title">Build / Installation</li><li class="chapter-item expanded "><a href="building.html"><strong aria-hidden="true">4.</strong> Building for basic instructions on compiling and installing Mesos.</a></li><li class="chapter-item expanded "><a href="binary-packages.html"><strong aria-hidden="true">5.</strong> Binary Packages for how to use Mesos binary packages.</a></li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">6.</strong> Configuration for build configuration options.</a></li><li class="chapter-item expanded "><a href="cmake.html"><strong aria-hidden="true">7.</strong> CMake for details about using the new CMake build system.</a></li><li class="chapter-item expanded "><a href="windows.html"><strong aria-hidden="true">8.</strong> Windows Support for the state of Windows support in Mesos.</a></li><li class="chapter-item expanded affix "><li class="part-title">Administration</li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">9.</strong> Configuration for command-line arguments.</a></li><li class="chapter-item expanded "><a href="high-availability.html"><strong aria-hidden="true">10.</strong> High Availability Master Setup</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="replicated-log-internals.html"><strong aria-hidden="true">10.1.</strong> Replicated Log for information on the Mesos replicated log.</a></li></ol></li><li class="chapter-item expanded "><a href="agent-recovery.html"><strong aria-hidden="true">11.</strong> Fault Tolerant Agent Setup</a></li><li class="chapter-item expanded "><a href="framework-rate-limiting.html"><strong aria-hidden="true">12.</strong> Framework Rate Limiting</a></li><li class="chapter-item expanded "><a href="maintenance.html"><strong aria-hidden="true">13.</strong> Maintenance for performing maintenance on a Mesos cluster.</a></li><li class="chapter-item expanded "><a href="upgrades.html"><strong aria-hidden="true">14.</strong> Upgrades for upgrading a Mesos cluster.</a></li><li class="chapter-item expanded "><a href="downgrades.html"><strong aria-hidden="true">15.</strong> Downgrades for downgrading a Mesos cluster.</a></li><li class="chapter-item expanded "><a href="logging.html"><strong aria-hidden="true">16.</strong> Logging</a></li><li class="chapter-item expanded "><a href="monitoring.html"><strong aria-hidden="true">17.</strong> Monitoring / Metrics</a></li><li class="chapter-item expanded "><a href="cli.html"><strong aria-hidden="true">18.</strong> Debugging using the new CLI</a></li><li class="chapter-item expanded "><a href="operational-guide.html"><strong aria-hidden="true">19.</strong> Operational Guide</a></li><li class="chapter-item expanded "><a href="fetcher.html"><strong aria-hidden="true">20.</strong> Fetcher Cache Configuration</a></li><li class="chapter-item expanded "><a href="fault-domains.html"><strong aria-hidden="true">21.</strong> Fault Domains</a></li><li class="chapter-item expanded "><a href="performance-profiling.html"><strong aria-hidden="true">22.</strong> Performance Profiling for debugging performance issues in Mesos.</a></li><li class="chapter-item expanded "><a href="memory-profiling.html"><strong aria-hidden="true">23.</strong> Memory Profiling for debugging potential memory leaks in Mesos.</a></li><li class="chapter-item expanded affix "><li class="part-title">Resource Management</li><li class="chapter-item expanded "><a href="attributes-resources.html"><strong aria-hidden="true">24.</strong> Attributes and Resources for how to describe the agents that comprise a cluster.</a></li><li class="chapter-item expanded "><a href="roles.html"><strong aria-hidden="true">25.</strong> Using Resource Roles</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="weights.html"><strong aria-hidden="true">25.1.</strong> Resource Role Weights for fair sharing.</a></li><li class="chapter-item expanded "><a href="quota.html"><strong aria-hidden="true">25.2.</strong> Resource Role Quota for how to configure Mesos to provide guaranteed resource allocations for use by a role.</a></li><li class="chapter-item expanded "><a href="reservation.html"><strong aria-hidden="true">25.3.</strong> Reservations for how operators and frameworks can reserve resources on individual agents for use by a role.</a></li><li class="chapter-item expanded "><a href="shared-resources.html"><strong aria-hidden="true">25.4.</strong> Shared Resources for how to share persistent volumes between tasks managed by different executors on the same agent.</a></li></ol></li><li class="chapter-item expanded "><a href="oversubscription.html"><strong aria-hidden="true">26.</strong> Oversubscription for how to configure Mesos to take advantage of unused resources to launch “best-effort” tasks.</a></li><li class="chapter-item expanded affix "><li class="part-title">Security</li><li class="chapter-item expanded "><a href="authentication.html"><strong aria-hidden="true">27.</strong> Authentication</a></li><li class="chapter-item expanded "><a href="authorization.html"><strong aria-hidden="true">28.</strong> Authorization</a></li><li class="chapter-item expanded "><a href="ssl.html"><strong aria-hidden="true">29.</strong> SSL</a></li><li class="chapter-item expanded "><a href="secrets.html"><strong aria-hidden="true">30.</strong> Secrets for managing secrets within Mesos.</a></li><li class="chapter-item expanded affix "><li class="part-title">Containerization</li><li class="chapter-item expanded "><a href="containerizers.html"><strong aria-hidden="true">31.</strong> Containerizer Overview</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="containerizer-internals.html"><strong aria-hidden="true">31.1.</strong> Containerizer Internals for implementation details of containerizers.</a></li><li class="chapter-item expanded "><a href="docker-containerizer.html"><strong aria-hidden="true">31.2.</strong> Docker Containerizer for launching a Docker image as a Task, or as an Executor.</a></li><li class="chapter-item expanded "><a href="mesos-containerizer.html"><strong aria-hidden="true">31.3.</strong> Mesos Containerizer default containerizer, supports both Linux and POSIX systems.</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="container-image.html"><strong aria-hidden="true">31.3.1.</strong> Container Images for supporting container images in Mesos containerizer.</a></li><li class="chapter-item expanded "><a href="isolators/docker-volume.html"><strong aria-hidden="true">31.3.2.</strong> Docker Volume Support</a></li><li class="chapter-item expanded "><a href="gpu-support.html"><strong aria-hidden="true">31.3.3.</strong> Nvidia GPU Support for how to run Mesos with Nvidia GPU support.</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="sandbox.html"><strong aria-hidden="true">32.</strong> Container Sandboxes</a></li><li class="chapter-item expanded "><a href="container-volume.html"><strong aria-hidden="true">33.</strong> Container Volumes</a></li><li class="chapter-item expanded "><a href="nested-container-and-task-group.html"><strong aria-hidden="true">34.</strong> Nested Container and Task Group (Pod)</a></li><li class="chapter-item expanded "><a href="standalone-containers.html"><strong aria-hidden="true">35.</strong> Standalone Containers</a></li><li class="chapter-item expanded affix "><li class="part-title">Networking</li><li class="chapter-item expanded "><a href="networking.html"><strong aria-hidden="true">36.</strong> Networking Overview</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="networking-for-mesos-managed-containers.html"><strong aria-hidden="true">36.1.</strong> Networking in Detail</a></li><li class="chapter-item expanded "><a href="cni.html"><strong aria-hidden="true">36.2.</strong> Container Network Interface (CNI)</a></li><li class="chapter-item expanded "><a href="isolators/network-port-mapping.html"><strong aria-hidden="true">36.3.</strong> Port Mapping Isolator</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Storage</li><li class="chapter-item expanded "><a href="multiple-disk.html"><strong aria-hidden="true">37.</strong> Multiple Disks for how to allow tasks to use multiple isolated disk resources.</a></li><li class="chapter-item expanded "><a href="persistent-volume.html"><strong aria-hidden="true">38.</strong> Persistent Volume for how to allow tasks to access persistent storage resources.</a></li><li class="chapter-item expanded "><a href="csi.html"><strong aria-hidden="true">39.</strong> Container Storage Interface (CSI) Support</a></li><li class="chapter-item expanded affix "><li class="part-title">Scheduler and Executor Development</li><li class="chapter-item expanded "><a href="running-workloads.html"><strong aria-hidden="true">40.</strong> Running Workloads in Mesos explains how a scheduler can specify and run tasks.</a></li><li class="chapter-item expanded "><a href="app-framework-development-guide.html"><strong aria-hidden="true">41.</strong> Framework Development Guide describes how to build applications on top of Mesos.</a></li><li class="chapter-item expanded "><a href="high-availability-framework-guide.html"><strong aria-hidden="true">42.</strong> Guide for Designing Highly Available Mesos Frameworks</a></li><li class="chapter-item expanded "><a href="reconciliation.html"><strong aria-hidden="true">43.</strong> Reconciliation for ensuring a framework’s state remains eventually consistent in the face of failures.</a></li><li class="chapter-item expanded "><a href="task-state-reasons.html"><strong aria-hidden="true">44.</strong> Task State Reasons describes how task state reasons are used in Mesos.</a></li><li class="chapter-item expanded "><a href="health-checks.html"><strong aria-hidden="true">45.</strong> Task Health Checking</a></li><li class="chapter-item expanded "><a href="scheduler-http-api.html"><strong aria-hidden="true">46.</strong> v1 Scheduler HTTP API for communication between schedulers and the Mesos master.</a></li><li class="chapter-item expanded "><a href="executor-http-api.html"><strong aria-hidden="true">47.</strong> v1 Executor HTTP API describes the new HTTP API for communication between executors and the Mesos agent.</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Apache Mesos</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/AVENTER-UG/mesos-docs/" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="mesos-architecture"><a class="header" href="#mesos-architecture">Mesos Architecture</a></h1>
<p><img src="images/architecture3.jpg" alt="Mesos Architecture" /></p>
<p>The above figure shows the main components of Mesos.  Mesos consists of a <em>master</em> daemon that manages <em>agent</em> daemons running on each cluster node, and <em>Mesos frameworks</em> that run <em>tasks</em> on these agents.</p>
<p>The master enables fine-grained sharing of resources (CPU, RAM, ...) across
frameworks by making them <em>resource offers</em>. Each resource offer contains a list
of <code>&lt;agent ID, resource1: amount1, resource2: amount2, ...&gt;</code> (NOTE: as
keyword 'slave' is deprecated in favor of 'agent', driver-based frameworks will
still receive offers with slave ID, whereas frameworks using the v1 HTTP API receive offers with agent ID). The master decides <em>how many</em> resources to offer to each framework according to a given organizational policy, such as fair sharing or strict priority. To support a diverse set of policies, the master employs a modular architecture that makes it easy to add new allocation modules via a plugin mechanism.</p>
<p>A framework running on top of Mesos consists of two components: a <em>scheduler</em> that registers with the master to be offered resources, and an <em>executor</em> process that is launched on agent nodes to run the framework's tasks (see the <a href="app-framework-development-guide.html">App/Framework development guide</a> for more details about framework schedulers and executors). While the master determines <strong>how many</strong> resources are offered to each framework, the frameworks' schedulers select <strong>which</strong> of the offered resources to use. When a framework accepts offered resources, it passes to Mesos a description of the tasks it wants to run on them. In turn, Mesos launches the tasks on the corresponding agents.</p>
<h2 id="example-of-resource-offer"><a class="header" href="#example-of-resource-offer">Example of resource offer</a></h2>
<p>The figure below shows an example of how a framework gets scheduled to run a task.</p>
<p><img src="images/architecture-example.jpg" alt="Mesos Architecture" /></p>
<p>Let's walk through the events in the figure.</p>
<ol>
<li>Agent 1 reports to the master that it has 4 CPUs and 4 GB of memory free. The master then invokes the allocation policy module, which tells it that framework 1 should be offered all available resources.</li>
<li>The master sends a resource offer describing what is available on agent 1 to framework 1.</li>
<li>The framework's scheduler replies to the master with information about two tasks to run on the agent, using &lt;2 CPUs, 1 GB RAM&gt; for the first task, and &lt;1 CPUs, 2 GB RAM&gt; for the second task.</li>
<li>Finally, the master sends the tasks to the agent, which allocates appropriate resources to the framework's executor, which in turn launches the two tasks (depicted with dotted-line borders in the figure). Because 1 CPU and 1 GB of RAM are still unallocated, the allocation module may now offer them to framework 2.</li>
</ol>
<p>In addition, this resource offer process repeats when tasks finish and new resources become free.</p>
<p>While the thin interface provided by Mesos allows it to scale and allows the frameworks to evolve independently, one question remains: how can the constraints of a framework be satisfied without Mesos knowing about these constraints? For example, how can a framework achieve data locality without Mesos knowing which nodes store the data required by the framework? Mesos answers these questions by simply giving frameworks the ability to <strong>reject</strong> offers. A framework will reject the offers that do not satisfy its constraints and accept the ones that do.  In particular, we have found that a simple policy called delay scheduling, in which frameworks wait for a limited time to acquire nodes storing the input data, yields nearly optimal data locality.</p>
<p>You can also read much more about the Mesos architecture in this <a href="https://www.usenix.org/conference/nsdi11/mesos-platform-fine-grained-resource-sharing-data-center">technical paper</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="video-and-slides-of-mesos-presentations"><a class="header" href="#video-and-slides-of-mesos-presentations">Video and Slides of Mesos Presentations</a></h1>
<p><em>(Listed in reverse chronological order)</em></p>
<h2 id="mesoscon-north-america-2018"><a class="header" href="#mesoscon-north-america-2018">MesosCon North America 2018</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PL-cRvJ6sAbfjvQCLT3ktrpnVwrJKTodfm">Video playlist</a> + <a href="https://mesoscon18.sched.com/">Slides</a></p>
<h2 id="jolt-running-distributed-fault-tolerant-tests-at-scale-using-mesos"><a class="header" href="#jolt-running-distributed-fault-tolerant-tests-at-scale-using-mesos">Jolt: Running Distributed, Fault-Tolerant Tests at Scale using Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=2uGwlVs8Cpw">Video</a>
Sunil Shah, Kyle Kelly, and Timmy Zhu Presented November 1, 2017 at <a href="https://www.meetup.com/Bay-Area-Mesos-User-Group/events/244469969/">Bay Area Mesos User Group Meetup</a></p>
<h2 id="mesoscon-europe-2017"><a class="header" href="#mesoscon-europe-2017">MesosCon Europe 2017</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6rSBqPhTh_lmeMmxn6AOSjf">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2017/mesoscon-europe/program/slides">Slides</a></p>
<h2 id="mesoscon-north-america-2017"><a class="header" href="#mesoscon-north-america-2017">MesosCon North America 2017</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6qAEnkhkh5tGI6oX_xXD3X4">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2017/mesoscon-north-america/program/slides">Slides</a></p>
<h2 id="mesoscon-asia-2017"><a class="header" href="#mesoscon-asia-2017">MesosCon Asia 2017</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6rZfzCL_b-W9yxcJQhZ0RUg">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2017/mesoscon-asia/program/slides">Slides</a></p>
<h2 id="mesoscon-asia-2016"><a class="header" href="#mesoscon-asia-2016">MesosCon Asia 2016</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLbzoR-pLrL6pLSHrXSg7IYgzSlkOh132K">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2016/mesoscon-asia/program/slides">Slides</a></p>
<h2 id="mesoscon-europe-2016"><a class="header" href="#mesoscon-europe-2016">MesosCon Europe 2016</a></h2>
<p><a href="http://events17.linuxfoundation.org/events/archive/2016/mesoscon-europe/program/slides">Slides</a></p>
<h2 id="mesoscon-north-america-2016"><a class="header" href="#mesoscon-north-america-2016">MesosCon North America 2016</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLGeM09tlguZQVL7ZsfNMffX9h1rGNVqnC">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2016/mesoscon-north-america/program/slides">Slides</a></p>
<h2 id="mesoscon-europe-2015"><a class="header" href="#mesoscon-europe-2015">MesosCon Europe 2015</a></h2>
<p><a href="https://www.youtube.com/watch?v=K-x7yOy8Ymk&amp;list=PLGeM09tlguZS6MhlSZDbf-gANWdKgje0I">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2015/mesoscon-europe/program/slides">Slides</a></p>
<h2 id="mesoscon-north-america-2015"><a class="header" href="#mesoscon-north-america-2015">MesosCon North America 2015</a></h2>
<p><a href="https://www.youtube.com/watch?v=aV6pdWveN7s&amp;list=PLVjgeV_avap2arug3vIz8c6l72rvh9poV">Video playlist</a> + <a href="http://events17.linuxfoundation.org/events/archive/2015/mesoscon-north-america/program/slides">Slides</a></p>
<h2 id="building-and-deploying-applications-to-apache-mesos"><a class="header" href="#building-and-deploying-applications-to-apache-mesos">Building and Deploying Applications to Apache Mesos</a></h2>
<p><a href="https://www.slideshare.net/charmalloc/buildingdeployingapplicationsmesos">Slides</a>
Joe Stein
Presented February 26, 2015 at <a href="http://www.meetup.com/DigitalOcean_Community/events/220580767/">DigitalOcean Community Meetup</a></p>
<h2 id="mesoscon-2014"><a class="header" href="#mesoscon-2014">MesosCon 2014</a></h2>
<p><a href="https://www.youtube.com/playlist?list=PLDVc2EaAVPg9kp8cFzjR1Yxj96I4U5EGN">Video playlist</a></p>
<h2 id="datacenter-computing-with-apache-mesos"><a class="header" href="#datacenter-computing-with-apache-mesos">Datacenter Computing with Apache Mesos</a></h2>
<p><a href="http://www.slideshare.net/pacoid/datacenter-computing-with-apache-mesos">Slides</a>
Paco Nathan
Presented April 15, 2014 at <a href="http://www.meetup.com/bigdatadc/events/172610652/">Big Data DC Meetup</a></p>
<h2 id="apache-spark-at-viadeo-running-on-mesos"><a class="header" href="#apache-spark-at-viadeo-running-on-mesos">Apache Spark at Viadeo (Running on Mesos)</a></h2>
<p><a href="http://www.youtube.com/watch?v=shaZslr49vQ&amp;t=16m55s">Video</a> + <a href="https://speakerdeck.com/ecepoi/apache-spark-at-viadeo">Slides</a>
Eugen Cepoi
Presented April 9, 2014 at Paris Hadoop User Group</p>
<h2 id="mesos-hubspot-and-singularity"><a class="header" href="#mesos-hubspot-and-singularity">Mesos, HubSpot, and Singularity</a></h2>
<p><a href="https://www.youtube.com/watch?v=ROn14csiikw">Video</a>
Tom Petr
Presented April 3rd, 2014 at @TwitterOSS #conf</p>
<h2 id="building-distributed-frameworks-on-mesos"><a class="header" href="#building-distributed-frameworks-on-mesos">Building Distributed Frameworks on Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=n5GT7OFSh58">Video</a>
Benjamin Hindman
Presented March 25th, 2014 at <a href="https://www.eventbrite.com/e/aurora-and-mesosframeworksmeetup-tickets-10850994617">Aurora and Mesos Frameworks Meetup</a></p>
<h2 id="introduction-to-apache-aurora"><a class="header" href="#introduction-to-apache-aurora">Introduction to Apache Aurora</a></h2>
<p><a href="https://www.youtube.com/watch?v=asd_h6VzaJc">Video</a>
Bill Farner
Presented March 25th, 2014 at <a href="https://www.eventbrite.com/e/aurora-and-mesosframeworksmeetup-tickets-10850994617">Aurora and Mesos Frameworks Meetup</a></p>
<h2 id="improving-resource-efficiency-with-apache-mesos"><a class="header" href="#improving-resource-efficiency-with-apache-mesos">Improving Resource Efficiency with Apache Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=YpmElyi94AA">Video</a>
Christina Delimitrou
Presented April 3rd, 2014 at @TwitterOSS #conf</p>
<h2 id="apache-mesos-as-an-sdk-for-building-distributed-frameworks"><a class="header" href="#apache-mesos-as-an-sdk-for-building-distributed-frameworks">Apache Mesos as an SDK for Building Distributed Frameworks</a></h2>
<p><a href="http://www.slideshare.net/pacoid/strata-sc-2014-apache-mesos-as-an-sdk-for-building-distributed-frameworks">Slides</a>
Paco Nathan
Presented February 13th, 2014 at <a href="http://strataconf.com/">Strata</a></p>
<h2 id="run-your-data-center-like-googles-with-apache-mesos"><a class="header" href="#run-your-data-center-like-googles-with-apache-mesos">Run your Data Center like Google's with Apache Mesos</a></h2>
<p><a href="https://www.youtube.com/watch?v=2YWVGMuMTrg">Video and Demo</a>
Abhishek Parolkar
Presented November 14th, 2013 at <a href="http://www.cloudexpoasia.com/">Cloud Expo Asia 2013</a></p>
<h2 id="datacenter-management-with-mesos"><a class="header" href="#datacenter-management-with-mesos">Datacenter Management with Mesos</a></h2>
<p><a href="http://www.youtube.com/watch?v=YB1VW0LKzJ4">Video</a>
Benjamin Hindman
Presented August 29th, 2013 at <a href="http://ampcamp.berkeley.edu/3/">AMP Camp</a></p>
<h2 id="building-a-framework-on-mesos-a-case-study-with-jenkins"><a class="header" href="#building-a-framework-on-mesos-a-case-study-with-jenkins">Building a Framework on Mesos: A Case Study with Jenkins</a></h2>
<p><a href="http://www.youtube.com/watch?v=TPXw_lMTJVk">Video</a>
Vinod Kone
Presented July 25, 2013 at <a href="http://www.meetup.com/Distributed-data-processing-with-Mesos/events/128585772/">SF Mesos Meetup</a></p>
<h2 id="hadoop-on-mesos"><a class="header" href="#hadoop-on-mesos">Hadoop on Mesos</a></h2>
<p><a href="http://www.youtube.com/watch?v=SFj5EMw8THk">Video</a>
Brenden Matthews
Presented July 25, 2013 at <a href="http://www.meetup.com/Distributed-data-processing-with-Mesos/events/128585772/">SF Mesos Meetup</a></p>
<h2 id="introduction-to-apache-mesos"><a class="header" href="#introduction-to-apache-mesos">Introduction to Apache Mesos</a></h2>
<p><a href="https://speakerdeck.com/benh/apache-mesos-nyc-meetup">Slides</a>
Benjamin Hindman
Presented August 20, 2013 at <a href="https://mesos-nyc-aug2013.eventbrite.com/">NYC Mesos Meetup</a></p>
<h2 id="chronos-a-distributed-fault-tolerant-and-highly-available-job-orchestration-framework-for-mesos"><a class="header" href="#chronos-a-distributed-fault-tolerant-and-highly-available-job-orchestration-framework-for-mesos">Chronos: A Distributed, Fault-Tolerant and Highly Available Job Orchestration Framework for Mesos</a></h2>
<p><a href="https://speakerdeck.com/mesos/chronos-august-2013-nyc-meetup">Slides</a>
Florian Leibert
Presented August 20, 2013 at <a href="https://mesos-nyc-aug2013.eventbrite.com/">NYC Mesos Meetup</a></p>
<h2 id="airbnb-tech-talk"><a class="header" href="#airbnb-tech-talk">Airbnb Tech Talk</a></h2>
<p><a href="http://www.youtube.com/watch?v=Hal00g8o1iY">Video</a>
Benjamin Hindman Presented September 6, 2012 at <a href="http://airbnb.com">Airbnb</a></p>
<h2 id="managing-twitter-clusters-with-mesos"><a class="header" href="#managing-twitter-clusters-with-mesos">Managing Twitter Clusters with Mesos</a></h2>
<p><a href="http://www.youtube.com/watch?v=37OMbAjnJn0">Video</a>
Benjamin Hindman Presented August 22, 2012 at <a href="http://ampcamp.berkeley.edu">AMP Camp</a></p>
<h2 id="mesos-a-platform-for-fine-grained-resource-sharing-in-datacenters"><a class="header" href="#mesos-a-platform-for-fine-grained-resource-sharing-in-datacenters">Mesos: A Platform for Fine-Grained Resource Sharing in Datacenters</a></h2>
<p><a href="http://www.youtube.com/watch?v=dB8IDu7g9Nc">Video</a>
Matei Zaharia
Presented March 2011 at <a href="http://berkeley.edu">UC Berkeley</a></p>
<h2 id="mesos-efficiently-sharing-the-datacenter"><a class="header" href="#mesos-efficiently-sharing-the-datacenter">Mesos: Efficiently Sharing the Datacenter</a></h2>
<p><a href="http://vimeo.com/17821090">Video</a>
Benjamin Hindman
Presented November 8, 2010 at <a href="http://linkedin.com">LinkedIn</a></p>
<h2 id="mesos-a-resource-management-platform-for-hadoop-and-big-data-clusters"><a class="header" href="#mesos-a-resource-management-platform-for-hadoop-and-big-data-clusters">Mesos: A Resource Management Platform for Hadoop and Big Data Clusters</a></h2>
<p><a href="http://www.youtube.com/watch?v=lE3jR6nM3bw">Video</a>
Matei Zaharia
Presented Summer 2010 at <a href="http://yahoo.com">Yahoo</a></p>
<h1 id="apache-mesos---paid-training"><a class="header" href="#apache-mesos---paid-training">Apache Mesos - Paid Training</a></h1>
<h2 id="automated-machine-learning-pipeline-with-mesos"><a class="header" href="#automated-machine-learning-pipeline-with-mesos">Automated Machine Learning Pipeline with Mesos</a></h2>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/automated-machine-learning-pipeline-mesos-integrated-course">Video</a>
Karl Whitford
Packt (November 2017)</p>
<h2 id="docker-apache-mesos--dcos-run-and-manage-cloud-datacenter-video"><a class="header" href="#docker-apache-mesos--dcos-run-and-manage-cloud-datacenter-video">Docker, Apache Mesos &amp; DCOS: Run and manage cloud datacenter (<a href="https://www.packtpub.com/networking-and-servers/docker-apache-mesos-dcos-run-and-manage-cloud-datacenter-video">Video</a>)</a></h2>
<p>Manuj Aggarwal
Packt (January 2018) </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-release-and-support-policy"><a class="header" href="#mesos-release-and-support-policy">Mesos Release and Support policy</a></h1>
<p>The Mesos versioning and release policy gives operators and developers clear guidelines on:</p>
<ul>
<li>Making modifications to the existing APIs without affecting backward compatibility.</li>
<li>How long a Mesos API will be supported.</li>
<li>Upgrading a Mesos installation across release versions.</li>
</ul>
<p>This document describes the release strategy for Mesos post 1.0.0 release.</p>
<h2 id="release-schedule"><a class="header" href="#release-schedule">Release Schedule</a></h2>
<p>Mesos releases are time-based, though we do make limited adjustments to the release schedule to accommodate feature development. This gives users and developers a predictable cadence to consume and produce features, while ensuring that each release can include the developments that users are waiting for.</p>
<p>If a feature is not ready by the time a release is cut, that feature should be disabled. This means that features should be developed in such a way that they are opt-in by default and can be easily disabled (e.g., flag).</p>
<p>A new Mesos release is cut approximately every <strong>3 months</strong>. The versioning scheme is <a href="http://semver.org">SemVer</a>. Typically, the minor release version is incremented by 1 (e.g., 1.1, 1.2, 1.3 etc) for every release, unless it is a major release.</p>
<p>Every (minor) release is a stable release and recommended for production use. This means a release candidate will go through rigorous testing (unit tests, integration tests, benchmark tests, cluster tests, scalability, etc.) before being officially released. In the rare case that a regular release is not deemed stable, a patch release will be released that will stabilize it.</p>
<p>At any given time, 3 releases are supported: the latest release and the two prior. Support means fixing of <em>critical issues</em> that affect the release. Once an issue is deemed critical, it will be fixed in only those <strong>affected</strong> releases that are still <strong>supported</strong>. This is called a patch release and increments the patch version by 1 (e.g., 1.2.1). Once a release reaches End Of Life (i.e., support period has ended), no more patch releases will be made for that release. Note that this is not related to backwards compatibility guarantees and deprecation periods (discussed later).</p>
<p>Which issues are considered critical?</p>
<ul>
<li>Security fixes</li>
<li>Compatibility regressions</li>
<li>Functional regressions</li>
<li>Performance regressions</li>
<li>Fixes for 3rd party integration (e.g., Docker remote API)</li>
</ul>
<p>Whether an issue is considered critical or not is sometimes subjective. In some cases it is obvious and sometimes it is fuzzy. Users should work with committers to figure out the criticality of an issue and get agreement and commitment for support.</p>
<p>Patch releases are normally done <strong>once per month</strong>.</p>
<p>If a particular issue is affecting a user and the user cannot wait until the next scheduled patch release, they can request an off-schedule patch release for a specific supported version. This should be done by sending an email to the dev list.</p>
<h2 id="upgrades"><a class="header" href="#upgrades">Upgrades</a></h2>
<p>All stable releases will be loosely compatible. Loose compatibility means:</p>
<ul>
<li>Master or agent can be upgraded to a new release version as long as they or the ecosystem components (scheduler, executor, zookeeper, service discovery layer, monitoring etc) do not depend on deprecated features (e.g., deprecated flags, deprecated metrics).</li>
<li>There should be no unexpected effect on externally visible behavior that is not deprecated. See API compatibility section for what should be expected for Mesos APIs.</li>
</ul>
<blockquote>
<p>NOTE: The compatibility guarantees do not apply to modules yet. See Modules section below for details.</p>
</blockquote>
<p>This means users should be able to upgrade (as long as they are not depending on deprecated / removed features) Mesos master or agent from a stable release version N directly to another stable release version M without having to go through intermediate release versions. For the purposes of upgrades, a stable release means the release with the latest patch version. For example, among 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1 releases 1.2.1, 1.3.0 and 1.4.1 are considered stable and so a user should be able to upgrade from 1.2.1 directly to 1.4.1. Look at the API compatability section below for how frameworks can do seamless upgrades.</p>
<p>The deprecation period for any given feature will be <strong>6 months</strong>. Having a set period allows Mesos developers to not indefinitely accrue technical debt and allows users time to plan for upgrades.</p>
<p>The detailed information about upgrading to a particular Mesos version would be posted <a href="upgrades.html">here</a>.</p>
<h2 id="api-versioning"><a class="header" href="#api-versioning">API versioning</a></h2>
<p>The Mesos APIs (constituting Scheduler, Executor, Internal, Operator/Admin APIs) will have a version in the URL. The versioned URL will have a prefix of <strong><code>/api/vN</code></strong> where &quot;N&quot; is the version of the API. The &quot;/api&quot; prefix is chosen to distinguish API resources from Web UI paths.</p>
<p>Examples:</p>
<ul>
<li>http://localhost:5050/api/v1/scheduler :  Scheduler HTTP API hosted by the master.</li>
<li>http://localhost:5051/api/v1/executor  :  Executor HTTP API hosted by the agent.</li>
</ul>
<p>A given Mesos installation might host multiple versions of the same API i.e., Scheduler API v1 and/or v2 etc.</p>
<h3 id="api-version-vs-release-version"><a class="header" href="#api-version-vs-release-version">API version vs Release version</a></h3>
<ul>
<li>To keep things simple, the stable version of the API will correspond to the major release version of Mesos.
<ul>
<li>For example, v1 of the API will be supported by Mesos release versions 1.0.0, 1.4.0, 1.20.0 etc.</li>
</ul>
</li>
<li>vN version of the API might also be supported by release versions of N-1 series but the vN API is not considered stable until the last release version of N-1 series.</li>
<li>For example, v2 of the API might be introduced in Mesos 1.12.0 release but it is only considered stable in Mesos 1.21.0 release if it is the last release of &quot;1&quot; series. Note that all Mesos 1.x.y versions will still support v1 of the API.</li>
<li>The API version is only bumped if we need to make a backwards <a href="versioning.html#api-compatibility">incompatible</a> API change. We will strive to support a given API version for at least a year.</li>
<li>The deprecation clock for vN-1 API will start as soon as we release &quot;N.0.0&quot; version of Mesos. We will strive to give enough time (e.g., 6 months) for frameworks/operators to upgrade to vN API before we stop supporting vN-1 API.</li>
</ul>
<p><a name="api-compatibility"></a></p>
<h3 id="api-compatibility"><a class="header" href="#api-compatibility">API Compatibility</a></h3>
<p>The API compatibility is determined by the corresponding protobuf guarantees.</p>
<p>As an example, the following are considered &quot;backwards compatible&quot; changes for Scheduler API:</p>
<ul>
<li>Adding new types of Calls i.e., new types of HTTP requests to &quot;/scheduler&quot;.</li>
<li>Adding new optional fields to existing requests to &quot;/scheduler&quot;.</li>
<li>Adding new types of Events i.e., new types of chunks streamed on &quot;/scheduler&quot;.</li>
<li>Adding new header fields to chunked response streamed on &quot;/scheduler&quot;.</li>
<li>Adding new fields (or changing the order of fields) to chunks' body streamed on &quot;/scheduler&quot;.</li>
<li>Adding new API resources (e.g., &quot;/foobar&quot;).</li>
</ul>
<p>The following are considered backwards incompatible changes for Scheduler API:</p>
<ul>
<li>Adding new required fields to existing requests to &quot;/scheduler&quot;.</li>
<li>Renaming/removing fields from existing requests to &quot;/scheduler&quot;.</li>
<li>Renaming/removing fields from chunks streamed on &quot;/scheduler&quot;.</li>
<li>Renaming/removing existing Calls.</li>
</ul>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<h3 id="release-branches"><a class="header" href="#release-branches">Release branches</a></h3>
<p>For regular releases, the work is done on the master branch. There are no feature branches but there will be release branches.</p>
<p>When it is time to cut a minor release, a new branch (e.g., 1.2.x) is created off the master branch. We chose 'x' instead of patch release number to disambiguate branch names from tag names. Then the first RC (-rc1) is tagged on the release branch. Subsequent RCs, in case the previous RCs fail testing, should be tagged on the release branch.</p>
<p>Patch releases are also based off the release branches. Typically the fix for an issue that is affecting supported releases lands on the master branch and is then backported to the release branch(es). In rare cases, the fix might directly go into a release branch without landing on master (e.g.,  fix / issue is not applicable to master).</p>
<p>Having a branch for each minor release reduces the amount of work a release manager needs to do when it is time to do a release. It is the responsibility of the committer of a fix to commit it to all the affecting release branches. This is important because the committer has more context about the issue / fix at the time of the commit than a release manager at the time of release. The release manager of a minor release will be responsible for all its patch releases as well. Just like the master branch, history rewrites are not allowed in the release branch (i.e., no git push --force).</p>
<h3 id="api-protobufs"><a class="header" href="#api-protobufs">API protobufs</a></h3>
<p>Most APIs in Mesos accept protobuf messages with a corresponding JSON field mapping. To support multiple versions of the API, we decoupled the versioned protobufs backing the API from the &quot;internal&quot; protobufs used by the Mesos code.</p>
<p>For example, the protobufs for the v1 Scheduler API are located at:</p>
<pre><code>include/mesos/v1/scheduler/scheduler.proto

package mesos.v1.scheduler;
option java_package = &quot;org.apache.mesos.v1.scheduler&quot;;
option java_outer_classname = &quot;Protos&quot;;
...
</code></pre>
<p>The corresponding internal protobufs for the Scheduler API are located at:</p>
<pre><code>include/mesos/scheduler/scheduler.proto

package mesos.scheduler;
option java_package = &quot;org.apache.mesos.scheduler&quot;;
option java_outer_classname = &quot;Protos&quot;;
...
</code></pre>
<p>The users of the API send requests (and receive responses) based on the versioned protobufs. We implemented <a href="https://github.com/apache/mesos/blob/master/src/internal/evolve.hpp">evolve</a>/<a href="https://github.com/apache/mesos/blob/master/src/internal/devolve.hpp">devolve</a> converters that can convert protobufs from any supported version to the internal protobuf and vice versa.</p>
<p>Internally, message passing between various Mesos components would use the internal unversioned protobufs. When sending response (if any) back to the user of the API, the unversioned protobuf would be converted back to a versioned protobuf.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building"><a class="header" href="#building">Building</a></h1>
<h2 id="downloading-mesos"><a class="header" href="#downloading-mesos">Downloading Mesos</a></h2>
<p>There are different ways you can get Mesos:</p>
<p>1. Download the latest stable release from <a href="http://mesos.apache.org/downloads/">Apache</a> (<em><strong>Recommended</strong></em>)</p>
<pre><code>$ wget https://downloads.apache.org/mesos/1.11.0/mesos-1.11.0.tar.gz
$ tar -zxf mesos-1.11.0.tar.gz
</code></pre>
<p>2. Clone the Mesos git <a href="https://gitbox.apache.org/repos/asf/mesos.git">repository</a> (<em><strong>Advanced Users Only</strong></em>)</p>
<pre><code>$ git clone https://gitbox.apache.org/repos/asf/mesos.git
</code></pre>
<p><em>NOTE: If you have problems running the above commands, you may need to first run through the <em><strong>System Requirements</strong></em> section below to install the <code>wget</code>, <code>tar</code>, and <code>git</code> utilities for your system.</em></p>
<h2 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h2>
<p>Mesos runs on Linux (64 Bit) and Mac OS X (64 Bit). To build Mesos from source, GCC 4.8.1+ or Clang 3.5+ is required.</p>
<p>On Linux, a kernel version &gt;= 2.6.28 is required at both build time and run time. For full support of process isolation under Linux a recent kernel &gt;= 3.10 is required.</p>
<p>The Mesos agent also runs on Windows. To build Mesos from source, follow the instructions in the <a href="windows.html">Windows</a> section.</p>
<p>Make sure your hostname is resolvable via DNS or via <code>/etc/hosts</code> to allow full support of Docker's host-networking capabilities, needed for some of the Mesos tests. When in doubt, please validate that <code>/etc/hosts</code> contains your hostname.</p>
<h3 id="ubuntu-1404"><a class="header" href="#ubuntu-1404">Ubuntu 14.04</a></h3>
<p>Following are the instructions for stock Ubuntu 14.04. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Update the packages.
$ sudo apt-get update

# Install a few utility tools.
$ sudo apt-get install -y tar wget git

# Install the latest OpenJDK.
$ sudo apt-get install -y openjdk-7-jdk

# Install autotools (Only necessary if building from git repository).
$ sudo apt-get install -y autoconf libtool

# Install other Mesos dependencies.
$ sudo apt-get -y install build-essential python-dev python-six python-virtualenv libcurl4-nss-dev libsasl2-dev libsasl2-modules maven libapr1-dev libsvn-dev
</code></pre>
<h3 id="ubuntu-1604"><a class="header" href="#ubuntu-1604">Ubuntu 16.04</a></h3>
<p>Following are the instructions for stock Ubuntu 16.04. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Update the packages.
$ sudo apt-get update

# Install a few utility tools.
$ sudo apt-get install -y tar wget git

# Install the latest OpenJDK.
$ sudo apt-get install -y openjdk-8-jdk

# Install autotools (Only necessary if building from git repository).
$ sudo apt-get install -y autoconf libtool

# Install other Mesos dependencies.
$ sudo apt-get -y install build-essential python-dev python-six python-virtualenv libcurl4-nss-dev libsasl2-dev libsasl2-modules maven libapr1-dev libsvn-dev zlib1g-dev iputils-ping
</code></pre>
<h3 id="mac-os-x-1011-el-capitan-macos-1012-sierra"><a class="header" href="#mac-os-x-1011-el-capitan-macos-1012-sierra">Mac OS X 10.11 (El Capitan), macOS 10.12 (Sierra)</a></h3>
<p>Following are the instructions for Mac OS X El Capitan. When building Mesos with the Apple-provided toolchain, the Command Line Tools from XCode &gt;= 8.0 are required; XCode 8 requires Mac OS X 10.11.5 or newer.</p>
<pre><code># Install Python 3: https://www.python.org/downloads/

# Install Command Line Tools. The Command Line Tools from XCode &gt;= 8.0 are required.
$ xcode-select --install

# Install Homebrew.
$ ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;

# Install Java.
$ brew install Caskroom/cask/java

# Install libraries.
$ brew install wget git autoconf automake libtool subversion maven xz

# Install Python dependencies.
$ sudo easy_install pip
$ pip install virtualenv
</code></pre>
<p>When compiling on macOS 10.12, the following is needed:</p>
<pre><code># There is an incompatiblity with the system installed svn and apr headers.
# We need the svn and apr headers from a brew installation of subversion.
# You may need to unlink the existing version of subversion installed via
# brew in order to configure correctly.
$ brew unlink subversion # (If already installed)
$ brew install subversion

# When configuring, the svn and apr headers from brew will be automatically
# detected, so no need to explicitly point to them.
# If the build fails due to compiler warnings, `--disable-werror` can be passed
# to configure to not treat warnings as errors.
$ ../configure

# Lastly, you may encounter the following error when the libprocess tests run:
$ ./libprocess-tests
Failed to obtain the IP address for '&lt;hostname&gt;'; the DNS service may not be able to resolve it: nodename nor servname provided, or not known

# If so, turn on 'Remote Login' within System Preferences &gt; Sharing to resolve the issue.
</code></pre>
<p><em>NOTE: When upgrading from Yosemite to El Capitan, make sure to rerun <code>xcode-select --install</code> after the upgrade.</em></p>
<h3 id="centos-66"><a class="header" href="#centos-66">CentOS 6.6</a></h3>
<p>Following are the instructions for stock CentOS 6.6. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Install a recent kernel for full support of process isolation.
$ sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
$ sudo rpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm
$ sudo yum --enablerepo=elrepo-kernel install -y kernel-lt

# Make the just installed kernel the one booted by default, and reboot.
$ sudo sed -i 's/default=1/default=0/g' /boot/grub/grub.conf
$ sudo reboot

# Install a few utility tools. This also forces an update of `nss`,
# which is necessary for the Java bindings to build properly.
$ sudo yum install -y tar wget git which nss

# 'Mesos &gt; 0.21.0' requires a C++ compiler with full C++11 support,
# (e.g. GCC &gt; 4.8) which is available via 'devtoolset-2'.
# Fetch the Scientific Linux CERN devtoolset repo file.
$ sudo wget -O /etc/yum.repos.d/slc6-devtoolset.repo http://linuxsoft.cern.ch/cern/devtoolset/slc6-devtoolset.repo

# Import the CERN GPG key.
$ sudo rpm --import http://linuxsoft.cern.ch/cern/centos/7/os/x86_64/RPM-GPG-KEY-cern

# Fetch the Apache Maven repo file.
$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo

# 'Mesos &gt; 0.21.0' requires 'subversion &gt; 1.8' devel package, which is
# not available in the default repositories.
# Create a WANdisco SVN repo file to install the correct version:
$ sudo bash -c 'cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF
[WANdiscoSVN]
name=WANdisco SVN Repo 1.8
enabled=1
baseurl=http://opensource.wandisco.com/centos/6/svn-1.8/RPMS/$basearch/
gpgcheck=1
gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco
EOF'

# Install essential development tools.
$ sudo yum groupinstall -y &quot;Development Tools&quot;

# Install 'devtoolset-2-toolchain' which includes GCC 4.8.2 and related packages.
# Installing 'devtoolset-3' might be a better choice since `perf` might
# conflict with the version of `elfutils` included in devtoolset-2.
$ sudo yum install -y devtoolset-2-toolchain

# Install other Mesos dependencies.
$ sudo yum install -y apache-maven python-devel python-six python-virtualenv java-1.7.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel

# Enter a shell with 'devtoolset-2' enabled.
$ scl enable devtoolset-2 bash
$ g++ --version  # Make sure you've got GCC &gt; 4.8!

# Process isolation is using cgroups that are managed by 'cgconfig'.
# The 'cgconfig' service is not started by default on CentOS 6.6.
# Also the default configuration does not attach the 'perf_event' subsystem.
# To do this, add 'perf_event = /cgroup/perf_event;' to the entries in '/etc/cgconfig.conf'.
$ sudo yum install -y libcgroup
$ sudo service cgconfig start
</code></pre>
<h3 id="centos-71"><a class="header" href="#centos-71">CentOS 7.1</a></h3>
<p>Following are the instructions for stock CentOS 7.1. If you are using a different OS, please install the packages accordingly.</p>
<pre><code># Install a few utility tools
$ sudo yum install -y tar wget git

# Fetch the Apache Maven repo file.
$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo

# Install the EPEL repo so that we can pull in 'libserf-1' as part of our
# subversion install below.
$ sudo yum install -y epel-release

# 'Mesos &gt; 0.21.0' requires 'subversion &gt; 1.8' devel package,
# which is not available in the default repositories.
# Create a WANdisco SVN repo file to install the correct version:
$ sudo bash -c 'cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF
[WANdiscoSVN]
name=WANdisco SVN Repo 1.9
enabled=1
baseurl=http://opensource.wandisco.com/centos/7/svn-1.9/RPMS/\$basearch/
gpgcheck=1
gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco
EOF'

# Parts of Mesos require systemd in order to operate. However, Mesos
# only supports versions of systemd that contain the 'Delegate' flag.
# This flag was first introduced in 'systemd version 218', which is
# lower than the default version installed by centos. Luckily, centos
# 7.1 has a patched 'systemd &lt; 218' that contains the 'Delegate' flag.
# Explicity update systemd to this patched version.
$ sudo yum update systemd

# Install essential development tools.
$ sudo yum groupinstall -y &quot;Development Tools&quot;

# Install other Mesos dependencies.
$ sudo yum install -y apache-maven python-devel python-six python-virtualenv java-1.8.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel
</code></pre>
<h3 id="windows"><a class="header" href="#windows">Windows</a></h3>
<p>Follow the instructions in the <a href="windows.html">Windows</a> section.</p>
<h2 id="building-mesos-posix"><a class="header" href="#building-mesos-posix">Building Mesos (Posix)</a></h2>
<pre><code># Change working directory.
$ cd mesos

# Bootstrap (Only required if building from git repository).
$ ./bootstrap

# Configure and build.
$ mkdir build
$ cd build
$ ../configure
$ make
</code></pre>
<p>In order to speed up the build and reduce verbosity of the logs, you can append <code>-j &lt;number of cores&gt; V=0</code> to <code>make</code>.</p>
<pre><code># Run test suite.
$ make check

# Install (Optional).
$ make install
</code></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>Mesos comes bundled with example frameworks written in C++, Java and Python.
The framework binaries will only be available after running <code>make check</code>, as
described in the <em><strong>Building Mesos</strong></em> section above.</p>
<pre><code># Change into build directory.
$ cd build

# Start Mesos master (ensure work directory exists and has proper permissions).
$ ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos

# Start Mesos agent (ensure work directory exists and has proper permissions).
$ ./bin/mesos-agent.sh --master=127.0.0.1:5050 --work_dir=/var/lib/mesos

# Visit the Mesos web page.
$ http://127.0.0.1:5050

# Run C++ framework (exits after successfully running some tasks).
$ ./src/test-framework --master=127.0.0.1:5050

# Run Java framework (exits after successfully running some tasks).
$ ./src/examples/java/test-framework 127.0.0.1:5050

# Run Python framework (exits after successfully running some tasks).
$ ./src/examples/python/test-framework 127.0.0.1:5050
</code></pre>
<p><em>Note: These examples assume you are running Mesos on your local machine.
Following them will not allow you to access the Mesos web page in a production
environment (e.g. on AWS). For that you will need to specify the actual IP of
your host when launching the Mesos master and ensure your firewall settings
allow access to port 5050 from the outside world.</em></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="binary-packages"><a class="header" href="#binary-packages">Binary Packages</a></h1>
<h2 id="downloading-the-mesos-rpm"><a class="header" href="#downloading-the-mesos-rpm">Downloading the Mesos RPM</a></h2>
<p>Download and install the latest stable CentOS7 RPM binary from the <a href="http://rpm.aventer.biz/CentOS/7/x86_64/">Repository</a>:</p>
<pre><code>$ cat &gt; /tmp/aventer.repo &lt;&lt;EOF
#aventer-mesos-el - packages by mesos from aventer
[aventer-rel]
name=AVENTER stable repository $releasever
baseurl=http://rpm.aventer.biz/CentOS/$releasever/$basearch/
enabled=1
gpgkey=https://www.aventer.biz/CentOS/support_aventer.asc
EOF

$ sudo mv /tmp/aventer.repo /etc/yum.repos.d/aventer.repo

$ sudo yum update

$ sudo yum install mesos
</code></pre>
<p>The above instructions show how to install the latest version of Mesos for RHEL 7.
Substitute <code>baseurl</code> the with the appropriate URL for your operating system.</p>
<h2 id="start-mesos-master-and-agent"><a class="header" href="#start-mesos-master-and-agent">Start Mesos Master and Agent.</a></h2>
<p>The RPM installation creates the directory <code>/var/lib/mesos</code> that can be used as a work directory.</p>
<p>Start the Mesos master with the following command:</p>
<pre><code>$ mesos-master --work_dir=/var/lib/mesos
</code></pre>
<p>On a different terminal, start the Mesos agent, and associate it with the Mesos master started above:</p>
<pre><code>$ mesos-agent --work_dir=/var/lib/mesos --master=127.0.0.1:5050
</code></pre>
<p>This is the simplest way to try out Mesos after downloading the RPM. For more complex and production
setup instructions refer to the <a href="http://mesos.apache.org/documentation/latest/#administration">Administration</a> section of the docs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-runtime-configuration"><a class="header" href="#mesos-runtime-configuration">Mesos Runtime Configuration</a></h1>
<p>The Mesos master and agent can take a variety of configuration options
through command-line arguments or environment variables. A list of the
available options can be seen by running <code>mesos-master --help</code> or
<code>mesos-agent --help</code>. Each option can be set in two ways:</p>
<ul>
<li>
<p>By passing it to the binary using <code>--option_name=value</code>, either
specifying the value directly, or specifying a file in which the value
resides (<code>--option_name=file://path/to/file</code>). The path can be
absolute or relative to the current working directory.</p>
</li>
<li>
<p>By setting the environment variable <code>MESOS_OPTION_NAME</code> (the option
name with a <code>MESOS_</code> prefix added to it).</p>
</li>
</ul>
<p>Configuration values are searched for first in the environment, then
on the command-line.</p>
<p>Additionally, this documentation lists only a recent snapshot of the options in
Mesos. A definitive source for which flags your version of Mesos supports can be
found by running the binary with the flag <code>--help</code>, for example <code>mesos-master --help</code>.</p>
<h2 id="master-and-agent-options"><a class="header" href="#master-and-agent-options">Master and Agent Options</a></h2>
<p><em>These are options common to both the Mesos master and agent.</em></p>
<p>See <a href="configuration/master-and-agent.html">configuration/master-and-agent.md</a>.</p>
<h2 id="master-options"><a class="header" href="#master-options">Master Options</a></h2>
<p>See <a href="configuration/master.html">configuration/master.md</a>.</p>
<h2 id="agent-options"><a class="header" href="#agent-options">Agent Options</a></h2>
<p>See <a href="configuration/agent.html">configuration/agent.md</a>.</p>
<h2 id="libprocess-options"><a class="header" href="#libprocess-options">Libprocess Options</a></h2>
<p>See <a href="configuration/libprocess.html">configuration/libprocess.md</a>.</p>
<h1 id="mesos-build-configuration"><a class="header" href="#mesos-build-configuration">Mesos Build Configuration</a></h1>
<h2 id="autotools-options"><a class="header" href="#autotools-options">Autotools Options</a></h2>
<p>If you have special compilation requirements, please refer to <code>./configure --help</code> when configuring Mesos.</p>
<p>See <a href="configuration/autotools.html">configuration/autotools.md</a>.</p>
<h2 id="cmake-options"><a class="header" href="#cmake-options">CMake Options</a></h2>
<p>See <a href="configuration/cmake.html">configuration/cmake.md</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-cmake-37"><a class="header" href="#install-cmake-37">Install CMake 3.7+</a></h1>
<h2 id="linux"><a class="header" href="#linux">Linux</a></h2>
<p>Install the latest version of CMake from <a href="https://cmake.org/download/">CMake.org</a>.
A self-extracting tarball is available to make this process painless.</p>
<p>Currently, few of the common Linux flavors package a sufficient CMake
version. Ubuntu versions 12.04 and 14.04 package CMake 2;
Ubuntu 16.04 packages CMake 3.5. If you already installed cmake from packages,
you may remove it via: <code>apt-get purge cmake</code>.</p>
<p>The standard CentOS package is CMake 2, and unfortunately even the <code>cmake3</code>
package in EPEL is only CMake 3.6, you may remove them via:
<code>yum remove cmake cmake3</code>.</p>
<h2 id="mac-os-x"><a class="header" href="#mac-os-x">Mac OS X</a></h2>
<p>HomeBrew's CMake version is sufficient: <code>brew install cmake</code>.</p>
<h2 id="windows-1"><a class="header" href="#windows-1">Windows</a></h2>
<p>Download and install the MSI from <a href="https://cmake.org/download/">CMake.org</a>.</p>
<p><strong>NOTE:</strong> Windows needs CMake 3.8+, rather than 3.7+.</p>
<h1 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h1>
<p>The most basic way to build with CMake, with no configuration, is fairly
straightforward:</p>
<pre><code>mkdir build
cd build
cmake ..
cmake --build .
</code></pre>
<p>The last step, <code>cmake --build .</code> can also take a <code>--target</code> command to build any
particular target (e.g. <code>mesos-tests</code>, or <code>tests</code> to build <code>mesos-tests</code>,
<code>libprocess-tests</code>, and <code>stout-tests</code>): <code>cmake --build . --target tests</code>. To
send arbitrary flags to the native build system underneath (e.g. <code>make</code>), append
the command with <code>-- &lt;flags to be passed&gt;</code>: <code>cmake --build . -- -j4</code>.</p>
<p>Also, <code>cmake --build</code> can be substituted by your build system of choice. For
instance, the default CMake generator on Linux produces GNU Makefiles, so after
configuring with <code>cmake ..</code>, you can just run <code>make tests</code> in the <code>build</code> folder
like usual. Similarly, if you configure with <code>-G Ninja</code> to use the Ninja
generator, you can then run <code>ninja tests</code> to build the <code>tests</code> target with
Ninja.</p>
<h1 id="installable-build"><a class="header" href="#installable-build">Installable build</a></h1>
<p>This example will build Mesos and install it into a custom prefix:</p>
<pre><code>mkdir build &amp;&amp; cd build
cmake -DCMAKE_INSTALL_PREFIX=/home/current_user/mesos
cmake --build . --target install
</code></pre>
<p>To additionally install <code>mesos-tests</code> executable and related test helpers
(this can be used to run Mesos tests against the installed binaries),
one can enable the <code>MESOS_INSTALL_TESTS</code> option.</p>
<p>To produce a set of binaries and libraries that will work after being
copied/moved to a different location, use <code>MESOS_FINAL_PREFIX</code>.</p>
<p>The example below employs both <code>MESOS_FINAL_PREFIX</code> and <code>MESOS_INSTALL_TESTS</code>.
On a build system:</p>
<pre><code>mkdir build &amp;&amp; cd build
cmake -DMESOS_FINAL_PREFIX=/opt/mesos -DCMAKE_INSTALL_PREFIX=/home/current_user/mesos -DMESOS_INSTALL_TESTS=ON
cmake --build . --target install
tar -czf mesos.tar.gz mesos -C /home/current_user
</code></pre>
<p>On a target system:</p>
<pre><code>sudo tar -xf mesos.tar.gz -C /opt
# Run tests against Mesos installation
sudo /opt/mesos/bin/mesos-tests
# Start Mesos agent
sudo /opt/mesos/bin/mesos-agent --work-dir=/var/lib/mesos ...
</code></pre>
<h1 id="supported-options"><a class="header" href="#supported-options">Supported options</a></h1>
<p>See <a href="configuration/cmake.html">configuration options</a>.</p>
<h1 id="examples-1"><a class="header" href="#examples-1">Examples</a></h1>
<p>See <a href="cmake-examples.html">CMake By Example</a>.</p>
<h1 id="documentation"><a class="header" href="#documentation">Documentation</a></h1>
<p>The <a href="https://cmake.org/cmake/help/latest/">CMake documentation</a> is written as a reference module. The most commonly
used sections are:</p>
<ul>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-buildsystem.7.html">buildsystem overview</a></li>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-commands.7.html">commands</a></li>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-properties.7.html">properties</a></li>
<li><a href="https://cmake.org/cmake/help/latest/manual/cmake-variables.7.html">variables</a></li>
</ul>
<p>The wiki also has a set of <a href="https://cmake.org/Wiki/CMake_Useful_Variables">useful variables</a>.</p>
<h1 id="dependency-graph"><a class="header" href="#dependency-graph">Dependency graph</a></h1>
<p>Like any build system, CMake has a dependency graph. The difference is
that targets in CMake's dependency graph are <em>much richer</em> compared to other
build systems. CMake targets have the notion of 'interfaces', where build
properties are saved as part of the target, and these properties can be
inherited transitively within the graph.</p>
<p>For example, say there is a library <code>mylib</code>, and anything which links it must
include its headers, located in <code>mylib/include</code>. When building the library, some
private headers must also be included, but not when linking to it. When
compiling the executable <code>myprogram</code>, <code>mylib</code>'s public headers must be included,
but not its private headers. There is no manual step to add <code>mylib/include</code> to
<code>myprogram</code> (and any other program which links to <code>mylib</code>), it is instead
deduced from the public interface property of <code>mylib</code>. This is represented by
the following code:</p>
<pre><code># A new library with a single source file (headers are found automatically).
add_library(mylib mylib.cpp)

# The folder of private headers, not exposed to consumers of `mylib`.
target_include_directories(mylib PRIVATE mylib/private)

# The folder of public headers, added to the compilation of any consumer.
target_include_directories(mylib PUBLIC mylib/include)

# A new exectuable with a single source file.
add_executable(myprogram main.cpp)

# The creation of the link dependency `myprogram` -&gt; `mylib`.
target_link_libraries(myprogram mylib)

# There is no additional step to add `mylib/include` to `myprogram`.
</code></pre>
<p>This same notion applies to practically every build property:
compile definitions via <a href="https://cmake.org/cmake/help/latest/command/target_compile_definitions.html"><code>target_compile_definitions</code></a>,
include directories via <a href="https://cmake.org/cmake/help/latest/command/target_include_directories.html"><code>target_include_directories</code></a>,
link libraries via <a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html"><code>target_link_libraries</code></a>,
compile options via <a href="https://cmake.org/cmake/help/latest/command/target_compile_options.html"><code>target_compile_options</code></a>,
and compile features via <a href="https://cmake.org/cmake/help/latest/command/target_compile_features.html"><code>target_compile_features</code></a>.</p>
<p>All of these commands also take an optional argument of
<code>&lt;INTERFACE|PUBLIC|PRIVATE&gt;</code>, which constrains their transitivity in the graph.
That is, a <code>PRIVATE</code> include directory is recorded for the target, but not
shared transitively to anything depending on the target, <code>PUBLIC</code> is used
for both the target and dependencies on it, and <code>INTERFACE</code> is used only
for dependencies.</p>
<p>Notably missing from this list are link directories. CMake explicitly prefers
finding and using the absolute paths to libraries, obsoleting link directories.</p>
<h1 id="common-mistakes"><a class="header" href="#common-mistakes">Common mistakes</a></h1>
<h2 id="booleans"><a class="header" href="#booleans">Booleans</a></h2>
<p>CMake treats <code>ON</code>, <code>OFF</code>, <code>TRUE</code>, <code>FALSE</code>, <code>1</code>, and <code>0</code> all as true/false
booleans. Furthermore, variables of the form <code>&lt;target&gt;-NOTFOUND</code> are also
treated as false (this is used for finding packages).</p>
<p>In Mesos, we prefer the boolean types <code>TRUE</code> and <code>FALSE</code>.</p>
<p>See <a href="https://cmake.org/cmake/help/latest/command/if.html"><code>if</code></a> for more info.</p>
<h2 id="conditionals"><a class="header" href="#conditionals">Conditionals</a></h2>
<p>For historical reasons, CMake conditionals such as <code>if</code> and <code>elseif</code>
automatically interpolate variable names. It is therefore dangerous to
interpolate them manually, because if <code>${FOO}</code> evaluates to <code>BAR</code>, and <code>BAR</code> is
another variable name, then <code>if (${FOO})</code> becomes <code>if (BAR)</code>, and <code>BAR</code> is then
evaluated again by the <code>if</code>. Stick to <code>if (FOO)</code> to check the value of <code>${FOO}</code>.
Do not use <code>if (${FOO})</code>.</p>
<p>Also see the CMake policies
<a href="https://cmake.org/cmake/help/latest/policy/CMP0012.html">CMP0012</a> and
<a href="https://cmake.org/cmake/help/latest/policy/CMP0054.html">CMP0054</a>.</p>
<h2 id="definitions"><a class="header" href="#definitions">Definitions</a></h2>
<p>When using <code>add_definitions()</code> (which should be used rarely, as it is for
&quot;global&quot; compile definitions), the flags must be prefixed with <code>-D</code> to be
treated as preprocessor definitions. However, when using
<code>target_compile_definitions()</code> (which should be preferred, as it is
for specific targets), the flags do not need the prefix.</p>
<h1 id="style"><a class="header" href="#style">Style</a></h1>
<p>In general, wrap at 80 lines, and use a two-space indent. When wrapping
arguments, put the command on a separate line and arguments on subsequent lines:</p>
<pre><code>target_link_libraries(
  program PRIVATE
  alpha
  beta
  gamma)
</code></pre>
<p>Otherwise keep it together:</p>
<pre><code>target_link_libraries(program PUBLIC library)
</code></pre>
<p>Always keep the trailing parenthesis with the last argument.</p>
<p>Use a single space between conditionals and their open parenthesis, e.g.
<code>if (FOO)</code>, but not for commands, e.g. <code>add_executable(program)</code>.</p>
<p>CAPITALIZE the declaration and use of custom functions and macros (e.g.
<code>EXTERNAL</code> and <code>PATCH_CMD</code>), and do not capitalize the use of CMake built-in
(including modules) functions and macros. CAPITALIZE variables.</p>
<h1 id="cmake-anti-patterns"><a class="header" href="#cmake-anti-patterns">CMake anti-patterns</a></h1>
<p>Because CMake handles much more of the grunt work for you than other build
systems, there are unfortunately a lot of CMake <a href="http://voices.canonical.com/jussi.pakkanen/2013/03/26/a-list-of-common-cmake-antipatterns/">anti-patterns</a> you should
look out for when writing new CMake code. These are some common problems
that should be avoided when writing new CMake code:</p>
<h2 id="superfluous-use-of-add_dependencies"><a class="header" href="#superfluous-use-of-add_dependencies">Superfluous use of <code>add_dependencies</code></a></h2>
<p>When you've linked library <code>a</code> to library <code>b</code> with <code>target_link_libraries(a b)</code>,
the CMake graph is already updated with the dependency information. It is
redundant to use <code>add_dependencies(a b)</code> to (re)specify the dependency. In fact,
this command should <em>rarely</em> be used.</p>
<p>The exceptions to this are:</p>
<ol>
<li>Setting a dependency from an imported library to a target added via
<code>ExternalProject_Add</code>.</li>
<li>Setting a dependency on Mesos modules since no explicit linking is done.</li>
<li>Setting a dependency between executables (e.g. the <code>mesos-agent</code> requiring the
<code>mesos-containerizer</code> executable). In general, runtime dependencies need
to be setup with <code>add_dependency</code>, but never link dependencies.</li>
</ol>
<h2 id="use-of-link_libraries-or-link_directories"><a class="header" href="#use-of-link_libraries-or-link_directories">Use of <code>link_libraries</code> or <code>link_directories</code></a></h2>
<p>Neither of these commands should ever be used. The only appropriate command used
to link libraries is <a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html"><code>target_link_libraries</code></a>, which records the information
in the CMake dependency graph. Furthermore, imported third-party libraries
should have correct locations recorded in their respective targets, so the use
of <code>link_directories</code> should never be necessary. The
<a href="https://cmake.org/cmake/help/latest/command/link_directories.html">official documentation</a> states:</p>
<blockquote>
<p>Note that this command is rarely necessary. Library locations returned by
<code>find_package()</code> and <code>find_library()</code> are absolute paths. Pass these absolute
library file paths directly to the <code>target_link_libraries()</code> command. CMake
will ensure the linker finds them.</p>
</blockquote>
<p>The difference is that the former sets global (or directory level) side effects,
and the latter sets specific target information stored in the graph.</p>
<h2 id="use-of-include_directories"><a class="header" href="#use-of-include_directories">Use of <code>include_directories</code></a></h2>
<p>This is similar to the above: the <a href="https://cmake.org/cmake/help/latest/command/target_include_directories.html"><code>target_include_directories</code></a> should always
be preferred so that the include directory information remains localized to the
appropriate targets.</p>
<h2 id="adding-anything-to-endif-"><a class="header" href="#adding-anything-to-endif-">Adding anything to <code>endif ()</code></a></h2>
<p>Old versions of CMake expected the style <code>if (FOO) ... endif (FOO)</code>, where the
<code>endif</code> contained the same expression as the <code>if</code> command. However, this is
tortuously redundant, so leave the parentheses in <code>endif ()</code> empty. This goes
for other endings too, such as <code>endforeach ()</code>, <code>endwhile ()</code>, <code>endmacro ()</code> and
<code>endfunction ()</code>.</p>
<h2 id="specifying-header-files-superfluously"><a class="header" href="#specifying-header-files-superfluously">Specifying header files superfluously</a></h2>
<p>One of the distinct advantages of using CMake for C and C++ projects is that
adding header files to the source list for a target is unnecessary. CMake is
designed to parse the source files (<code>.c</code>, <code>.cpp</code>, etc.) and determine their
required headers automatically. The exception to this is headers generated as
part of the build (such as protobuf or the JNI headers).</p>
<h2 id="checking-cmake_build_type"><a class="header" href="#checking-cmake_build_type">Checking <code>CMAKE_BUILD_TYPE</code></a></h2>
<p>See the <a href="cmake-examples.html#building-debug-or-release-configurations">&quot;Building debug or release configurations&quot;</a>
example for more information. In short, not all generators respect the variable
<code>CMAKE_BUILD_TYPE</code> at configuration time, and thus it must not be used in CMake
logic. A usable alternative (where supported) is a <a href="https://cmake.org/cmake/help/latest/manual/cmake-generator-expressions.7.html#logical-expressions">generator expression</a> such
as <code>$&lt;$&lt;CONFIG:Debug&gt;:DEBUG_MODE&gt;</code>.</p>
<h1 id="remaining-hacks"><a class="header" href="#remaining-hacks">Remaining hacks</a></h1>
<h2 id="3rdparty_dependencies"><a class="header" href="#3rdparty_dependencies"><code>3RDPARTY_DEPENDENCIES</code></a></h2>
<p>Until Mesos on Windows is stable, we keep some dependencies in an external
repository, <a href="https://github.com/mesos/3rdparty">3rdparty</a>. When
all dependencies are bundled with Mesos, this extra repository will no longer be
necessary. Until then, the CMake variable <code>3RDPARTY_DEPENDENCIES</code> points by
default to this URL, but it can also point to the on-disk location of a local
clone of the repo. With this option you can avoid pulling from GitHub for every
clean build. Note that this must be an absolute path with forward slashes, e.g.
<code>-D3RDPARTY_DEPENDENCIES=C:/3rdparty</code>, otherwise it will fail on Windows.</p>
<h2 id="external"><a class="header" href="#external"><code>EXTERNAL</code></a></h2>
<p>The CMake function <code>EXTERNAL</code> defines a few variables that make it easy for us
to track the directory structure of a dependency. In particular, if our
library's name is <code>boost</code>, we invoke:</p>
<pre><code>EXTERNAL(boost ${BOOST_VERSION} ${CMAKE_CURRENT_BINARY_DIR})
</code></pre>
<p>Which will define the following variables as side-effects in the current scope:</p>
<ul>
<li><code>BOOST_TARGET</code>     (a target folder name to put dep in e.g., <code>boost-1.53.0</code>)</li>
<li><code>BOOST_CMAKE_ROOT</code> (where to have CMake put the uncompressed source, e.g.,
<code>build/3rdparty/boost-1.53.0</code>)</li>
<li><code>BOOST_ROOT</code>       (where the code goes in various stages of build, e.g.,
<code>build/.../boost-1.53.0/src</code>, which might contain folders
<code>build-1.53.0-build</code>, <code>-lib</code>, and so on, for each build
step that dependency has)</li>
</ul>
<p>The implementation is in <code>3rdparty/cmake/External.cmake</code>.</p>
<p>This is not to be confused with the CMake module <a href="https://cmake.org/cmake/help/latest/module/ExternalProject.html">ExternalProject</a>, from which
we use <code>ExternalProject_Add</code> to download, extract, configure, and build our
dependencies.</p>
<h2 id="cmake_noop"><a class="header" href="#cmake_noop"><code>CMAKE_NOOP</code></a></h2>
<p>This is a CMake variable we define in <code>3rdparty/CMakeLists.txt</code> so that we can
cancel steps of <code>ExternalProject</code>. <code>ExternalProject</code>'s default behavior is to
attempt to configure, build, and install a project using CMake. So when one of
these steps must be skipped, we use set it to <code>CMAKE_NOOP</code> so that nothing
is run instead.</p>
<h2 id="cmake_forward_args"><a class="header" href="#cmake_forward_args"><code>CMAKE_FORWARD_ARGS</code></a></h2>
<p>The <code>CMAKE_FORWARD_ARGS</code> variable defined in <code>3rdparty/CMakeLists.txt</code> is sent
as the <code>CMAKE_ARGS</code> argument to the <code>ExternalProject_Add</code> macro (along with any
per-project arguments), and is used when the external project is configured as a
CMake project. If either the <code>CONFIGURE_COMMAND</code> or <code>BUILD_COMMAND</code> arguments of
<code>ExternalProject_Add</code> are used, then the <code>CMAKE_ARGS</code> argument will be ignored.
This variable ensures that compilation configurations are properly propagated to
third-party dependencies, such as compiler flags.</p>
<h3 id="cmake_ssl_forward_args"><a class="header" href="#cmake_ssl_forward_args"><code>CMAKE_SSL_FORWARD_ARGS</code></a></h3>
<p>The <code>CMAKE_SSL_FORWARD_ARGS</code> variable defined in <code>3rdparty/CMakeLists.txt</code>
is like <code>CMAKE_FORWARD_ARGS</code>, but only used for specific external projects
that find and link against OpenSSL.</p>
<h2 id="library_linkage"><a class="header" href="#library_linkage"><code>LIBRARY_LINKAGE</code></a></h2>
<p>This variable is a shortcut used in <code>3rdparty/CMakeLists.txt</code>. It is set to
<code>SHARED</code> when <code>BUILD_SHARED_LIBS</code> is true, and otherwise it is set to <code>STATIC</code>.
The <code>SHARED</code> and <code>STATIC</code> keywords are used to declare how a library should be
built; however, if left out then the type is deduced automatically from
<code>BUILD_SHARED_LIBS</code>.</p>
<h2 id="make_include_dir"><a class="header" href="#make_include_dir"><code>MAKE_INCLUDE_DIR</code></a></h2>
<p>This function works around a <a href="https://gitlab.kitware.com/cmake/cmake/issues/15052">CMake issue</a> with setting include
directories of imported libraries built with <code>ExternalProject_Add</code>. We have to
call this for each <code>IMPORTED</code> third-party dependency which has set
<code>INTERFACE_INCLUDE_DIRECTORIES</code>, just to make CMake happy. An example is Glog:</p>
<pre><code>MAKE_INCLUDE_DIR(glog)
</code></pre>
<h2 id="get_byproducts"><a class="header" href="#get_byproducts"><code>GET_BYPRODUCTS</code></a></h2>
<p>This function works around a <a href="https://cmake.org/pipermail/cmake/2015-April/060234.html">CMake issue</a> with the Ninja
generator where it does not understand imported libraries, and instead needs
<code>BUILD_BYPRODUCTS</code> explicitly set. This simply allows us to use
<code>ExternalProject_Add</code> and Ninja. For Glog, it looks like this:</p>
<pre><code>GET_BYPRODUCTS(glog)
</code></pre>
<p>Also see the CMake policy <a href="https://cmake.org/cmake/help/latest/policy/CMP0058.html">CMP0058</a>.</p>
<h2 id="patch_cmd"><a class="header" href="#patch_cmd"><code>PATCH_CMD</code></a></h2>
<p>The CMake function <code>PATCH_CMD</code> generates a patch command given a patch file.
If the path is not absolute, it's resolved to the current source directory.
It stores the command in the variable name supplied. This is used to easily
patch third-party dependencies. For Glog, it looks like this:</p>
<pre><code>PATCH_CMD(GLOG_PATCH_CMD glog-${GLOG_VERSION}.patch)
ExternalProject_Add(
  ${GLOG_TARGET}
  ...
  PATCH_COMMAND     ${GLOG_PATCH_CMD})
</code></pre>
<p>The implementation is in <code>3rdparty/cmake/PatchCommand.cmake</code>.</p>
<h3 id="windows-patchexe"><a class="header" href="#windows-patchexe">Windows <code>patch.exe</code></a></h3>
<p>While using <code>patch</code> on Linux is straightforward, doing the same on Windows takes
a bit of work. <code>PATH_CMD</code> encapsulates this:</p>
<ul>
<li>Checks the cache variable <code>PATCHEXE_PATH</code> for <code>patch.exe</code>.</li>
<li>Searches for <code>patch.exe</code> in its default locations.</li>
<li>Copies <code>patch.exe</code> and a custom manifest to the temporary directory.</li>
<li>Applies the manifest to avoid the UAC prompt.</li>
<li>Uses the patched <code>patch.exe</code>.</li>
</ul>
<p>As such, <code>PATCH_CMD</code> lets us apply patches as we do on Linux, without requiring
an administrative prompt.</p>
<p>Note that on Windows, the patch file must have CRLF line endings. A file with LF
line endings will cause the error: &quot;Assertion failed, hunk, file patch.c, line
343&quot;. For this reason, it is required to checkout the Mesos repo with <code>git config core.autocrlf true</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="windows-2"><a class="header" href="#windows-2">Windows</a></h1>
<p>Mesos 1.0.0 introduced experimental support for Windows.</p>
<h2 id="building-mesos"><a class="header" href="#building-mesos">Building Mesos</a></h2>
<h3 id="system-requirements-1"><a class="header" href="#system-requirements-1">System Requirements</a></h3>
<ol>
<li>
<p>Install the latest <a href="https://www.visualstudio.com/downloads/">Visual Studio 2017</a>:
The &quot;Community&quot; edition is sufficient (and free of charge).
During installation, choose the &quot;Desktop development with C++&quot; workload.</p>
</li>
<li>
<p>Install <a href="https://cmake.org/download/">CMake 3.8.0</a> or later.
During installation, choose to &quot;Add CMake to the system PATH for all users&quot;.</p>
</li>
<li>
<p>Install <a href="http://gnuwin32.sourceforge.net/packages/patch.htm">GNU patch for Windows</a>.</p>
</li>
<li>
<p>If building from source, install <a href="https://git-scm.com/download/win">Git</a>.</p>
</li>
<li>
<p>Make sure there are no spaces in your build directory.
For example, <code>C:/Program Files (x86)/mesos</code> is an invalid build directory.</p>
</li>
<li>
<p>If developing Mesos, install <a href="https://www.python.org/downloads/">Python 3</a>
(not Python 2), in order to use our <code>support</code> scripts (e.g.
to post and apply patches, or lint source code).</p>
</li>
</ol>
<h3 id="build-instructions"><a class="header" href="#build-instructions">Build Instructions</a></h3>
<p>Following are the instructions for Windows 10.</p>
<pre><code># Clone (or extract) Mesos.
git clone https://gitbox.apache.org/repos/asf/mesos.git
cd mesos

# Configure using CMake for an out-of-tree build.
mkdir build
cd build
cmake .. -G &quot;Visual Studio 15 2017 Win64&quot; -T &quot;host=x64&quot;

# Build Mesos.
# To build just the Mesos agent, add `--target mesos-agent`.
cmake --build .

# The Windows agent exposes new isolators that must be used as with
# the `--isolation` flag. To get started point the agent to a working
# master, using eiher an IP address or zookeeper information.
.\src\mesos-agent.exe --master=&lt;master&gt; --work_dir=&lt;work folder&gt; --launcher_dir=&lt;repository&gt;\build\src
</code></pre>
<h2 id="running-mesos"><a class="header" href="#running-mesos">Running Mesos</a></h2>
<p>If you deploy the executables to another machine, you must also
install the <a href="https://aka.ms/vs/15/release/VC_redist.x64.exe">Microsoft Visual C++ Redistributable for Visual Studio 2017</a>.</p>
<h2 id="known-limitations"><a class="header" href="#known-limitations">Known Limitations</a></h2>
<p>The current implementation is known to have the following limitations:</p>
<ul>
<li>
<p>Only the agent should be run on Windows. The Mesos master can be
launched, but only for testing as the master does not support
high-availability setups on Windows.</p>
</li>
<li>
<p>While Mesos supports NTFS long paths internally, tasks which do not support
long paths must be run on agent whose <code>--work_dir</code> is a short path.</p>
</li>
<li>
<p>The minimum versions of Windows supported are: Windows 10 Creators Update (AKA
version 1703, build number 15063), and <a href="https://docs.microsoft.com/en-us/windows-server/get-started/get-started-with-1709">Windows Server, version 1709</a>.
It is likely that this will increase, due to evolving Windows container
support and developer features which ease porting.</p>
</li>
<li>
<p>The ability to <a href="https://blogs.windows.com/buildingapps/2016/12/02/symlinks-windows-10/">create symlinks</a> as a non-admin user requires
Developer Mode to be enabled. Otherwise the agent will need to be
run under an administrator.</p>
</li>
</ul>
<h2 id="build-configuration-examples"><a class="header" href="#build-configuration-examples">Build Configuration Examples</a></h2>
<h3 id="building-with-ninja"><a class="header" href="#building-with-ninja">Building with Ninja</a></h3>
<p>Instead of using MSBuild, it is also possible to build Mesos on
Windows using <a href="https://ninja-build.org/">Ninja</a>, which can result in
significantly faster builds. To use Ninja, you need to download it and
ensure <code>ninja.exe</code> is in your <code>PATH</code>.</p>
<ul>
<li>Download the <a href="https://github.com/ninja-build/ninja/releases">Windows binary</a>.</li>
<li>Unzip it and place <code>ninja.exe</code> in your <code>PATH</code>.</li>
<li>Open an &quot;x64 Native Tools Command Prompt for VS 2017&quot; to set your
environment.</li>
<li>In that command prompt, type <code>powershell</code> to use a better shell.</li>
<li>Similar to above, configure CMake with <code>cmake .. -G Ninja</code>.</li>
<li>Now you can use <code>ninja</code> to build the various targets.</li>
<li>You may want to use <code>ninja -v</code> to make it verbose, as it's otherwise
very quiet.</li>
</ul>
<p>Note that with Ninja it is imperative to open the correct developer
command prompt so that the 64-bit build tools are used, as Ninja does
not otherwise know how to find them.</p>
<h3 id="building-with-java"><a class="header" href="#building-with-java">Building with Java</a></h3>
<p>This enables more unit tests, but we do not yet officially produce
<code>mesos-master</code>.</p>
<p>When building with Java on Windows, you must add the <a href="https://maven.apache.org/guides/getting-started/windows-prerequisites.html">Maven</a> build tool to
your path. The <code>JAVA_HOME</code> environment variable must also be manually set.
An installation of the Java SDK can be found form <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">Oracle</a>.</p>
<p>As of this writing, Java 9 is not yet supported, but Java 8 has been tested.</p>
<p>The Java build defaults to <code>OFF</code> because it is slow. To build the Java
components on Windows, turn it <code>ON</code>:</p>
<pre><code class="language-powershell">mkdir build; cd build
$env:PATH += &quot;;C:\...\apache-maven-3.3.9\bin\&quot;
$env:JAVA_HOME = &quot;C:\Program Files\Java\jdk1.8.0_144&quot;
cmake .. -DENABLE_JAVA=ON -G &quot;Visual Studio 15 2017 Win64&quot; -T &quot;host=x64&quot;
cmake --build . --target mesos-java
</code></pre>
<p>Note that the <code>mesos-java</code> library does not have to be manually built; as
<code>libmesos</code> will link it when Java is enabled.</p>
<p>Unfortunately, on Windows the <code>FindJNI</code> CMake module will populate <code>JAVA_JVM_LIBRARY</code> with
the path to the static <code>jvm.lib</code>, but this variable must point to the shared
library, <code>jvm.dll</code>, as it is loaded at runtime. Set it correctly like this:</p>
<pre><code>$env:JAVA_JVM_LIBRARY = &quot;C:\Program Files\Java\jdk1.8.0_144\jre\bin\server\jvm.dll&quot;
</code></pre>
<p>The library may still fail to load at runtime with the following error:</p>
<blockquote>
<p>&quot;The specified module could not be found.&quot;</p>
</blockquote>
<p>If this is the case, and the path to <code>jvm.dll</code> is verified to be correct, then
the error message actually indicates that the dependencies of <code>jvm.dll</code> could
not be found. On Windows, the DLL search path includes the environment variable
<code>PATH</code>, so add the <code>bin</code> folder which contains <code>server\jvm.dll</code> to <code>PATH</code>:</p>
<pre><code>$env:PATH += &quot;;C:\Program Files\Java\jdk1.8.0_144\jre\bin&quot;
</code></pre>
<h3 id="building-with-openssl"><a class="header" href="#building-with-openssl">Building with OpenSSL</a></h3>
<p>When building with OpenSSL on Windows, you must build or install a distribution
of OpenSSL for Windows. A commonly chosen distribution is
<a href="https://slproweb.com/products/Win32OpenSSL.html">Shining Light Productions' OpenSSL</a>.</p>
<p>As of this writing, OpenSSL 1.1.x is supported.</p>
<p>Use <code>-DENABLE_SSL=ON</code> to build with OpenSSL.</p>
<p>Note that it will link to OpenSSL dynamically, so if the built executables are
deployed elsewhere, that machine also needs OpenSSL installed.</p>
<p>Beware that the OpenSSL installation, nor Mesos itself, comes with a certificate
bundle, and so it is likely that certificate verification will fail.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-runtime-configuration-1"><a class="header" href="#mesos-runtime-configuration-1">Mesos Runtime Configuration</a></h1>
<p>The Mesos master and agent can take a variety of configuration options
through command-line arguments or environment variables. A list of the
available options can be seen by running <code>mesos-master --help</code> or
<code>mesos-agent --help</code>. Each option can be set in two ways:</p>
<ul>
<li>
<p>By passing it to the binary using <code>--option_name=value</code>, either
specifying the value directly, or specifying a file in which the value
resides (<code>--option_name=file://path/to/file</code>). The path can be
absolute or relative to the current working directory.</p>
</li>
<li>
<p>By setting the environment variable <code>MESOS_OPTION_NAME</code> (the option
name with a <code>MESOS_</code> prefix added to it).</p>
</li>
</ul>
<p>Configuration values are searched for first in the environment, then
on the command-line.</p>
<p>Additionally, this documentation lists only a recent snapshot of the options in
Mesos. A definitive source for which flags your version of Mesos supports can be
found by running the binary with the flag <code>--help</code>, for example <code>mesos-master --help</code>.</p>
<h2 id="master-and-agent-options-1"><a class="header" href="#master-and-agent-options-1">Master and Agent Options</a></h2>
<p><em>These are options common to both the Mesos master and agent.</em></p>
<p>See <a href="configuration/master-and-agent.html">configuration/master-and-agent.md</a>.</p>
<h2 id="master-options-1"><a class="header" href="#master-options-1">Master Options</a></h2>
<p>See <a href="configuration/master.html">configuration/master.md</a>.</p>
<h2 id="agent-options-1"><a class="header" href="#agent-options-1">Agent Options</a></h2>
<p>See <a href="configuration/agent.html">configuration/agent.md</a>.</p>
<h2 id="libprocess-options-1"><a class="header" href="#libprocess-options-1">Libprocess Options</a></h2>
<p>See <a href="configuration/libprocess.html">configuration/libprocess.md</a>.</p>
<h1 id="mesos-build-configuration-1"><a class="header" href="#mesos-build-configuration-1">Mesos Build Configuration</a></h1>
<h2 id="autotools-options-1"><a class="header" href="#autotools-options-1">Autotools Options</a></h2>
<p>If you have special compilation requirements, please refer to <code>./configure --help</code> when configuring Mesos.</p>
<p>See <a href="configuration/autotools.html">configuration/autotools.md</a>.</p>
<h2 id="cmake-options-1"><a class="header" href="#cmake-options-1">CMake Options</a></h2>
<p>See <a href="configuration/cmake.html">configuration/cmake.md</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-high-availability-mode"><a class="header" href="#mesos-high-availability-mode">Mesos High-Availability Mode</a></h1>
<p>If the Mesos master is unavailable, existing tasks can continue to execute, but new resources cannot be allocated and new tasks cannot be launched. To reduce the chance of this situation occurring, Mesos has a high-availability mode that uses multiple Mesos masters: one active master (called the <em>leader</em> or leading master) and several <em>backups</em> in case it fails. The masters elect the leader, with <a href="http://zookeeper.apache.org/">Apache ZooKeeper</a> both coordinating the election and handling leader detection by masters, agents, and scheduler drivers. More information regarding <a href="https://zookeeper.apache.org/doc/current/recipes.html#sc_leaderElection">how leader election works</a> is available on the Apache Zookeeper website.</p>
<p>This document describes how to configure Mesos to run in high-availability mode. For more information on developing highly available frameworks, see a <a href="high-availability-framework-guide.html">companion document</a>.</p>
<p><strong>Note</strong>: This document assumes you know how to start, run, and work with ZooKeeper, whose client library is included in the standard Mesos build.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>To put Mesos into high-availability mode:</p>
<ol>
<li>
<p>Ensure that the ZooKeeper cluster is up and running.</p>
</li>
<li>
<p>Provide the znode path to all masters, agents, and framework schedulers as follows:</p>
<ul>
<li>
<p>Start the mesos-master binaries using the <code>--zk</code> flag, e.g. <code>--zk=zk://host1:port1,host2:port2,.../path</code></p>
</li>
<li>
<p>Start the mesos-agent binaries with <code>--master=zk://host1:port1,host2:port2,.../path</code></p>
</li>
<li>
<p>Start any framework schedulers using the same <code>zk</code> path as in the last two steps. The SchedulerDriver must be constructed with this path, as shown in the <a href="app-framework-development-guide.html">Framework Development Guide</a>.</p>
</li>
</ul>
</li>
</ol>
<p>From now on, the Mesos masters and agents all communicate with ZooKeeper to find out which master is the current leading master. This is in addition to the usual communication between the leading master and the agents.</p>
<p>In addition to ZooKeeper, one can get the location of the leading master by sending an HTTP request to <a href="endpoints/master/redirect.html">/redirect</a> endpoint on any master.</p>
<p>For HTTP endpoints that only work at the leading master, requests made to endpoints at a non-leading master will result in either a <code>307 Temporary Redirect</code> (with the location of the leading master) or <code>503 Service Unavailable</code> (if the master does not know who the current leader is).</p>
<p>Refer to the <a href="app-framework-development-guide.html">Scheduler API</a> for how to deal with leadership changes.</p>
<h2 id="component-disconnection-handling"><a class="header" href="#component-disconnection-handling">Component Disconnection Handling</a></h2>
<p>When a network partition disconnects a component (master, agent, or scheduler driver) from ZooKeeper, the component's Master Detector induces a timeout event. This notifies the component that it has no leading master. Depending on the component, the following happens. (Note that while a component is disconnected from ZooKeeper, a master may still be in communication with agents or schedulers and vice versa.)</p>
<ul>
<li>
<p>Agents disconnected from ZooKeeper no longer know which master is the leader. They ignore messages from masters to ensure they don't act on a non-leader's decisions. When an agent reconnects to ZooKeeper, ZooKeeper informs it of the current leader and the agent stops ignoring messages from the leader.</p>
</li>
<li>
<p>Masters enter leaderless state irrespective of whether they are a leader or not before the disconnection.</p>
<ul>
<li>
<p>If the leader was disconnected from ZooKeeper, it aborts its process. The user/developer/administrator can then start a new master instance which will try to reconnect to ZooKeeper.</p>
<ul>
<li>Note that many production deployments of Mesos use a process supervisor (such as systemd or supervisord) that is configured to automatically restart the Mesos master if the process aborts unexpectedly.</li>
</ul>
</li>
<li>
<p>Otherwise, the disconnected backup waits to reconnect with ZooKeeper and possibly get elected as the new leading master.</p>
</li>
</ul>
</li>
<li>
<p>Scheduler drivers disconnected from the leading master notify the scheduler about their disconnection from the leader.</p>
</li>
</ul>
<p>When a network partition disconnects an agent from the leader:</p>
<ul>
<li>
<p>The agent fails health checks from the leader.</p>
</li>
<li>
<p>The leader marks the agent as deactivated and sends its tasks to the LOST state. The  <a href="app-framework-development-guide.html">Framework Development Guide</a> describes these various task states.</p>
</li>
<li>
<p>Deactivated agents may not reregister with the leader and are told to shut down upon any post-deactivation communication.</p>
</li>
</ul>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<p>For monitoring the current number of masters in the cluster communicating with each other to form a quorum, see the monitoring guide's <a href="monitoring.html#replicated-log">Replicated Log</a> on <code>registrar/log/ensemble_size</code>.
For creating alerts covering failures in leader election, have a look at the monitoring guide's <a href="monitoring.html#basic-alerts">Basic Alerts</a> on <code>master/elected</code>.</p>
<h2 id="implementation-details-1"><a class="header" href="#implementation-details-1">Implementation Details</a></h2>
<p>Mesos implements two levels of ZooKeeper leader election abstractions, one in <code>src/zookeeper</code> and the other in <code>src/master</code> (look for <code>contender|detector.hpp|cpp</code>).</p>
<ul>
<li>
<p>The lower level <code>LeaderContender</code> and <code>LeaderDetector</code> implement a generic ZooKeeper election algorithm loosely modeled after this
<a href="http://zookeeper.apache.org/doc/current/recipes.html#sc_leaderElection">recipe</a> (sans herd effect handling due to the master group's small size, which is often 3).</p>
</li>
<li>
<p>The higher level <code>MasterContender</code> and <code>MasterDetector</code> wrap around ZooKeeper's contender and detector abstractions as adapters to provide/interpret the ZooKeeper data.</p>
</li>
<li>
<p>Each Mesos master simultaneously uses both a contender and a detector to try to elect themselves and detect who the current leader is. A separate detector is necessary because each master's WebUI redirects browser traffic to the current leader when that master is not elected. Other Mesos components (i.e., agents and scheduler drivers) use the detector to find the current leader and connect to it.</p>
</li>
</ul>
<p>The notion of the group of leader candidates is implemented in <code>Group</code>. This abstraction handles reliable (through queues and retries of retryable errors under the covers) ZooKeeper group membership registration, cancellation, and monitoring. It watches for several ZooKeeper session events:</p>
<ul>
<li>Connection</li>
<li>Reconnection</li>
<li>Session Expiration</li>
<li>ZNode creation, deletion, updates</li>
</ul>
<p>We also explicitly timeout our sessions when disconnected from ZooKeeper for a specified amount of time. See <code>--zk_session_timeout</code> configuration option. This is because the ZooKeeper client libraries only notify of session expiration upon reconnection. These timeouts are of particular interest for network partitions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-mesos-replicated-log"><a class="header" href="#the-mesos-replicated-log">The Mesos Replicated Log</a></h1>
<p>Mesos provides a library that lets you create replicated fault-tolerant append-only logs; this library is known as the <em>replicated log</em>. The Mesos master uses this library to store cluster state in a replicated, durable way; the library is also available for use by frameworks to store replicated framework state or to implement the common &quot;<a href="https://en.wikipedia.org/wiki/State_machine_replication">replicated state machine</a>&quot; pattern.</p>
<h2 id="what-is-the-replicated-log"><a class="header" href="#what-is-the-replicated-log">What is the replicated log?</a></h2>
<p><img src="images/log-cluster.png" alt="Aurora and the Replicated Log" /></p>
<p>The replicated log provides <em>append-only</em> storage of <em>log entries</em>; each log entry can contain arbitrary data. The log is <em>replicated</em>, which means that each log entry has multiple copies in the system. Replication provides both fault tolerance and high availability. In the following example, we use <a href="https://aurora.apache.org/">Apache Aurora</a>, a fault tolerant scheduler (i.e., framework) running on top of Mesos, to show a typical replicated log setup.</p>
<p>As shown above, there are multiple Aurora instances running simultaneously (for high availability), with one elected as the leader. There is a log replica on each host running Aurora. Aurora can access the replicated log through a thin library containing the log API.</p>
<p>Typically, the leader is the only one that appends data to the log. Each log entry is replicated and sent to all replicas in the system. Replicas are strongly consistent. In other words, all replicas agree on the value of each log entry. Because the log is replicated, when Aurora decides to failover, it does not need to copy the log from a remote host.</p>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<p>The replicated log can be used to build a wide variety of distributed applications. For example, Aurora uses the replicated log to store all task states and job configurations. The Mesos master's <em>registry</em> also leverages the replicated log to store information about all agents in the cluster.</p>
<p>The replicated log is often used to allow applications to manage replicated state in a strongly consistent way. One way to do this is to store a state-mutating operation in each log entry and have all instances of the distributed application agree on the same initial state (e.g., empty state). The replicated log ensures that each application instance will observe the same sequence of log entries in the same order; as long as applying a state-mutating operation is deterministic, this ensures that all application instances will remain consistent with one another. If any instance of the application crashes, it can reconstruct the current version of the replicated state by starting at the initial state and re-applying all the logged mutations in order.</p>
<p>If the log grows too large, an application can write out a snapshot and then delete all the log entries that occurred before the snapshot. Using this approach, we will be exposing a <a href="https://github.com/apache/mesos/blob/master/src/state/state.hpp">distributed state</a> abstraction in Mesos with replicated log as a backend.</p>
<p>Similarly, the replicated log can be used to build <a href="https://en.wikipedia.org/wiki/State_machine_replication">replicated state machines</a>. In this scenario, each log entry contains a state machine command. Since replicas are strongly consistent, all servers will execute the same commands in the same order.</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p><img src="images/log-architecture.png" alt="Replicated Log Architecture" /></p>
<p>The replicated log uses the <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29">Paxos consensus algorithm</a> to ensure that all replicas agree on every log entry's value. It is similar to what's described in <a href="https://ramcloud.stanford.edu/~ongaro/userstudy/paxos.pdf">these slides</a>. Readers who are familiar with Paxos can skip this section.</p>
<p>The above figure is an implementation overview. When a user wants to append data to the log, the system creates a log writer. The log writer internally creates a coordinator. The coordinator contacts all replicas and executes the Paxos algorithm to make sure all replicas agree about the appended data. The coordinator is sometimes referred to as the <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"><em>proposer</em></a>.</p>
<p>Each replica keeps an array of log entries. The array index is the log position. Each log entry is composed of three components: the value written by the user, the associated Paxos state and a <em>learned</em> bit where true means this log entry's value has been agreed. Therefore, a replica in our implementation is both an <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"><em>acceptor</em></a> and a <a href="https://en.wikipedia.org/wiki/Paxos_%28computer_science%29"><em>learner</em></a>.</p>
<h3 id="reaching-consensus-for-a-single-log-entry"><a class="header" href="#reaching-consensus-for-a-single-log-entry">Reaching consensus for a single log entry</a></h3>
<p>A Paxos round can help all replicas reach consensus on a single log entry's value. It has two phases: a promise phase and a write phase. Note that we are using slightly different terminology from the <a href="https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">original Paxos paper</a>. In our implementation, the <em>prepare</em> and <em>accept</em> phases in the original paper are referred to as the <em>promise</em> and <em>write</em> phases, respectively. Consequently, a prepare request (response) is referred to as a promise request (response), and an accept request (response) is referred to as a write request (response).</p>
<p>To append value <em>X</em> to the log at position <em>p</em>, the coordinator first broadcasts a promise request to all replicas with proposal number <em>n</em>, asking replicas to promise that they will not respond to any request (promise/write request) with a proposal number lower than <em>n</em>. We assume that <em>n</em> is higher than any other previously used proposal number, and will explain how we do this later.</p>
<p>When receiving the promise request, each replica checks its Paxos state to decide if it can safely respond to the request, depending on the promises it has previously given out. If the replica is able to give the promise (i.e., passes the proposal number check), it will first persist its promise (the proposal number <em>n</em>) on disk and reply with a promise response. If the replica has been previously written (i.e., accepted a write request), it needs to include the previously written value along with the proposal number used in that write request into the promise response it's about to send out.</p>
<p>Upon receiving promise responses from a <a href="https://en.wikipedia.org/wiki/Quorum_%28distributed_computing%29">quorum</a> of replicas, the coordinator first checks if there exist any previously written value from those responses. The append operation cannot continue if a previously written value is found because it's likely that a value has already been agreed on for that log entry. This is one of the key ideas in Paxos: restrict the value that can be written to ensure consistency.</p>
<p>If no previous written value is found, the coordinator broadcasts a write request to all replicas with value <em>X</em> and proposal number <em>n</em>. On receiving the write request, each replica checks the promise it has given again, and replies with a write response if the write request's proposal number is equal to or larger than the proposal number it has promised. Once the coordinator receives write responses from a quorum of replicas, the append operation succeeds.</p>
<h3 id="optimizing-append-latency-using-multi-paxos"><a class="header" href="#optimizing-append-latency-using-multi-paxos">Optimizing append latency using Multi-Paxos</a></h3>
<p>One naive solution to implement a replicated log is to run a full Paxos round (promise phase and write phase) for each log entry. As discussed in the <a href="https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">original Paxos paper</a>, if the leader is relatively stable, <em>Multi-Paxos</em> can be used to eliminate the need for the promise phase for most of the append operations, resulting in improved performance.</p>
<p>To do that, we introduce a new type of promise request called an <em>implicit</em> promise request. An implicit promise request can be viewed as a <em>batched</em> promise request for a (potentially infinite) set of log entries. Broadcasting an implicit promise request is conceptually equivalent to broadcasting a promise request for every log entry whose value has not yet been agreed. If the implicit promise request broadcasted by a coordinator gets accepted by a quorum of replicas, this coordinator is no longer required to run the promise phase if it wants to append to a log entry whose value has not yet been agreed because the promise phase has already been done in <em>batch</em>. The coordinator in this case is therefore called <em>elected</em> (a.k.a., the leader), and has <em>exclusive</em> access to the replicated log. An elected coordinator may be <em>demoted</em> (or lose exclusive access) if another coordinator broadcasts an implicit promise request with a higher proposal number.</p>
<p>One question remaining is how can we find out those log entries whose values have not yet been agreed. We have a very simple solution: if a replica accepts an implicit promise request, it will include its largest known log position in the response. An elected coordinator will only append log entries at positions larger than <em>p</em>, where <em>p</em> is greater than any log position seen in these responses.</p>
<p>Multi-Paxos has better performance if the leader is stable. The replicated log itself does not perform leader election. Instead, we rely on the user of the replicated log to choose a stable leader. For example, Aurora uses <a href="https://zookeeper.apache.org/">ZooKeeper</a> to elect the leader.</p>
<h3 id="enabling-local-reads"><a class="header" href="#enabling-local-reads">Enabling local reads</a></h3>
<p>As discussed above, in our implementation, each replica is both an acceptor and a learner. Treating each replica as a learner allows us to do local reads without involving other replicas. When a log entry's value has been agreed, the coordinator will broadcast a <em>learned</em> message to all replicas. Once a replica receives the learned message, it will set the learned bit in the corresponding log entry, indicating the value of that log entry has been agreed. We say a log entry is &quot;learned&quot; if its learned bit is set. The coordinator does not have to wait for replicas' acknowledgments.</p>
<p>To perform a read, the log reader will directly look up the underlying local replica. If the corresponding log entry is learned, the reader can just return the value to the user. Otherwise, a full Paxos round is needed to discover the agreed value. We always make sure that the replica co-located with the elected coordinator always has all log entries learned. We achieve that by running full Paxos rounds for those unlearned log entries after the coordinator is elected.</p>
<h3 id="reducing-log-size-using-garbage-collection"><a class="header" href="#reducing-log-size-using-garbage-collection">Reducing log size using garbage collection</a></h3>
<p>In case the log grows large, the application has the choice to truncate the log. To perform a truncation, we append a special log entry whose value is the log position to which the user wants to truncate the log. A replica can actually truncate the log once this special log entry has been learned.</p>
<h3 id="unique-proposal-number"><a class="header" href="#unique-proposal-number">Unique proposal number</a></h3>
<p>Many of the <a href="https://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">Paxos research papers</a> assume that each proposal number is globally unique, and a coordinator can always come up with a proposal number that is larger than any other proposal numbers in the system. However, implementing this is not trivial, especially in a distributed environment. <a href="https://ramcloud.stanford.edu/~ongaro/userstudy/paxos.pdf">Some researchers suggest</a> concatenating a globally unique server id to each proposal number. But it is still not clear how to generate a globally unique id for each server.</p>
<p>Our solution does not make the above assumptions. A coordinator can use an arbitrary proposal number initially. During the promise phase, if a replica knows a proposal number higher than the proposal number used by the coordinator, it will send the largest known proposal number back to the coordinator. The coordinator will retry the promise phase with a higher proposal number.</p>
<p>To avoid livelock (e.g., when two coordinators completing), we inject a randomly delay between T and 2T before each retry. T has to be chosen carefully. On one hand, we want T &gt;&gt; broadcast time such that one coordinator usually times out and wins before others wake up. On the other hand, we want T to be as small as possible such that we can reduce the wait time. Currently, we use T = 100ms. This idea is actually borrowed from <a href="https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">Raft</a>.</p>
<h2 id="automatic-replica-recovery"><a class="header" href="#automatic-replica-recovery">Automatic replica recovery</a></h2>
<p>The algorithm described above has a critical vulnerability: if a replica loses its durable state (i.e., log files) due to either disk failure or operational error, that replica may cause inconsistency in the log if it is simply restarted and re-added to the group. The operator needs to stop the application on all hosts, copy the log files from the leader's host, and then restart the application. Note that the operator cannot copy the log files from an arbitrary replica because copying an unlearned log entry may falsely assemble a quorum for an incorrect value, leading to inconsistency.</p>
<p>To avoid the need for operator intervention in this situation, the Mesos replicated log includes support for <em>auto recovery</em>. As long as a quorum of replicas is working properly, the users of the application won't notice any difference.</p>
<h3 id="non-voting-replicas"><a class="header" href="#non-voting-replicas">Non-voting replicas</a></h3>
<p>To enable auto recovery, a key insight is that a replica that loses its durable state should not be allowed to respond to requests from coordinators after restart. Otherwise, it may introduce inconsistency in the log as it could have accepted a promise/write request which it would not have accepted if its previous Paxos state had not been lost.</p>
<p>To solve that, we introduce a new status variable for each replica. A normal replica is said in VOTING status, meaning that it is allowed to respond to requests from coordinators. A replica with no persisted state is put in EMPTY status by default. A replica in EMPTY status is not allowed to respond to any request from coordinators.</p>
<p>A replica in EMPTY status will be promoted to VOTING status if the following two conditions are met:</p>
<ol>
<li>a sufficient amount of missing log entries are recovered such that if other replicas fail, the remaining replicas can recover all the learned log entries, and</li>
<li>its future responses to a coordinator will not break any of the promises (potentially lost) it has given out.</li>
</ol>
<p>In the following, we discuss how we achieve these two conditions.</p>
<h3 id="catch-up"><a class="header" href="#catch-up">Catch-up</a></h3>
<p>To satisfy the above two conditions, a replica needs to perform <em>catch-up</em> to recover lost states. In other words, it will run Paxos rounds to find out those log entries whose values that have already been agreed. The question is how many log entries the local replica should catch-up before the above two conditions can be satisfied.</p>
<p>We found that it is sufficient to catch-up those log entries from position <em>begin</em> to position <em>end</em> where <em>begin</em> is the smallest position seen in a quorum of VOTING replicas and <em>end</em> is the largest position seen in a quorum of VOTING replicas.</p>
<p>Here is our correctness argument. For a log entry at position <em>e</em> where <em>e</em> is larger than <em>end</em>, obviously no value has been agreed on. Otherwise, we should find at least one VOTING replica in a quorum of replicas such that its end position is larger than <em>end</em>. For the same reason, a coordinator should not have collected enough promises for the log entry at position <em>e</em>. Therefore, it's safe for the recovering replica to respond requests for that log entry. For a log entry at position <em>b</em> where <em>b</em> is smaller than <em>begin</em>, it should have already been truncated and the truncation should have already been agreed. Therefore, allowing the recovering replica to respond requests for that position is also safe.</p>
<h3 id="auto-initialization"><a class="header" href="#auto-initialization">Auto initialization</a></h3>
<p>Since we don't allow an empty replica (a replica in EMPTY status) to respond to requests from coordinators, that raises a question for bootstrapping because initially, each replica is empty. The replicated log provides two choices here. One choice is to use a tool (<code>mesos-log</code>) to explicitly initialize the log on each replica by setting the replica's status to VOTING, but that requires an extra step when setting up an application.</p>
<p>The other choice is to do automatic initialization. Our idea is: we allow a replica in EMPTY status to become VOTING immediately if it finds all replicas are in EMPTY status. This is based on the assumption that the only time <em>all</em> replicas are in EMPTY status is during start-up. This may not be true if a catastrophic failure causes all replicas to lose their durable state, and that's exactly the reason we allow conservative users to disable auto-initialization.</p>
<p>To do auto-initialization, if we use a single-phase protocol and allow a replica to directly transit from EMPTY status to VOTING status, we may run into a state where we cannot make progress even if all replicas are in EMPTY status initially. For example, say the quorum size is 2. All replicas are in EMPTY status initially. One replica will first set its status to VOTING because if finds all replicas are in EMPTY status. After that, neither the VOTING replica nor the EMPTY replicas can make progress. To solve this problem, we use a two-phase protocol and introduce an intermediate transient status (STARTING) between EMPTY and VOTING status. A replica in EMPTY status can transit to STARTING status if it finds all replicas are in either EMPTY or STARTING status. A replica in STARTING status can transit to VOTING status if it finds all replicas are in either STARTING or VOTING status. In that way, in our previous example, all replicas will be in STARTING status before any of them can transit to VOTING status.</p>
<h2 id="non-leading-voting-replica-catch-up"><a class="header" href="#non-leading-voting-replica-catch-up">Non-leading VOTING replica catch-up</a></h2>
<p>Starting with Mesos 1.5.0 it is possible to perform eventually consistent reads from a non-leading VOTING log replica. This makes possible to do additional work on non-leading framework replicas, e.g. offload some reading from a leader to standbys reduce failover time by keeping in-memory storage represented by the replicated log &quot;hot&quot;.</p>
<p>To serve eventually consistent reads a replica needs to perform <em>catch-up</em> to recover the latest log state in a manner similar to how it is done during <a href="replicated-log-internals.html#catch-up">EMPTY replica recovery</a>. After that the recovered positions can be replayed without fear of seeing &quot;holes&quot;.</p>
<p>A truncation can take place during the non-leading replica catch-up. The replica may try to fill the truncated position if truncation happens after the replica has recovered <em>begin</em> and <em>end</em> positions, which may lead to producing inconsistent data during log replay. In order to protect against it we use a special tombstone flag that signals to the replica that the position was truncated and <em>begin</em> needs to be adjusted. The replica is not blocked from truncations during or after catching-up, which means that the user may need to retry the catch-up procedure if positions that were recovered became truncated during log replay.</p>
<h2 id="future-work"><a class="header" href="#future-work">Future work</a></h2>
<p>Currently, replicated log does not support dynamic quorum size change, also known as <em>reconfiguration</em>. Supporting reconfiguration would allow us more easily to add, move or swap hosts for replicas. We plan to support reconfiguration in the future.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="agent-recovery"><a class="header" href="#agent-recovery">Agent Recovery</a></h1>
<p>If the <code>mesos-agent</code> process on a host exits (perhaps due to a Mesos bug or
because the operator kills the process while <a href="upgrades.html">upgrading Mesos</a>),
any executors/tasks that were being managed by the <code>mesos-agent</code> process will
continue to run.</p>
<p>By default, all the executors/tasks that were being managed by the old
<code>mesos-agent</code> process are expected to gracefully exit on their own, and
will be shut down after the agent restarted if they did not.</p>
<p>However, if a framework enabled  <em>checkpointing</em> when it registered with the
master, any executors belonging to that framework can reconnect to the new
<code>mesos-agent</code> process and continue running uninterrupted. Hence, enabling
framework checkpointing allows tasks to tolerate Mesos agent upgrades and
unexpected <code>mesos-agent</code> crashes without experiencing any downtime.</p>
<p>Agent recovery works by having the agent checkpoint information about its own
state and about the tasks and executors it is managing to local disk, for
example the <code>SlaveInfo</code>, <code>FrameworkInfo</code> and <code>ExecutorInfo</code> messages or the
unacknowledged status updates of running tasks.</p>
<p>When the agent restarts, it will verify that its current configuration, set
from the environment variables and command-line flags, is compatible with the
checkpointed information and will refuse to restart if not.</p>
<p>A special case occurs when the agent detects that its host system was rebooted
since the last run of the agent: The agent will try to recover its previous ID
as usual, but if that fails it will actually erase the information of the
previous run and will register with the master as a new agent.</p>
<p>Note that executors and tasks that exited between agent shutdown and restart
are not automatically restarted during agent recovery.</p>
<h2 id="framework-configuration"><a class="header" href="#framework-configuration">Framework Configuration</a></h2>
<p>A framework can control whether its executors will be recovered by setting
the <code>checkpoint</code> flag in its <code>FrameworkInfo</code> when registering with the master.
Enabling this feature results in increased I/O overhead at each agent that runs
tasks launched by the framework. By default, frameworks do <strong>not</strong> checkpoint
their state.</p>
<h2 id="agent-configuration"><a class="header" href="#agent-configuration">Agent Configuration</a></h2>
<p>Four <a href="configuration/agent.html">configuration flags</a> control the recovery
behavior of a Mesos agent:</p>
<ul>
<li>
<p><code>strict</code>: Whether to do agent recovery in strict mode [Default: true].</p>
<ul>
<li>If strict=true, all recovery errors are considered fatal.</li>
<li>If strict=false, any errors (e.g., corruption in checkpointed data) during
recovery are ignored and as much state as possible is recovered.</li>
</ul>
</li>
<li>
<p><code>reconfiguration_policy</code>: Which kind of configuration changes are accepted
when trying to recover [Default: equal].</p>
<ul>
<li>If reconfiguration_policy=equal, no configuration changes are accepted.</li>
<li>If reconfiguration_policy=additive, the agent will allow the new
configuration to contain additional attributes, increased resourced or an
additional fault domain. For a more detailed description, see
<a href="https://gitbox.apache.org/repos/asf?p=mesos.git;a=blob;f=src/slave/compatibility.hpp;h=78b421a01abe5d2178c93832577577a7ba282b38;hb=HEAD#l37">this</a>.</li>
</ul>
</li>
<li>
<p><code>recover</code>: Whether to recover status updates and reconnect with old
executors [Default: reconnect]</p>
<ul>
<li>If recover=reconnect, reconnect with any old live executors, provided
the executor's framework enabled checkpointing.</li>
<li>If recover=cleanup, kill any old live executors and exit. Use this
option when doing an incompatible agent or executor upgrade!
<strong>NOTE:</strong> If no checkpointing information exists, no recovery is performed
and the agent registers with the master as a new agent.</li>
</ul>
</li>
<li>
<p><code>recovery_timeout</code>: Amount of time allotted for the agent to
recover [Default: 15 mins].</p>
<ul>
<li>If the agent takes longer than <code>recovery_timeout</code> to recover, any
executors that are waiting to reconnect to the agent will self-terminate.
<strong>NOTE:</strong> If none of the frameworks have enabled checkpointing, the
executors and tasks running at an agent die when the agent dies and are
not recovered.</li>
</ul>
</li>
</ul>
<p>A restarted agent should reregister with master within a timeout (75 seconds
by default: see the <code>--max_agent_ping_timeouts</code> and <code>--agent_ping_timeout</code>
<a href="configuration.html">configuration flags</a>). If the agent takes longer than this
timeout to reregister, the master shuts down the agent, which in turn will
shutdown any live executors/tasks.</p>
<p>Therefore, it is highly recommended to automate the process of restarting an
agent, e.g. using a process supervisor such as <a href="http://mmonit.com/monit/">monit</a>
or <code>systemd</code>.</p>
<h2 id="known-issues-with-systemd-and-process-lifetime"><a class="header" href="#known-issues-with-systemd-and-process-lifetime">Known issues with <code>systemd</code> and process lifetime</a></h2>
<p>There is a known issue when using <code>systemd</code> to launch the <code>mesos-agent</code>. A
description of the problem can be found in <a href="https://issues.apache.org/jira/browse/MESOS-3425">MESOS-3425</a>
and all relevant work can be tracked in the epic <a href="https://issues.apache.org/jira/browse/MESOS-3007">MESOS-3007</a>.</p>
<p>This problem was fixed in Mesos <code>0.25.0</code> for the mesos containerizer when
cgroups isolation is enabled. Further fixes for the posix isolators and docker
containerizer are available in <code>0.25.1</code>, <code>0.26.1</code>, <code>0.27.1</code>, and <code>0.28.0</code>.</p>
<p>It is recommended that you use the default <a href="http://www.freedesktop.org/software/systemd/man/systemd.kill.html">KillMode</a>
for systemd processes, which is <code>control-group</code>, which kills all child processes
when the agent stops. This ensures that &quot;side-car&quot; processes such as the
<code>fetcher</code> and <code>perf</code> are terminated alongside the agent.
The systemd patches for Mesos explicitly move executors and their children into
a separate systemd slice, dissociating their lifetime from the agent. This
ensures the executors survive agent restarts.</p>
<p>The following excerpt of a <code>systemd</code> unit configuration file shows how to set
the flag explicitly:</p>
<pre><code>[Service]
ExecStart=/usr/bin/mesos-agent
KillMode=control-cgroup
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="framework-rate-limiting"><a class="header" href="#framework-rate-limiting">Framework Rate Limiting</a></h1>
<p>Framework rate limiting is a feature introduced in Mesos 0.20.0.</p>
<h2 id="what-is-framework-rate-limiting"><a class="header" href="#what-is-framework-rate-limiting">What is Framework Rate Limiting</a></h2>
<p>In a multi-framework environment, this feature aims to protect the throughput of high-SLA (e.g., production, service) frameworks by having the master throttle messages from other (e.g., development, batch) frameworks.</p>
<p>To throttle messages from a framework, the Mesos cluster operator sets a <code>qps</code> (queries per seconds) value for each framework identified by its principal (You can also throttle a group of frameworks together but we'll assume individual frameworks in this doc unless otherwise stated; see the <code>RateLimits</code> <a href="https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto">Protobuf definition</a> and the configuration notes below). The master then promises not to process messages from that framework at a rate above <code>qps</code>. The outstanding messages are stored in memory on the master.</p>
<h2 id="rate-limits-configuration"><a class="header" href="#rate-limits-configuration">Rate Limits Configuration</a></h2>
<p>The following is a sample config file (in JSON format) which could be specified with the <code>--rate_limits</code> master flag.</p>
<pre><code>{
  &quot;limits&quot;: [
    {
      &quot;principal&quot;: &quot;foo&quot;,
      &quot;qps&quot;: 55.5
      &quot;capacity&quot;: 100000
    },
    {
      &quot;principal&quot;: &quot;bar&quot;,
      &quot;qps&quot;: 300
    },
    {
      &quot;principal&quot;: &quot;baz&quot;,
    }
  ],
  &quot;aggregate_default_qps&quot;: 333,
  &quot;aggregate_default_capacity&quot;: 1000000
}
</code></pre>
<p>In this example, framework <code>foo</code> is throttled at the configured <code>qps</code> and <code>capacity</code>, framework <code>bar</code> is given unlimited capacity and framework <code>baz</code> is not throttled at all. If there is a fourth framework <code>qux</code> or a framework without a principal connected to the master, it is throttled by the rules <code>aggregate_default_qps</code> and <code>aggregate_default_capacity</code>.</p>
<h3 id="configuration-notes"><a class="header" href="#configuration-notes">Configuration Notes</a></h3>
<p>Below are the fields in the JSON configuration.</p>
<ul>
<li><strong>principal</strong>: (Required) uniquely identifies the entity being throttled or given unlimited rate explicitly.
<ul>
<li>It should match the framework's <code>FrameworkInfo.principal</code> (See <a href="https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto">definition</a>).</li>
<li>You can have multiple frameworks use the same principal (e.g., some Mesos frameworks launch a new framework instance for each job), in which case the combined traffic from all frameworks using the same principal are throttled at the specified QPS.</li>
</ul>
</li>
<li><strong>qps</strong>: (Optional) queries per second, i.e., the rate.
<ul>
<li>Once set, the master guarantees that it does not process messages from this principal higher than this rate. However the master could be slower than this rate, especially if the specified rate is too high.</li>
<li>To explicitly give a framework unlimited rate (i.e., not throttling it), add an entry to <code>limits</code> without the qps.</li>
</ul>
</li>
<li><strong>capacity</strong>: (Optional) The number of <em>outstanding</em> messages frameworks of this principal can put on the master. If not specified, this principal is given unlimited capacity. Note that it is possible the queued messages use too much memory and cause the master to OOM if the capacity is set too high or not set.
<ul>
<li>NOTE: If <code>qps</code> is not specified, <code>capacity</code> is ignored.</li>
</ul>
</li>
<li>Use <strong>aggregate_default_qps</strong> and <strong>aggregate_default_capacity</strong> to safeguard the master from unspecified frameworks. All the frameworks not specified in <code>limits</code> get this default rate and capacity.
<ul>
<li>The rate and capacity are aggregate values for all of them, i.e., their combined traffic is throttled together.</li>
<li>Same as above, if <code>aggregate_default_qps</code> is not specified, <code>aggregate_default_capacity</code> is ignored.</li>
<li>If these fields are not present, the unspecified frameworks are not throttled.
This is an implicit way of giving frameworks unlimited rate compared to the explicit way above (using an entry in <code>limits</code> with only the principal).
We recommend using the explicit option especially when the master does not require authentication to prevent unexpected frameworks from overwhelming the master.</li>
</ul>
</li>
</ul>
<h2 id="using-framework-rate-limiting"><a class="header" href="#using-framework-rate-limiting">Using Framework Rate Limiting</a></h2>
<h3 id="monitoring-framework-traffic"><a class="header" href="#monitoring-framework-traffic">Monitoring Framework Traffic</a></h3>
<p>While a framework is registered with the master, the master exposes counters for all messages received and processed from that framework at its metrics endpoint: <code>http://&lt;master&gt;/metrics/snapshot</code>. For instance, framework <code>foo</code> has two message counters <code>frameworks/foo/messages_received</code> and <code>frameworks/foo/messages_processed</code>. Without framework rate limiting the two numbers should differ by little or none (because messages are processed ASAP) but when a framework is being throttled the difference indicates the outstanding messages as a result of the throttling.</p>
<p>By continuously monitoring the counters, you can derive the rate messages arrive and how fast the message queue length for the framework is growing (if it is throttled). This should depict the characteristics of the framework in terms of network traffic.</p>
<h2 id="configuring-rate-limits"><a class="header" href="#configuring-rate-limits">Configuring Rate Limits</a></h2>
<p>Since the goal for framework rate limiting is to prevent low-SLA frameworks from using <strong>too much</strong> resources and not to model their traffic and behavior as precisely as possible, you can start by using large <code>qps</code> values to throttle them. The fact that they are throttled (regardless of the configured <code>qps</code>) is already effective in giving messages from high-SLA frameworks higher priority because they are processed ASAP.</p>
<p>To calculate how much <code>capacity</code> the master can handle, you need to know the memory limit for the master process, the amount of memory it typically uses to serve similar workload without rate limiting (e.g., use <code>ps -o rss $MASTER_PID</code>) and average sizes of the framework messages (queued messages are stored as <a href="https://github.com/apache/mesos/blob/master/3rdparty/libprocess/include/process/message.hpp">serialized Protocol Buffers with a few additional fields</a>) and you should sum up all capacity values in the config.
However since this kind of calculation is imprecise, you should start with small values that tolerate reasonable temporary framework burstiness but far from the memory limit to leave enough headroom for the master and frameworks that don't have limited capacity.</p>
<h2 id="handling-capacity-exceeded-error"><a class="header" href="#handling-capacity-exceeded-error">Handling &quot;Capacity Exceeded&quot; Error</a></h2>
<p>When a framework <strong>exceeds the capacity</strong>, a FrameworkErrorMessage is sent back to the framework which will <a href="https://github.com/apache/mesos/blob/master/src/sched/sched.cpp">abort the scheduler driver and invoke the error() callback</a>. It doesn't kill any tasks or the scheduler itself. The framework developer can choose to restart or failover the scheduler instance to remedy the consequences of dropped messages (unless your framework doesn't assume all messages sent to the master are processed).</p>
<p>After version 0.20.0 we are going to iterate on this feature by having the master send an early alert when the message queue for this framework <strong>starts to build up</strong> (<a href="https://issues.apache.org/jira/browse/MESOS-1664">MESOS-1664</a>, consider it a &quot;soft limit&quot;). The scheduler can react by throttling itself (to avoid the error message) or ignoring this alert if it's a temporary burst by design.</p>
<p>Before the early alerting is implemented we <strong>don't recommend using the rate limiting feature to throttle production frameworks</strong> for now unless you are sure about the consequences of the error message. Of course it's OK to use it to protect production frameworks by throttling other frameworks and it doesn't have any effect on the master if it's not explicitly enabled.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performing-node-maintenance-in-a-mesos-cluster"><a class="header" href="#performing-node-maintenance-in-a-mesos-cluster">Performing Node Maintenance in a Mesos Cluster</a></h1>
<p>Operators regularly need to perform maintenance tasks on machines that comprise
a Mesos cluster.  Most Mesos upgrades can be done without affecting running
tasks, but there are situations where maintenance may affect running tasks.
For example:</p>
<ul>
<li>Hardware repair</li>
<li>Kernel upgrades</li>
<li>Agent upgrades (e.g., adjusting agent attributes or resources)</li>
</ul>
<p>Before performing maintenance on an agent node in a Mesos cluster, it is
typically desirable to gracefully migrate tasks away from the node beforehand in
order to minimize service disruption when the machine is taken down. Mesos
provides several ways to accomplish this migration:</p>
<ul>
<li>Automatic agent draining, which does not explicitly require cooperation from
schedulers</li>
<li>Manual node draining, which allows operators to exercise precise control over
the task draining process</li>
<li>Maintenance primitives, which permit complex coordination but do require that
schedulers react to the maintenance-related messages that they receive</li>
</ul>
<h1 id="automatic-node-draining"><a class="header" href="#automatic-node-draining">Automatic Node Draining</a></h1>
<p>Node draining was added to provide a simple method for operators to drain tasks
from nodes on which they plan to perform maintenance, without requiring that
schedulers implement support for any maintenance-specific messages.</p>
<p>Initiating draining will cause all tasks on the target agent node to receive a
kill event immediately, assuming the agent is currently reachable. If the agent
is unreachable, initiation of the kill event will be delayed until the agent is
reachable by the master again. When the tasks receive a kill event, a SIGTERM
signal will be sent to the task to begin the killing process. Depending on the
particular task's behavior, this signal may be sufficient to terminate it. Some
tasks may use this signal to begin the process of graceful termination, which
may take some time. After some delay, a SIGKILL signal will be sent to the task,
which forcefully terminates the task if it is still running. The delay between
the SIGTERM and SIGKILL signals is determined by the length of the task's kill
grace period. If no grace period is set for the task, a default value of several
seconds will be used.</p>
<h2 id="initiating-draining-on-a-node"><a class="header" href="#initiating-draining-on-a-node">Initiating Draining on a Node</a></h2>
<p>To begin draining an agent, issue the operator API <a href="operator-http-api.html#drain_agent"><code>DRAIN_AGENT</code>
call</a> to the master:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DRAIN_AGENT&quot;, &quot;drain_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}}}' masterhost:5050/api/v1
</code></pre>
<p>This will immediately begin the process of killing all tasks on the agent. Once
draining has begun, it cannot be cancelled. To monitor the progress of the
draining process, you can inspect the state of the agent via the master operator
API <a href="operator-http-api.html#get_state"><code>GET_STATE</code></a> or
<a href="operator-http-api.html#get_agents"><code>GET_AGENTS</code></a> calls:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;GET_AGENTS&quot;}' masterhost:5050/api/v1
</code></pre>
<p>Locate the relevant agent and inspect its <code>drain_info.state</code> field. While
draining, the state will be <code>DRAINING</code>. When all tasks on the agent have
terminated, all their terminal status updates have been acknowledged by the
schedulers, and all offer operations on the agent have finished, draining is
complete and the agent's drain state will transition to <code>DRAINED</code>. At this
point, the node may be taken down for maintenance.</p>
<h2 id="options-for-automatic-node-draining"><a class="header" href="#options-for-automatic-node-draining">Options for Automatic Node Draining</a></h2>
<p>You may set an upper bound on the kill grace period of draining tasks by
specifying the <code>max_grace_period</code> option when draining:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DRAIN_AGENT&quot;, &quot;drain_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}, &quot;max_grace_period&quot;: &quot;10mins&quot;}}' masterhost:5050/api/v1
</code></pre>
<p>In cases where you know that the node being drained will not return after
draining is complete, and you would like it to be automatically permanently
removed from the cluster, you may specify the <code>mark_gone</code> option:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DRAIN_AGENT&quot;, &quot;drain_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}, &quot;mark_gone&quot;: true}}' masterhost:5050/api/v1
</code></pre>
<p>This can be useful, for example, in the case of autoscaled cloud instances,
where an instance is being scaled down and will never return. This is equivalent
to issuing the <a href="operator-http-api.html#mark_agent_gone"><code>MARK_AGENT_GONE</code></a> call on
the agent immediately after it finishes draining. WARNING: draining with the
<code>mark_gone</code> option is irreversible, and results in the loss of all local
persistent data on the agent node. Use this option with caution!</p>
<h2 id="reactivating-a-node-after-maintenance"><a class="header" href="#reactivating-a-node-after-maintenance">Reactivating a Node After Maintenance</a></h2>
<p>Once maintenance on an agent is complete, it must be reactivated so that it can
reregister with the master and rejoin the cluster. You may use the master
operator API <a href="operator-http-api.html#reactivate_agent"><code>REACTIVATE_AGENT</code></a> call to
accomplish this:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;REACTIVATE_AGENT&quot;, &quot;reactivate_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}}}' masterhost:5050/api/v1
</code></pre>
<h1 id="manual-node-draining"><a class="header" href="#manual-node-draining">Manual Node Draining</a></h1>
<p>If you require greater control over the draining process, you may be able to
drain the agent manually using both the Mesos operator API as well as APIs
exposed by the schedulers running tasks on the agent.</p>
<h2 id="deactivating-an-agent"><a class="header" href="#deactivating-an-agent">Deactivating an Agent</a></h2>
<p>The first step in the manual draining process is agent deactivation, which
prevents new tasks from launching on the target agent:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;DEACTIVATE_AGENT&quot;, &quot;deactivate_agent&quot;: {&quot;agent_id&quot;: {&quot;value&quot;: &quot;&lt;mesos-agent-id&gt;&quot;}}}' masterhost:5050/api/v1
</code></pre>
<p>If you receive a <code>200 OK</code> response, then the agent has been deactivated. You can
confirm the deactivation state of any agent by inspecting its <code>deactivated</code>
field in the response of the master operator API
<a href="operator-http-api.html#get_state"><code>GET_STATE</code></a> or
<a href="operator-http-api.html#get_agents"><code>GET_AGENTS</code></a> calls. Once the agent is
deactivated, you can use the APIs exposed by the schedulers responsible for the
tasks running on the agent to kill those tasks manually. To verify that all
tasks on the agent have terminated and their terminal status updates have been
acknowledged by the schedulers, ensure that the <code>pending_tasks</code>, <code>queued_tasks</code>,
and <code>launched_tasks</code> fields in the response to the
<a href="operator-http-api.html#get_tasks-1"><code>GET_TASKS</code></a> agent operator API call are
empty:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;GET_TASKS&quot;}' agenthost:5051/api/v1
</code></pre>
<p>If you are making use of volumes backed by network storage on the target agent,
it's possible that there may be a long-running offer operation on the agent
which has not yet finished. To check if this is the case, issue the agent
operator API <a href="operator-http-api.html#get_operations-1"><code>GET_OPERATIONS</code></a> call to
the agent:</p>
<pre><code>$ curl -X POST -d '{&quot;type&quot;: &quot;GET_OPERATIONS&quot;}' agenthost:5051/api/v1
</code></pre>
<p>If any operations have a <code>latest_status</code> with a state of <code>OPERATION_PENDING</code>,
you should wait for them to finish before taking down the node. Unfortunately,
it is not possible to cancel or forcefully terminate such storage operations. If
such an operation becomes stuck in the pending state, you should inspect the
relevant storage backend for any issues.</p>
<p>Once all tasks on the agent have terminated and all offer operations are
finished, the node may be taken down for maintenance. Once maintenance is
complete, the procedure for reactivating the node is the same as that detailed
in the section on automatic node draining.</p>
<h1 id="maintenance-primitives"><a class="header" href="#maintenance-primitives">Maintenance Primitives</a></h1>
<p>Frameworks require visibility into any actions that disrupt cluster operation
in order to meet Service Level Agreements or to ensure uninterrupted services
for their end users.  Therefore, to reconcile the requirements of frameworks
and operators, frameworks must be aware of planned maintenance events and
operators must be aware of frameworks' ability to adapt to maintenance.
Maintenance primitives add a layer to facilitate communication between the
frameworks and operator.</p>
<h2 id="terminology"><a class="header" href="#terminology">Terminology</a></h2>
<p>For the purpose of this section, an &quot;Operator&quot; is a person, tool, or script
that manages a Mesos cluster.</p>
<p>Maintenance primitives add several new concepts to Mesos. Those concepts are:</p>
<ul>
<li><strong>Maintenance</strong>: An operation that makes resources on a machine unavailable,
either temporarily or permanently.</li>
<li><strong>Maintenance window</strong>: A set of machines and an associated time interval during
which some maintenance is planned on those machines.</li>
<li><strong>Maintenance schedule</strong>: A list of maintenance windows.
A single machine may only appear in a schedule once.</li>
<li><strong>Unavailability</strong>: An operator-specified interval, defined by a start time
and duration, during which an associated machine may become unavailable.
In general, no assumptions should be made about the availability of the
machine (or resources) after the unavailability.</li>
<li><strong>Drain</strong>: An interval between the scheduling of maintenance and when the
machine(s) become unavailable.  Offers sent with resources from draining
machines will contain unavailability information.  Frameworks running on
draining machines will receive inverse offers (see next).  Frameworks
utilizing resources on affected machines are expected either to take
preemptive steps to prepare for the unavailability; or to communicate the
framework's inability to conform to the maintenance schedule.</li>
<li><strong>Inverse offer</strong>: A communication mechanism for the master to ask for
resources back from a framework.  This notifies frameworks about any
unavailability and gives frameworks a mechanism to respond about their
ability to comply.  Inverse offers are similar to offers in that they
can be accepted, declined, re-offered, and rescinded.</li>
</ul>
<p><strong>Note</strong>: Unavailability and inverse offers are not specific to maintenance.
The same concepts can be used for non-maintenance goals, such as reallocating
resources or resource preemption.</p>
<h2 id="how-does-it-work"><a class="header" href="#how-does-it-work">How does it work?</a></h2>
<p>Maintenance primitives were introduced in Mesos 0.25.0.  Several machine
maintenance modes were also introduced.  Those modes are illustrated below.</p>
<p><img src="images/maintenance-primitives-modes.png" alt="Maintenance mode transitions" /></p>
<p>All mode transitions must be initiated by the operator.  Mesos will not
change the mode of any machine, regardless of the estimate provided in
the maintenance schedule.</p>
<h3 id="scheduling-maintenance"><a class="header" href="#scheduling-maintenance">Scheduling maintenance</a></h3>
<p>A machine is transitioned from Up mode to Draining mode as soon as it is
scheduled for maintenance.  To transition a machine into Draining mode, an
operator constructs a maintenance schedule as a JSON document and posts it to
the <a href="endpoints/master/maintenance/schedule.html">/maintenance/schedule</a> HTTP
endpoint on the Mesos master. Each Mesos cluster has a single maintenance
schedule; posting a new schedule replaces the previous schedule, if any.</p>
<p>See the definition of a <a href="https://github.com/apache/mesos/blob/016b02d7ed5a65bcad9261a133c8237c2df66e6e/include/mesos/maintenance/maintenance.proto#L48-L67">maintenance::Schedule</a>
and of <a href="https://github.com/apache/mesos/blob/016b02d7ed5a65bcad9261a133c8237c2df66e6e/include/mesos/v1/mesos.proto#L140-L154">Unavailability</a>.</p>
<p>In a production environment, the schedule should be constructed to ensure that
enough agents are operational at any given point in time to ensure
uninterrupted service by the frameworks.</p>
<p>For example, in a cluster of three machines, the operator might schedule two
machines for one hour of maintenance, followed by another hour for the last
machine.  The timestamps for unavailability are expressed in nanoseconds since
the Unix epoch (note that making reliable use of maintenance primitives requires
that the system clocks of all machines in the cluster are roughly synchronized).</p>
<p>The schedule might look like:</p>
<pre><code>{
  &quot;windows&quot; : [
    {
      &quot;machine_ids&quot; : [
        { &quot;hostname&quot; : &quot;machine1&quot;, &quot;ip&quot; : &quot;10.0.0.1&quot; },
        { &quot;hostname&quot; : &quot;machine2&quot;, &quot;ip&quot; : &quot;10.0.0.2&quot; }
      ],
      &quot;unavailability&quot; : {
        &quot;start&quot; : { &quot;nanoseconds&quot; : 1443830400000000000 },
        &quot;duration&quot; : { &quot;nanoseconds&quot; : 3600000000000 }
      }
    }, {
      &quot;machine_ids&quot; : [
        { &quot;hostname&quot; : &quot;machine3&quot;, &quot;ip&quot; : &quot;10.0.0.3&quot; }
      ],
      &quot;unavailability&quot; : {
        &quot;start&quot; : { &quot;nanoseconds&quot; : 1443834000000000000 },
        &quot;duration&quot; : { &quot;nanoseconds&quot; : 3600000000000 }
      }
    }
  ]
}
</code></pre>
<p>The operator can then post the schedule to the master's
<a href="endpoints/master/maintenance/schedule.html">/maintenance/schedule</a> endpoint:</p>
<pre><code>curl http://localhost:5050/maintenance/schedule \
  -H &quot;Content-type: application/json&quot; \
  -X POST \
  -d @schedule.json
</code></pre>
<p>The machines in a maintenance schedule do not need to be registered with the
Mesos master at the time when the schedule is set.  The operator may add a
machine to the maintenance schedule prior to launching an agent on the machine.
For example, this can be useful to prevent a faulty machine from launching an
agent on boot.</p>
<p><strong>Note</strong>: Each machine in the maintenance schedule should have as
complete information as possible.  In order for Mesos to recognize an agent
as coming from a particular machine, both the <code>hostname</code> and <code>ip</code> fields must
match.  Any omitted data defaults to the empty string <code>&quot;&quot;</code>.  If there are
multiple hostnames or IPs for a machine, the machine's fields need to match
what the agent announces to the master.  If there is any ambiguity in a
machine's configuration, the operator should use the <code>--hostname</code> and <code>--ip</code>
options when starting agents.</p>
<p>The master checks that a maintenance schedule has the following properties:</p>
<ul>
<li>Each maintenance window in the schedule must have at least one machine
and a specified unavailability interval.</li>
<li>Each machine must only appear in the schedule once.</li>
<li>Each machine must have at least a hostname or IP included.
The hostname is not case-sensitive.</li>
<li>All machines that are in Down mode must be present in the schedule.
This is required because this endpoint does not handle the transition
from Down mode to Up mode.</li>
</ul>
<p>If any of these properties are not met, the maintenance schedule is rejected
with a corresponding error message and the master's state is not changed.</p>
<p>To update the maintenance schedule, the operator should first read the current
schedule, make any necessary changes, and then post the modified schedule. The
current maintenance schedule can be obtained by sending a GET request to the
master's <code>/maintenance/schedule</code> endpoint.</p>
<p>To cancel the maintenance schedule, the operator should post an empty schedule.</p>
<h3 id="draining-mode"><a class="header" href="#draining-mode">Draining mode</a></h3>
<p>As soon as a schedule is posted to the Mesos master, the following things occur:</p>
<ul>
<li>The schedule is stored in the <a href="replicated-log-internals.html">replicated log</a>.
This means the schedule is persisted in case of master failover.</li>
<li>All machines in the schedule are immediately transitioned into Draining
mode.  The mode of each machine is also persisted in the replicated log.</li>
<li>All frameworks using resources on affected agents are immediately
notified.  Existing offers from the affected agents are rescinded
and re-sent with additional unavailability data.  All frameworks using
resources from the affected agents are given inverse offers.</li>
<li>New offers from the affected agents will also include
the additional unavailability data.</li>
</ul>
<p>Frameworks should use this additional information to schedule tasks in a
maintenance-aware fashion. Exactly how to do this depends on the design
requirements of each scheduler, but tasks should typically be scheduled in a way
that maximizes utilization but that also attempts to vacate machines before that
machine's advertised unavailability period occurs. A scheduler might choose to
place long-running tasks on machines with no unavailability, or failing that, on
machines whose unavailability is the furthest away.</p>
<p>How a framework responds to an inverse offer indicates its ability to conform to
the maintenance schedule. Accepting an inverse offer communicates that the
framework is okay with the current maintenance schedule, given the current state
of the framework's resources.  The master and operator should interpret
acceptance as a best-effort promise by the framework to free all the resources
contained in the inverse offer before the start of the unavailability
interval. Declining an inverse offer is an advisory notice to the operator that
the framework is unable or unlikely to meet to the maintenance schedule.</p>
<p>For example:</p>
<ul>
<li>A data store may choose to start a new replica if one of its agents is
scheduled for maintenance. The data store should accept an inverse offer if it
can reasonably copy the data on the machine to a new host before the
unavailability interval described in the inverse offer begins. Otherwise, the
data store should decline the offer.</li>
<li>A stateful task on an agent with an impending unavailability may be migrated
to another available agent.  If the framework has sufficient resources to do
so, it would accept any inverse offers.  Otherwise, it would decline them.</li>
</ul>
<p>A framework can use a filter to control when it wants to be contacted again
with an inverse offer.  This is useful since future circumstances may change
the viability of the maintenance schedule.  The filter for inverse offers is
identical to the existing mechanism for re-offering offers to frameworks.</p>
<p><strong>Note</strong>: Accepting or declining an inverse offer does not result in
immediate changes in the maintenance schedule or in the way Mesos acts.
Inverse offers only represent extra information that frameworks may
find useful. In the same manner, rejecting or accepting an inverse offer is a
hint for an operator. The operator may or may not choose to take that hint
into account.</p>
<h3 id="starting-maintenance"><a class="header" href="#starting-maintenance">Starting maintenance</a></h3>
<p>The operator starts maintenance by posting a list of machines to the
<a href="endpoints/master/machine/down.html">/machine/down</a> HTTP endpoint. The list of
machines is specified in JSON format; each element of the list is a
<a href="https://github.com/apache/mesos/blob/016b02d7ed5a65bcad9261a133c8237c2df66e6e/include/mesos/v1/mesos.proto#L157-L167">MachineID</a>.</p>
<p>For example, to start maintenance on two machines:</p>
<pre><code>[
  { &quot;hostname&quot; : &quot;machine1&quot;, &quot;ip&quot; : &quot;10.0.0.1&quot; },
  { &quot;hostname&quot; : &quot;machine2&quot;, &quot;ip&quot; : &quot;10.0.0.2&quot; }
]
</code></pre>
<pre><code>curl http://localhost:5050/machine/down \
  -H &quot;Content-type: application/json&quot; \
  -X POST \
  -d @machines.json
</code></pre>
<p>The master checks that a list of machines has the following properties:</p>
<ul>
<li>The list of machines must not be empty.</li>
<li>Each machine must only appear once.</li>
<li>Each machine must have at least a hostname or IP included.
The hostname is not case-sensitive.</li>
<li>If a machine's IP is included, it must be correctly formed.</li>
<li>All listed machines must be present in the schedule.</li>
</ul>
<p>If any of these properties are not met, the operation is rejected with a
corresponding error message and the master's state is not changed.</p>
<p>The operator can start maintenance on any machine that is scheduled for
maintenance. Machines that are not scheduled for maintenance cannot be directly
transitioned from Up mode into Down mode.  However, the operator may schedule a
machine for maintenance with a timestamp equal to the current time or in the
past, and then immediately start maintenance on that machine.</p>
<p>This endpoint can be used to start maintenance on machines that are not
currently registered with the Mesos master. This can be useful if a machine has
failed and the operator intends to remove it from the cluster; starting
maintenance on the machine prevents the machine from being accidentally rebooted
and rejoining the Mesos cluster.</p>
<p>The operator must explicitly transition a machine from Draining to Down
mode. That is, Mesos will keep a machine in Draining mode even if the
unavailability window arrives or passes.  This means that the operation of the
machine is not disrupted in any way and offers (with unavailability information)
are still sent for this machine.</p>
<p>When maintenance is triggered by the operator, all agents on the machine are
told to shutdown.  These agents are removed from the master, which means that a
<code>TASK_LOST</code> status update will be sent for every task running on each of those
agents. The scheduler driver's <code>slaveLost</code> callback will also be invoked for
each of the removed agents. Any agents on machines in maintenance are also
prevented from reregistering with the master in the future (until maintenance
is completed and the machine is brought back up).</p>
<h3 id="completing-maintenance"><a class="header" href="#completing-maintenance">Completing maintenance</a></h3>
<p>When maintenance is complete or if maintenance needs to be cancelled,
the operator can stop maintenance.  The process is very similar
to starting maintenance (same validation criteria as the previous section).
The operator posts a list of machines to the master's <a href="endpoints/master/machine/up.html">/machine/up</a> endpoint:</p>
<pre><code>[
  { &quot;hostname&quot; : &quot;machine1&quot;, &quot;ip&quot; : &quot;10.0.0.1&quot; },
  { &quot;hostname&quot; : &quot;machine2&quot;, &quot;ip&quot; : &quot;10.0.0.2&quot; }
]
</code></pre>
<pre><code>curl http://localhost:5050/machine/up \
  -H &quot;Content-type: application/json&quot; \
  -X POST \
  -d @machines.json
</code></pre>
<p><strong>Note</strong>: The duration of the maintenance window, as indicated by the
&quot;unavailability&quot; field in the maintenance schedule, is a best-effort guess made
by the operator.  Stopping maintenance before the end of the unavailability
interval is allowed, as is stopping maintenance after the end of the
unavailability interval.  Machines are never automatically transitioned out of
maintenance.</p>
<p>Frameworks are informed about the completion or cancellation of maintenance when
offers from that machine start being sent.  There is no explicit mechanism for
notifying frameworks when maintenance has finished.  After maintenance has
finished, new offers are no longer tagged with unavailability and inverse offers
are no longer sent.  Also, agents running on the machine will be allowed to
register with the Mesos master.</p>
<h3 id="viewing-maintenance-status"><a class="header" href="#viewing-maintenance-status">Viewing maintenance status</a></h3>
<p>The current maintenance status (Up, Draining, or Down) of each machine in the
cluster can be viewed by accessing the master's
<a href="endpoints/master/maintenance/status.html">/maintenance/status</a> HTTP endpoint. For
each machine that is Draining, this endpoint also includes the frameworks' responses to
inverse offers for resources on that machine. For more information, see the
format of the <a href="https://github.com/apache/mesos/blob/fa36917dd142f66924c5f7ed689b87d5ceabbf79/include/mesos/maintenance/maintenance.proto#L73-L84">ClusterStatus message</a>.</p>
<blockquote>
<p>NOTE: The format of the data returned by this endpoint may change in a
future release of Mesos.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Upgrading Mesos
layout: documentation</h2>
<h1 id="upgrading-mesos"><a class="header" href="#upgrading-mesos">Upgrading Mesos</a></h1>
<p>This document serves as a guide for users who wish to upgrade an existing Mesos cluster. Some versions require particular upgrade techniques when upgrading a running cluster. Some upgrades will have incompatible changes.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>This section provides an overview of the changes for each version (in particular when upgrading from the next lower version). For more details please check the respective sections below.</p>
<p>We categorize the changes as follows:</p>
<pre><code>A New feature/behavior
C Changed feature/behavior
D Deprecated feature/behavior
R Removed feature/behavior
</code></pre>
<table class="table table-bordered" style="table-layout: fixed;">
  <thead>
    <tr>
      <th width="10%">
        Version
      </th>
      <th width="18%">
        Mesos Core
      </th>
      <th width="18%">
        Flags
      </th>
      <th width="18%">
        Framework API
      </th>
      <th width="18%">
        Module API
      </th>
      <th width="18%">
        Endpoints
      </th>
    </tr>
  </thead>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.10.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-10-x-ssl-env-var-rename">Renamed LIBPROCESS_SSL_VERIFY_CERT and LIBPROCESS_SSL_REQUIRE_CERT environment variables.</a></li>
      <li>D <a href="upgrades.html#1-10-x-limits-cfs-quota">CPU limits affect the function of the agent's `cgroups_enable_cfs` flag.</a></li>
    </ul>
 </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-10-x-agent-features">agent_features</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-10-x-synchronous-authorization">Authorizers must support synchronous authorization.</a></li>
      <li>AC <a href="upgrades.html#1-10-x-allocator-module-changes">Resource consumption is exposed to allocators.</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-10-x-tasks-pending-authoirization-deprecated">v1 GetTasks pending_tasks</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.9.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-9-x-quota-guarantees">Quota Limits</a></li>
      <li>A <a href="upgrades.html#1-9-x-linux-nnp-isolator">Linux NNP isolator</a></li>
      <li>A <a href="upgrades.html#1-9-x-hostname-validation-scheme">hostname_validation_scheme</a></li>
      <li>C <a href="upgrades.html#1-9-x-client-certificate-verification">TLS certificate verification behaviour</a></li>
      <li>C <a href="upgrades.html#1-9-x-configurable-ipc">Configurable IPC namespace and /dev/shm</a></li>
      <li>A <a href="upgrades.html#1-9-x-automatic-agent-draining">Automatic Agent Draining</a></li>
    </ul>
 </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-9-x-docker-ignore-runtime">docker_ignore_runtime</a></li>
      <li>A <a href="upgrades.html#1-9-x-configurable-ipc">disallow_sharing_agent_ipc_namespace</a></li>
      <li>A <a href="upgrades.html#1-9-x-configurable-ipc">default_container_shm_size</a></li>
      <li>C <a href="upgrades.html#1-9-x-agent-features">agent_features</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-9-x-configurable-ipc">LinuxInfo.ipc_mode and LinuxInfo.shm_size</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-9-x-update-quota">SET_QUOTA and REMOVE QUOTA deprecated
            in favor of UPDATE_QUOTA</a></li>
      <li>D <a href="upgrades.html#1-9-x-quota-guarantees">Quota guarantees deprecated in favor
            of using quota limits</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.8.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-8-x-linux-seccomp-isolator">Linux Seccomp isolator</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-8-x-linux-seccomp-isolator">seccomp_config_dir</a></li>
      <li>A <a href="upgrades.html#1-8-x-linux-seccomp-isolator">seccomp_profile_name</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.7.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-7-x-linux-devices-isolator">Linux devices isolator</a></li>
      <li>A <a href="upgrades.html#1-7-x-auto-load-subsystems">Automatically load local enabled cgroups subsystems</a></li>
      <li>A <a href="upgrades.html#1-7-x-container-specific-cgroups-mounts">Container-specific cgroups mounts</a></li>
      <li>A <a href="upgrades.html#1-7-x-volume-mode-support">Volume mode support</a></li>
      <li>A <a href="upgrades.html#1-7-x-resource-provider-acls">Resource Provider ACLs</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-7-x-enforce-container-ports">enforce_container_ports</a></li>
      <li>A <a href="upgrades.html#1-7-x-gc-non-executor-container-sandboxes">gc_non_executor_container_sandboxes</a></li>
      <li>A <a href="upgrades.html#1-7-x-network-cni-root-dir-persist">network_cni_root_dir_persist</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-7-x-create-disk">`CREATE_DISK` and `DESTROY_DISK` operations and ACLs</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-7-x-container-logger">ContainerLogger module interface changes</a></li>
      <li>C <a href="upgrades.html#1-7-x-isolator-recover">Isolator::recover module interface changes</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-7-x-json-serialization">JSON serialization changes</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.6.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-6-x-grpc-requirement">Requirement for gRPC library</a></li>
      <li>C <a href="upgrades.html#1-6-x-csi-support">CSI v0.2 Support</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-6-x-fetcher-stall-timeout">fetcher_stall_timeout</a></li>
    </ul>
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-6-x-xfs-kill-containers">xfs_kill_containers</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-6-x-disk-profile-adaptor">Disk profile adaptor module changes</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.5.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-5-x-task-starting">Built-in executors send a TASK_STARTING update</a></li>
      <li>A <a href="upgrades.html#1-5-x-network-ports-isolator">Network ports isolator</a></li>
      <li>C <a href="upgrades.html#1-5-x-relative-disk-source-root-path">Relative source root paths for disk resources</a></li>
      <li>A <a href="upgrades.html#1-5-x-reconfiguration-policy">Agent state recovery after resource changes</a></li>
      <li>C <a href="upgrades.html#1-5-x-protobuf-requirement">Requirement for Protobuf library</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-5-x-network-ports-isolator">container_ports_watch_interval</a></li>
      <li>A <a href="upgrades.html#1-5-x-network-ports-isolator">check_agent_port_range_only</a></li>
      <li>D <a href="upgrades.html#1-5-x-executor-secret-key">executor_secret_key</a></li>
      <li>A <a href="upgrades.html#1-5-x-reconfiguration-policy">reconfiguration_policy</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-5-x-task-resource-limitation">Added the TaskStatus.limitation message</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-5-x-get-containers">Allowed to view nested/standalone containers</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.4.x
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-4-x-ambient-capabilities">Container capabilities are made ambient if supported</a></li>
      <li>A <a href="upgrades.html#1-4-x-bounding-capabilities">Support for explicit bounding capabilities</a></li>
      <li>C <a href="upgrades.html#1-4-x-agent-recovery">Agent recovery post reboot</a></li>
      <li>C <a href="upgrades.html#1-4-x-xfs-no-enforce">XFS disk isolator support for not enforcing disk limits</a></li>
      <li>C <a href="upgrades.html#1-4-x-update-minimal-docker-version">Update the minimal supported Docker version</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-4-x-agent-capabilities-flags">effective_capabilities</a></li>
      <li>A <a href="upgrades.html#1-4-x-agent-capabilities-flags">bounding-capabilities</a></li>
      <li>D <a href="upgrades.html#1-4-x-agent-capabilities-flags">allowed-capabilities</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-4-x-bounding-capabilities">Support for explicit setting bounding capabilities</a></li>
      <li>D <a href="upgrades.html#1-4-x-linuxinfo-capabilities">LinuxInfo.effective_capabilities deprecates LinuxInfo.capabilities</a></li>
      <li>C <a href="upgrades.html#1-4-x-mesos-library">`Resources` class in the internal Mesos C++ library only supports post-`RESERVATION_REFINEMENT` format</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-4-x-allocator-update-slave">Changed semantics of Allocator::updateSlave</a></li>
    </ul>
  </td>
<td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.3.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-3-x-disallow-old-agents">Prevent registration by old Mesos agents</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-3-x-setquota-removequota-acl">--acls (set_quotas and remove_quotas)</a></li>
      <li>R <a href="upgrades.html#1-3-x-shutdown-framework-acl">--acls (shutdown_frameworks)</a></li>
      <li>A <a href="upgrades.html#1-3-x-executor-authentication">authenticate_http_executors</a></li>
      <li>A <a href="upgrades.html#1-3-x-executor-authentication">executor_secret_key</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-3-x-multi-role-support">MULTI_ROLE support</a></li>
      <li>D <a href="upgrades.html#1-3-x-framework-info-role">FrameworkInfo.roles deprecates FrameworkInfo.role</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-3-x-allocator-interface-change">Allocator MULTI_ROLE interface changes</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-3-x-endpoints-roles">MULTI_ROLE deprecates 'role' field in endpoints</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.2.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-2-1-disallow-old-agents">Prevent registration by old Mesos agents</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-2-x-heartbeat-flag">http_heartbeat_interval</a></li>
      <li>A <a href="upgrades.html#1-2-x-backend-flag">image_provisioner_backend</a></li>
      <li>A <a href="upgrades.html#1-2-x-unreachable-flag">max_unreachable_tasks_per_framework</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-2-x-revive-suppress">Revive and Suppress v1 scheduler Calls</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-2-x-container-logger-interface">Container Logger prepare method</a></li>
      <li>C <a href="upgrades.html#1-2-x-allocator-module-changes">Allocator module changes</a></li>
      <li>A <a href="upgrades.html#1-2-x-new-authz-actions">New Authorizer module actions</a></li>
      <li>D <a href="upgrades.html#1-2-x-renamed-authz-actions">Renamed Authorizer module actions (deprecated old aliases)</a></li>
      <li>R <a href="upgrades.html#1-2-x-removed-hooks">Removed slavePreLaunchDockerEnvironmentDecorator and slavePreLaunchDockerHook</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>A <a href="upgrades.html#1-2-x-debug-endpoints">LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT, ATTACH_CONTAINER_OUTPUT</a></li>
      <li>D <a href="upgrades.html#1-2-x-recovered-frameworks">v1 GetFrameworks recovered_frameworks</a></li>
      <li>D <a href="upgrades.html#1-2-x-orphan-executors">v1 GetExecutors orphan_executors</a></li>
      <li>D <a href="upgrades.html#1-2-x-orphan-tasks">v1 GetTasks orphan_tasks</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.1.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>R <a href="upgrades.html#1-1-x-container-logger-interface">Container Logger recovery method</a></li>
      <li>C <a href="upgrades.html#1-1-x-allocator-updateallocation">Allocator updateAllocation method</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  1.0.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>CD <a href="upgrades.html#1-0-x-allocator-metrics">Allocator Metrics</a></li>
      <li>C <a href="upgrades.html#1-0-x-persistent-volume">Destruction of persistent volumes</a></li>
      <li>C <a href="upgrades.html#1-0-x-slave">Slave to Agent rename</a></li>
      <li>C <a href="upgrades.html#1-0-x-quota-acls">Quota ACLs</a></li>
      <li>R <a href="upgrades.html#1-0-x-executor-environment-variables">Executor environment variables inheritance</a></li>
      <li>R <a href="upgrades.html#1-0-x-deprecated-fields-in-container-config">Deprecated fields in ContainerConfig</a></li>
      <li>C <a href="upgrades.html#1-0-x-persistent-volume-ownership">Persistent volume ownership</a></li>
      <li>C <a href="upgrades.html#1-0-x-fetcher-user">Fetcher assumes same user as task</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#1-0-x-docker-timeout-flag">docker_stop_timeout</a></li>
      <li>D <a href="upgrades.html#1-0-x-credentials-file">credential(s) (plain text format)</a></li>
      <li>C <a href="upgrades.html#1-0-x-slave">Slave to Agent rename</a></li>
      <li>R <a href="upgrades.html#1-0-x-workdir">work_dir default value</a></li>
      <li>D <a href="upgrades.html#1-0-x-deprecated-ssl-env-variables">SSL environment variables</a></li>
      <li>ACD <a href="upgrades.html#1-0-x-http-authentication-flags">HTTP authentication</a></li>
      <li>R <a href="upgrades.html#1-0-x-registry-strict">registry_strict</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>DC <a href="upgrades.html#1-0-x-executorinfo">ExecutorInfo.source</a></li>
      <li>A <a href="upgrades.html#1-0-x-v1-commandinfo">CommandInfo.URI output_file</a></li>
      <li>C <a href="upgrades.html#1-0-x-scheduler-proto">scheduler.proto optional fields</a></li>
      <li>C <a href="upgrades.html#1-0-x-executor-proto">executor.proto optional fields</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-0-x-authorizer">Authorizer</a></li>
      <li>C <a href="upgrades.html#1-0-x-allocator">Allocator</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#1-0-x-status-code">HTTP return codes</a></li>
      <li>R <a href="upgrades.html#1-0-x-status-code">/observe</a></li>
      <li>C <a href="upgrades.html#1-0-x-endpoint-authorization">Added authorization</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.28.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-28-x-resource-precision">Resource Precision</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-28-x-autherization-acls">Authentication ACLs</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.27.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#0-27-x-implicit-roles">--roles</a></li>
      <li>D <a href="upgrades.html#0-27-x-acl-shutdown-flag">--acls (shutdown_frameworks)</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-27-x-executor-lost-callback">executorLost callback</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-27-x-allocator-api">Allocator API</a></li>
      <li>C <a href="upgrades.html#0-27-x-isolator-api">Isolator API</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.26.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-26-x-taskstatus-reason">TaskStatus::Reason Enum</a></li>
      <li>C <a href="upgrades.html#0-26-x-credential-protobuf">Credential Protobuf</a></li>
      <li>C <a href="upgrades.html#0-26-x-network-info-protobuf">NetworkInfo Protobuf</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-26-x-state-endpoint">State Endpoint</a></li>
    </ul>
  </td>
</tr>
<tr>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Version-->
  0.25.x
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Mesos Core-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Flags-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Framework API-->
    <ul style="padding-left:10px;">
      <li>C <a href="upgrades.html#0-25-x-scheduler-bindings">C++/Java/Python Scheduler Bindings</a></li>
    </ul>
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Module API-->
  </td>
  <td style="word-wrap: break-word; overflow-wrap: break-word;"><!--Endpoints-->
    <ul style="padding-left:10px;">
      <li>D <a href="upgrades.html#0-25-x-json-endpoints">*.json Endpoints</a></li>
    </ul>
  </td>
</tr>
</table>
<h2 id="upgrading-from-19x-to-110x"><a class="header" href="#upgrading-from-19x-to-110x">Upgrading from 1.9.x to 1.10.x</a></h2>
<p><a name="1-10-x-ssl-env-var-rename"></a></p>
<ul>
<li>The canonical name for the environment variable <code>LIBPROCESS_SSL_VERIFY_CERT</code> was changed to <code>LIBPROCESS_SSL_VERIFY_SERVER_CERT</code>.
The canonical name for the environment variable <code>LIBPROCESS_SSL_REQUIRE_CERT</code> was changed to <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code>.
The old names will continue to work as before, but operators are encouraged to update their configuration to reduce confusion.</li>
</ul>
<p><a name="1-10-x-limits-cfs-quota"></a></p>
<ul>
<li>The Mesos agent's <code>cgroups_enable_cfs</code> flag previously controlled whether or not CFS quota would be used for all tasks on the agent. Resource limits have been added to tasks, and when a CPU limit is specified on a task, the agent will now apply a CFS quota regardless of the value of <code>cgroups_enable_cfs</code>.</li>
</ul>
<p><a name="1-10-x-agent-features"></a></p>
<ul>
<li>The Mesos agent now requires the new <code>TASK_RESOURCE_LIMITS</code> feature. This capability is set by default, but if the <code>--agent_features</code> flag is specified explicitly, <code>TASK_RESOURCE_LIMITS</code> must be included.</li>
</ul>
<p><a name="1-10-x-synchronous-authorization"></a></p>
<ul>
<li>Authorizers now must implement a method <code>getApprover(...)</code> (see the
<a href="authorization.html#implementing-an-authorizer">authorization documentation</a>
and <a href="https://issues.apache.org/jira/browse/MESOS-10056">MESOS-10056</a>)
that returns <code>ObjectApprover</code>s that are valid throughout their whole lifetime.
Keeping the state of an <code>ObjectApprover</code> up-to-date becomes a responsibility
of the authorizer. This is a <strong>breaking change</strong> for authorizer modules.</li>
</ul>
<p><a name="1-10-x-tasks-pending-authoirization-deprecated"></a></p>
<ul>
<li>The field <code>pending_tasks</code> in <code>GetTasks</code> master API call has been deprecated.
From now on, this field will be empty. Moreover, the notion of
<em>tasks pending authorization</em> no longer exists
(see <a href="https://issues.apache.org/jira/browse/MESOS-10056">MESOS-10056</a>).</li>
</ul>
<p><a name="1-10-x-allocator-module-changes"></a></p>
<ul>
<li>Allocator interface has been changed to supply allocator with information on
resources actually consumed by frameworks. A method
<code>transitionOfferedToAllocated(...)</code> has been added and the signature of
<code>recoverResources(...)</code> has been extended. Note that allocators must implement
these new/extended method signatures, but are free to ignore resource
consumption data provided by master.</li>
</ul>
<h2 id="upgrading-from-18x-to-19x"><a class="header" href="#upgrading-from-18x-to-19x">Upgrading from 1.8.x to 1.9.x</a></h2>
<p><a name="1-9-x-automatic-agent-draining"></a></p>
<ul>
<li>A new <code>DRAINING</code> state has been added to Mesos agents. Once an agent is draining, all tasks running on that agent are gracefully
killed and no offers for that agent are sent to schedulers, preventing the launching of new tasks.
Operators can put an agent into <code>DRAINING</code> state by using the <code>DRAIN_AGENT</code> operator API call.
See <a href="maintenance.html"><code>docs/maintenance</code></a> for details.</li>
</ul>
<p><a name="1-9-x-agent-features"></a></p>
<ul>
<li>The Mesos agent now requires the new <code>AGENT_DRAINING</code> feature. This capability is set by default, but if the <code>--agent_features</code> flag is specified explicitly, <code>AGENT_DRAINING</code> must be included.</li>
</ul>
<p><a name="1-9-x-linux-nnp-isolator"></a></p>
<ul>
<li>A new <a href="isolators/linux-nnp.html"><code>linux/nnp</code></a> isolator has been added. The isolator supports setting of the <code>no_new_privs</code> bit in the container, preventing tasks from acquiring additional privileges.</li>
</ul>
<p><a name="1-9-x-docker-ignore-runtime"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#docker_ignore_runtime"><code>--docker_ignore_runtime</code></a> flag has been added. This causes the agent to ignore any runtime configuration present in Docker images.</li>
</ul>
<p><a name="1-9-x-hostname-validation-scheme"></a></p>
<ul>
<li>A new libprocess TLS flag <code>--hostname_validation_scheme</code> along with the corresponding environment variable <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME</code>
has been added. Using this flag, users can configure the way libprocess performs hostname validation for TLS connections.
See <a href="ssl.html"><code>docs/ssl</code></a> for details.</li>
</ul>
<p><a name="1-9-x-client-certificate-verification"></a></p>
<ul>
<li>The semantics of the libprocess environment variables <code>LIBPROCESS_SSL_VERIFY_CERT</code> and <code>LIBPROCESS_SSL_REQUIRE_CERT</code> have been slightly updated such that
the former now only applies to client-mode and the latter only to server-mode connections. As part of this re-adjustment, the following two changes have
been introduced that might require changes for operators running Mesos in unusual TLS configurations.
<ul>
<li>Anonymous ciphers can not be used anymore when <code>LIBPROCESS_SSL_VERIFY_CERT</code> is set to true. This is because the use of anonymous ciphers enables
a malicious attacker to bypass certificate verification by choosing a certificate-less cipher.
Users that rely on anonymous ciphers being available should make sure that <code>LIBPROCESS_SSL_VERIFY_CERT</code> is set to false.</li>
<li>For incoming connections, certificates are not verified unless <code>LIBPROCESS_SSL_REQUIRE_CERT</code> is set to true.
This is because verifying the certificate can lead to false negatives, where a connection is aborted even though presenting no certificate at all
would have been successfull. Users that rely on incoming connection requests presenting valid TLS certificates should make sure that
the <code>LIBPROCESS_SSL_REQUIRE_CERT</code> option is set to true.</li>
</ul>
</li>
</ul>
<p><a name="1-9-x-configurable-ipc"></a></p>
<ul>
<li>The Mesos containerizer now supports configurable IPC namespace and /dev/shm. Container can be configured to have a private IPC namespace and /dev/shm or share them from its parent via the field <code>LinuxInfo.ipc_mode</code>, and the size of its private /dev/shm is also configurable via the field <code>LinuxInfo.shm_size</code>. Operators can control whether it is allowed to share host's IPC namespace and /dev/shm with top level containers via the agent flag <code>--disallow_sharing_agent_ipc_namespace</code>, and specify the default size of the /dev/shm for the container which has a private /dev/shm via the agent flag <code>--default_container_shm_size</code>.</li>
</ul>
<p><a name="1-9-x-update-quota"></a></p>
<ul>
<li>The <code>SET_QUOTA</code> and <code>REMOVE QUOTA</code> master calls are deprecated in favor of a new <code>UPDATE_QUOTA</code> master call.</li>
</ul>
<p><a name="1-9-x-quota-guarantees"></a></p>
<ul>
<li>Prior to Mesos 1.9, the quota related APIs only exposed quota &quot;guarantees&quot; which ensured a minimum amount of resources would be available to a role. Setting guarantees also set implicit quota limits. In Mesos 1.9+, quota limits are now exposed directly.
<ul>
<li>Quota guarantees are now deprecated in favor of using only quota limits. Enforcement of quota guarantees required that Mesos holds back enough resources to meet all of the unsatisfied quota guarantees. Since Mesos is moving towards an optimistic offer model (to improve multi-role / multi- scheduler scalability, see MESOS-1607), it will become no longer possible to enforce quota guarantees by holding back resources. In such a model, quota limits are simple to enforce, but quota guarantees would require a complex &quot;effective limit&quot; propagation model to leave space for unsatisfied guarantees.</li>
<li>For these reasons, quota guarantees, while still functional in Mesos 1.9, are now deprecated. A combination of limits and priority based preemption will be simpler in an optimistic offer model.</li>
</ul>
</li>
</ul>
<h2 id="upgrading-from-17x-to-18x"><a class="header" href="#upgrading-from-17x-to-18x">Upgrading from 1.7.x to 1.8.x</a></h2>
<p><a name="1-8-x-linux-seccomp-isolator"></a></p>
<ul>
<li>A new <a href="isolators/linux-seccomp.html"><code>linux/seccomp</code></a> isolator has been added. The isolator supports the following new agent flags:
<ul>
<li><code>--seccomp_config_dir</code> specifies the directory path of the Seccomp profiles.</li>
<li><code>--seccomp_profile_name</code> specifies the path of the default Seccomp profile relative to the <code>seccomp_config_dir</code>.</li>
</ul>
</li>
</ul>
<h2 id="upgrading-from-16x-to-17x"><a class="header" href="#upgrading-from-16x-to-17x">Upgrading from 1.6.x to 1.7.x</a></h2>
<p><a name="1-7-x-linux-devices-isolator"></a></p>
<ul>
<li>A new <a href="isolators/linux-devices.html"><code>linux/devices</code></a> isolator has been
added. This isolator automatically populates containers with devices
that have been whitelisted with the <code>--allowed_devices</code> agent flag.</li>
</ul>
<p><a name="1-7-x-auto-load-subsystems"></a></p>
<ul>
<li>A new option <code>cgroups/all</code> has been added to the agent flag <code>--isolation</code>. This allows cgroups isolator to automatically load all the local enabled cgroups subsystems. If this option is specified in the agent flag <code>--isolation</code> along with other cgroups related options (e.g., <code>cgroups/cpu</code>), those options will be just ignored.</li>
</ul>
<p><a name="1-7-x-container-specific-cgroups-mounts"></a></p>
<ul>
<li>Added container-specific cgroups mounts under <code>/sys/fs/cgroup</code> to containers with image launched by Mesos containerizer.</li>
</ul>
<p><a name="1-7-x-volume-mode-support"></a></p>
<ul>
<li>Previously the <code>HOST_PATH</code>, <code>SANDBOX_PATH</code>, <code>IMAGE</code>, <code>SECRET</code>, and <code>DOCKER_VOLUME</code> volumes were always mounted for container in read-write mode, i.e., the <code>Volume.mode</code> field was not honored. Now we will mount these volumes based on the <code>Volume.mode</code> field so framework can choose to mount the volume for the container in either read-write mode or read-only mode.</li>
</ul>
<p><a name="1-7-x-create-disk"></a></p>
<ul>
<li>To simplify the API for CSI-backed disk resources, the following operations and corresponding ACLs have been introduced to replace the experimental <code>CREATE_VOLUME</code>, <code>CREATE_BLOCK</code>, <code>DESTROY_VOLUME</code> and <code>DESTROY_BLOCK</code> operations:
<ul>
<li><code>CREATE_DISK</code> to create a <code>MOUNT</code> or <code>BLOCK</code> disk resource from a <code>RAW</code> disk resource. The <code>CreateMountDisk</code> and <code>CreateBlockDisk</code> ACLs control which principals are allowed to create <code>MOUNT</code> or <code>BLOCK</code> disks for which roles.</li>
<li><code>DESTROY_DISK</code> to reclaim a <code>MOUNT</code> or <code>BLOCK</code> disk resource back to a <code>RAW</code> disk resource. The <code>DestroyMountDisk</code> and <code>DestroyBlockDisk</code> ACLs control which principals are allowed to reclaim <code>MOUNT</code> or <code>BLOCK</code> disks for which roles.</li>
</ul>
</li>
</ul>
<p><a name="1-7-x-resource-provider-acls"></a></p>
<ul>
<li>A new <code>ViewResourceProvider</code> ACL has been introduced to control which principals are allowed to call the <code>GET_RESOURCE_PROVIDERS</code> agent API.</li>
</ul>
<p><a name="1-7-x-enforce-container-ports"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#enforce_container_ports"><code>--enforce_container_ports</code></a> flag has been added to toggle whether the <a href="isolators/network-ports.html"><code>network/ports</code></a> isolator should enforce TCP ports usage limits.</li>
</ul>
<p><a name="1-7-x-gc-non-executor-container-sandboxes"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#gc_non_executor_container_sandboxes"><code>--gc_non_executor_container_sandboxes</code></a>
agent flag has been added to garbage collect the sandboxes of nested
containers, which includes the tasks groups launched by the default executor.
We recommend enabling the flag if you have frameworks that launch multiple
task groups on the same default executor instance.</li>
</ul>
<p><a name="1-7-x-network-cni-root-dir-persist"></a></p>
<ul>
<li>A new <a href="configuration/agent.html#network_cni_root_dir_persist"><code>--network_cni_root_dir_persist</code></a> flag has been added to toggle whether the <a href="cni.html"><code>network/cni</code></a> isolator should persist the network information across reboots.</li>
</ul>
<p><a name="1-7-x-container-logger"></a></p>
<ul>
<li><code>ContainerLogger</code> module interface has been changed. The <code>prepare()</code> method now takes <code>ContainerID</code> and <code>ContainerConfig</code> instead.</li>
</ul>
<p><a name="1-7-x-isolator-recover"></a></p>
<ul>
<li><code>Isolator::recover()</code> has been updated to take an <code>std::vector</code> instead of <code>std::list</code> of container states.</li>
</ul>
<p><a name="1-7-x-json-serialization"></a></p>
<ul>
<li>As a result of adapting rapidjson for performance improvement, all JSON endpoints serialize differently while still conforming to the ECMA-404 spec for JSON. This means that if a client has a JSON de-serializer that conforms to ECMA-404 they will see no change. Otherwise, they may break. As an example, Mesos would previously serialize '/' as '/', but the spec does not require the escaping and rapidjson does not escape '/'.</li>
</ul>
<h2 id="upgrading-from-15x-to-16x"><a class="header" href="#upgrading-from-15x-to-16x">Upgrading from 1.5.x to 1.6.x</a></h2>
<p><a name="1-6-x-grpc-requirement"></a></p>
<ul>
<li>gRPC version 1.10+ is required to build Mesos when enabling gRPC-related features. Please upgrade your gRPC library if you are using an unbundled one.</li>
</ul>
<p><a name="1-6-x-csi-support"></a></p>
<ul>
<li>CSI v0.2 is now supported as experimental. Due to the incompatibility between CSI v0.1 and v0.2, the experimental support for CSI v0.1 is removed, and the operator must remove all storage local resource providers within an agent before upgrading the agent. NOTE: This is a <strong>breaking change</strong> for storage local resource providers.</li>
</ul>
<p><a name="1-6-x-fetcher-stall-timeout"></a></p>
<ul>
<li>A new agent flag <code>--fetcher_stall_timeout</code> has been added. This flag specifies the amount of time for the container image and artifact fetchers to wait before aborting a stalled download (i.e., the speed keeps below one byte per second). NOTE: This flag only applies when downloading data from the net and does not apply to HDFS.</li>
</ul>
<p><a name="1-6-x-disk-profile-adaptor"></a></p>
<ul>
<li>The disk profile adaptor module has been changed to support CSI v0.2, and its header file has been renamed to be consistent with other modules. See <code>disk_profile_adaptor.hpp</code> for interface changes.</li>
</ul>
<p><a name="1-6-x-xfs-kill-containers"></a></p>
<ul>
<li>A new agent flag <code>--xfs_kill_containers</code> has been added. By setting this flag, the <a href="isolators/disk-xfs.html"><code>disk/xfs</code></a> isolator
will now kill containers that exceed the disk limit.</li>
</ul>
<h2 id="upgrading-from-14x-to-15x"><a class="header" href="#upgrading-from-14x-to-15x">Upgrading from 1.4.x to 1.5.x</a></h2>
<p><a name="1-5-x-task-starting"></a></p>
<ul>
<li>The built-in executors will now send a <code>TASK_STARTING</code> status update for
every task they've successfully received and are about to start.
The possibility of any executor sending this update has been documented since
the beginning of Mesos, but prior to this version the built-in executors did
not actually send it. This means that all schedulers using one of the built-in
executors must be upgraded to expect <code>TASK_STARTING</code> updates before upgrading
Mesos itself.</li>
</ul>
<p><a name="1-5-x-task-resource-limitation"></a></p>
<ul>
<li>A new field, <code>limitation</code>, was added to the <code>TaskStatus</code> message. This
field is a <code>TaskResourceLimitation</code> message that describes the resources
that caused a task to fail with a resource limitation reason.</li>
</ul>
<p><a name="1-5-x-network-ports-isolator"></a></p>
<ul>
<li>A new <a href="isolators/network-ports.html"><code>network/ports</code></a> isolator has been added. The isolator supports the following new agent flags:
<ul>
<li><code>--container_ports_watch_interval</code> specifies the interval at which the isolator reconciles port assignments.</li>
<li><code>--check_agent_port_range_only</code> excludes ports outside the agent's range from port reconciliation.</li>
</ul>
</li>
</ul>
<p><a name="1-5-x-executor-secret-key"></a></p>
<ul>
<li>Agent flag <code>--executor_secret_key</code> has been deprecated. Operators should use <code>--jwt_secret_key</code> instead.</li>
</ul>
<p><a name="1-5-x-relative-disk-source-root-path"></a></p>
<ul>
<li>The fields <code>Resource.disk.source.path.root</code> and <code>Resource.disk.source.mount.root</code> can now be set to relative paths to an agent's work directory. The containerizers will interpret the paths based on the <code>--work_dir</code> flag on an agent.</li>
</ul>
<p><a name="1-5-x-get-containers"></a></p>
<ul>
<li>The agent operator API call <code>GET_CONTAINERS</code> has been updated to support listing nested or standalone containers. One can specify the following fields in the request:
<ul>
<li><code>show_nested</code>: Whether to show nested containers.</li>
<li><code>show_standalone</code>: Whether to show standalone containers.</li>
</ul>
</li>
</ul>
<p><a name="1-5-x-reconfiguration-policy"></a></p>
<ul>
<li>A new agent flag <code>--reconfiguration_policy</code> has been added. By setting the value of this flag to <code>additive</code>,
operators can allow the agent to be restarted with increased resources without requiring the agent ID to be
changed. Note that if this feature is used, the master version is required to be &gt;= 1.5 as well.</li>
</ul>
<p><a name="1-5-x-protobuf-requirement"></a></p>
<ul>
<li>Protobuf version 3+ is required to build Mesos. Please upgrade your Protobuf library if you are using an unbundled one.</li>
</ul>
<p><a name="1-5-x-log-reader-catchup"></a></p>
<ul>
<li>A new <code>catchup()</code> method has been added to the replicated log reader API. The method allows to catch-up positions missing in the local non-leading replica to allow safe eventually consistent reads from it. Note about backwards compatibility: In order for the feature to work correctly in presence of log truncations all log replicas need to be updated.</li>
</ul>
<h2 id="upgrading-from-13x-to-14x"><a class="header" href="#upgrading-from-13x-to-14x">Upgrading from 1.3.x to 1.4.x</a></h2>
<p><a name="1-4-x-ambient-capabilities"></a></p>
<ul>
<li>If the <code>mesos-agent</code> kernel supports ambient capabilities (Linux 4.3 or later), the capabilities specified in the <code>LinuxInfo.effective_capabilities</code> message will be made ambient in the container task.</li>
</ul>
<p><a name="1-4-x-bounding-capabilities"></a></p>
<ul>
<li>Explicitly setting the bounding capabilities of a task independently of the effective capabilities is now supported. Frameworks can specify the task bounding capabilities by using the <code>LinuxInfo.bounding_capabilities</code> message. Operators can specify the default bounding capabilities using the agent <code>--bounding_capabilities</code> flag. This flag also specifies the maximum bounding set that a framework is allowed to specify.</li>
</ul>
<p><a name="1-4-x-agent-recovery"></a></p>
<ul>
<li>Agent is now allowed to recover its agent ID post a host reboot. This prevents the unnecessary discarding of agent ID by prior Mesos versions. Notes about backwards compatibility:
<ul>
<li>In case the agent's recovery runs into agent info mismatch which may happen due to resource change associated with reboot, it'll fall back to recovering as a new agent (existing behavior).</li>
<li>In other cases such as checkpointed resources (e.g. persistent volumes) being incompatible with the agent's resources the recovery will still fail (existing behavior).</li>
</ul>
</li>
</ul>
<p><a name="1-4-x-linuxinfo-capabilities"></a></p>
<ul>
<li>The <code>LinuxInfo.capabilities</code> field has been deprecated in favor of <code>LinuxInfo.effective_capabilities</code>.</li>
</ul>
<p><a name="1-4-x-agent-capabilities-flags"></a></p>
<ul>
<li>Changes to capability-related agent flags:
<ul>
<li>The agent <code>--effective_capabilities</code> flag has been added to specify the default effective capability set for tasks.</li>
<li>The agent <code>--bounding_capabilities</code> flag has been added to specify the default bounding capability set for tasks.</li>
<li>The agent <code>--allowed-capabilities</code> flag has been deprecated in favor of <code>--effective_capabilities</code>.</li>
</ul>
</li>
</ul>
<p><a name="1-4-x-allocator-update-slave"></a></p>
<ul>
<li>The semantics of the optional resource argument passed in <code>Allocator::updateSlave</code> was change. While previously the passed value denoted a new amount of oversubscribed (revocable) resources on the agent, it now denotes the new amount of total resources on the agent. This requires custom allocator implementations to update their interpretation of the passed value.</li>
</ul>
<p><a name="1-4-x-xfs-no-enforce"></a></p>
<ul>
<li>The XFS Disk Isolator now supports the <code>--no-enforce_container_disk_quota</code> option to efficiently measure disk resource usage without enforcing any usage limits.</li>
</ul>
<p><a name="1-4-x-mesos-library"></a></p>
<ul>
<li>The <code>Resources</code> class in the internal Mesos C++ library changed its behavior to only support post-<code>RESERVATION_REFINEMENT</code> format. If a framework is using this internal utility, it is likely to break if the <code>RESERVATION_REFINEMENT</code> capability is not enabled.</li>
</ul>
<p><a name="1-4-x-update-minimal-docker-version"></a></p>
<ul>
<li>To specify the <code>--type=container</code> option for the <code>docker inspect &lt;container_name&gt;</code> command, the minimal supported Docker version has been updated from 1.0.0 to 1.8.0 since Docker supported <code>--type=container</code> for the <code>docker inspect</code> command starting from 1.8.0.</li>
</ul>
<h2 id="upgrading-from-12x-to-13x"><a class="header" href="#upgrading-from-12x-to-13x">Upgrading from 1.2.x to 1.3.x</a></h2>
<p><a name="1-3-x-disallow-old-agents"></a></p>
<ul>
<li>The master will no longer allow 0.x agents to register. Interoperability between 1.1+ masters and 0.x agents has never been supported; however, it was not explicitly disallowed, either. Starting with this release of Mesos, registration attempts by 0.x agents will be ignored.</li>
</ul>
<p><a name="1-3-x-setquota-removequota-acl"></a></p>
<ul>
<li>Support for deprecated ACLs <code>set_quotas</code> and <code>remove_quotas</code> has been removed from the local authorizer. Before upgrading the Mesos binaries, consolidate the ACLs used under <code>set_quotas</code> and <code>remove_quotes</code> under their replacement ACL <code>update_quotas</code>. After consolidation of the ACLs, the binaries could be safely replaced.</li>
</ul>
<p><a name="1-3-x-shutdown-framework-acl"></a></p>
<ul>
<li>Support for deprecated ACL <code>shutdown_frameworks</code> has been removed from the local authorizer. Before upgrading the Mesos binaries, replace all instances of the ACL <code>shutdown_frameworks</code> with the newer ACL <code>teardown_frameworks</code>. After updating the ACLs, the binaries can be safely replaced.</li>
</ul>
<p><a name="1-3-x-multi-role-support"></a>
<a name="1-3-x-framework-info-role"></a></p>
<ul>
<li>Support for multi-role frameworks deprecates the <code>FrameworkInfo.role</code> field in favor of <code>FrameworkInfo.roles</code> and the <code>MULTI_ROLE</code> capability. Frameworks using the new field can continue to use a single role.</li>
</ul>
<p><a name="1-3-x-endpoints-roles"></a></p>
<ul>
<li>Support for multi-role frameworks means that the framework <code>role</code> field in the master and agent endpoints is deprecated in favor of <code>roles</code>. Any tooling parsing endpoint information and relying on the role field needs to be updated before multi-role frameworks can be safely run in the cluster.</li>
</ul>
<p><a name="1-3-x-allocator-interface-change"></a></p>
<ul>
<li>Implementors of allocator modules have to provide new implementation functionality to satisfy the <code>MULTI_ROLE</code> framework capability. Also, the interface has changed.</li>
</ul>
<p><a name="1-3-x-executor-authentication"></a></p>
<ul>
<li>New Agent flags <code>authenticate_http_executors</code> and <code>executor_secret_key</code>: Used to enable required HTTP executor authentication and set the key file used for generation and authentication of HTTP executor tokens. Note that enabling these flags after upgrade is disruptive to HTTP executors that were launched before the upgrade. For more information on the recommended upgrade procedure when enabling these flags, see the <a href="authentication.html">authentication documentation</a>.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents/schedulers can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-11x-to-12x"><a class="header" href="#upgrading-from-11x-to-12x">Upgrading from 1.1.x to 1.2.x</a></h2>
<p><a name="1-2-1-disallow-old-agents"></a></p>
<ul>
<li>In Mesos 1.2.1, the master will no longer allow 0.x agents to register. Interoperability between 1.1+ masters and 0.x agents has never been supported; however, it was not explicitly disallowed, either. Starting with Mesos 1.2.1, registration attempts by 0.x agents will be ignored. <strong>NOTE:</strong> This applies only when upgrading to Mesos 1.2.1. Mesos 1.2.0 does not implement this behavior.</li>
</ul>
<p><a name="1-2-x-heartbeat-flag"></a></p>
<ul>
<li>New Agent flag http_heartbeat_interval: This flag sets a heartbeat interval for messages to be sent over persistent connections made against the agent HTTP API. Currently, this only applies to the LAUNCH_NESTED_CONTAINER_SESSION and ATTACH_CONTAINER_OUTPUT calls. (default: 30secs)</li>
</ul>
<p><a name="1-2-x-backend-flag"></a></p>
<ul>
<li>New Agent flag image_provisioner_backend: Strategy for provisioning container rootfs from images, e.g., aufs, bind, copy, overlay.</li>
</ul>
<p><a name="1-2-x-unreachable-flag"></a></p>
<ul>
<li>New Master flag max_unreachable_tasks_per_framework: Maximum number of unreachable tasks per framework to store in memory. (default: 1000)</li>
</ul>
<p><a name="1-2-x-revive-suppress"></a></p>
<ul>
<li>New Revive and Suppress v1 scheduler Calls: Revive or Suppress offers for a specified role. If role is unset, the call will revive/suppress offers for all of the roles the framework is subscribed to. (Especially for multi-role frameworks.)</li>
</ul>
<p><a name="1-2-x-container-logger-interface"></a></p>
<ul>
<li>Mesos 1.2 modifies the <code>ContainerLogger</code>'s <code>prepare()</code> method.  The method now takes an additional argument for the <code>user</code> the logger should run a subprocess as.  Please see <a href="https://issues.apache.org/jira/browse/MESOS-5856">MESOS-5856</a> for more information.</li>
</ul>
<p><a name="1-2-x-allocator-module-changes"></a></p>
<ul>
<li>Allocator module changes to support inactive frameworks, multi-role frameworks, and suppress/revive. See <code>allocator.hpp</code> for interface changes.</li>
</ul>
<p><a name="1-2-x-new-authz-actions"></a></p>
<ul>
<li>New Authorizer module actions: LAUNCH_NESTED_CONTAINER, KILL_NESTED_CONTAINER, WAIT_NESTED_CONTAINER, LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT, ATTACH_CONTAINER_OUTPUT, VIEW_CONTAINER, and SET_LOG_LEVEL. See <code>authorizer.proto</code> for module interface changes, and <code>acls.proto</code> for corresponding LocalAuthorizer ACL changes.</li>
</ul>
<p><a name="1-2-x-renamed-authz-actions"></a></p>
<ul>
<li>Renamed Authorizer module actions (and deprecated old aliases): REGISTER_FRAMEWORK, TEARDOWN_FRAMEWORK, RESERVE_RESOURCES, UNRESERVE_RESOURCES, CREATE_VOLUME, DESTROY_VOLUME, UPDATE_WEIGHT, GET_QUOTA. See <code>authorizer.proto</code> for interface changes.</li>
</ul>
<p><a name="1-2-x-removed-hooks"></a></p>
<ul>
<li>Removed slavePreLaunchDockerEnvironmentDecorator and slavePreLaunchDockerHook in favor of slavePreLaunchDockerTaskExecutorDecorator.</li>
</ul>
<p><a name="1-2-x-debug-endpoints"></a></p>
<ul>
<li>New Agent v1 operator API calls: LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT, ATTACH_CONTAINER_OUTPUT for debugging into running containers (Mesos containerizer only).</li>
</ul>
<p><a name="1-2-x-recovered-frameworks"></a></p>
<ul>
<li>Deprecated <code>recovered_frameworks</code> in v1 GetFrameworks call. Now it will be empty.</li>
</ul>
<p><a name="1-2-x-orphan-executors"></a></p>
<ul>
<li>Deprecated <code>orphan_executors</code> in v1 GetExecutors call. Now it will be empty.</li>
</ul>
<p><a name="1-2-x-orphan-tasks"></a></p>
<ul>
<li>Deprecated <code>orphan_tasks</code> in v1 GetTasks call. Now it will be empty.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents/schedulers can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-10x-to-11x"><a class="header" href="#upgrading-from-10x-to-11x">Upgrading from 1.0.x to 1.1.x</a></h2>
<p><a name="1-1-x-container-logger-interface"></a></p>
<ul>
<li>Mesos 1.1 removes the <code>ContainerLogger</code>'s <code>recover()</code> method.  The <code>ContainerLogger</code> had an incomplete interface for a stateful implementation.  This removes the incomplete parts to avoid adding tech debt in the containerizer.  Please see <a href="https://issues.apache.org/jira/browse/MESOS-6371">MESOS-6371</a> for more information.</li>
</ul>
<p><a name="1-1-x-allocator-updateallocation"></a></p>
<ul>
<li>Mesos 1.1 adds an <code>offeredResources</code> argument to the <code>Allocator::updateAllocation()</code> method. It is used to indicate the resources that the operations passed to <code>updateAllocation()</code> are applied to. <a href="https://issues.apache.org/jira/browse/MESOS-4431">MESOS-4431</a> (particularly <a href="https://reviews.apache.org/r/45961/">/r/45961/</a>) has more details on the motivation.</li>
</ul>
<h2 id="upgrading-from-028x-to-10x"><a class="header" href="#upgrading-from-028x-to-10x">Upgrading from 0.28.x to 1.0.x</a></h2>
<p><a name="1-0-x-deprecated-ssl-env-variables"></a></p>
<ul>
<li>Prior to Mesos 1.0, environment variables prefixed by <code>SSL_</code> are used to control libprocess SSL support. However, it was found that those environment variables may collide with some libraries or programs (e.g., openssl, curl). From Mesos 1.0, <code>SSL_*</code> environment variables are deprecated in favor of the corresponding <code>LIBPROCESS_SSL_*</code> variables.</li>
</ul>
<p><a name="1-0-x-persistent-volume-ownership"></a></p>
<ul>
<li>Prior to Mesos 1.0, Mesos agent recursively changes the ownership of the persistent volumes every time they are mounted to a container. From Mesos 1.0, this behavior has been changed. Mesos agent will do a <em>non-recursive</em> change of ownership of the persistent volumes.</li>
</ul>
<p><a name="1-0-x-deprecated-fields-in-container-config"></a></p>
<ul>
<li>Mesos 1.0 removed the camel cased protobuf fields in <code>ContainerConfig</code> (see <code>include/mesos/slave/isolator.proto</code>):
<ul>
<li><code>required ExecutorInfo executorInfo = 1;</code></li>
<li><code>optional TaskInfo taskInfo = 2;</code></li>
</ul>
</li>
</ul>
<p><a name="1-0-x-executor-environment-variables"></a></p>
<ul>
<li>By default, executors will no longer inherit environment variables from the agent. The operator can still use the <code>--executor_environment_variables</code> flag on the agent to explicitly specify what environment variables the executors will get. Mesos generated environment variables (i.e., <code>$MESOS_</code>, <code>$LIBPROCESS_</code>) will not be affected. If <code>$PATH</code> is not specified for an executor, a default value <code>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code> will be used.</li>
</ul>
<p><a name="1-0-x-allocator-metrics"></a></p>
<ul>
<li>The allocator metric named <code>allocator/event_queue_dispatches</code> is now deprecated. The new name is <code>allocator/mesos/event_queue_dispatches</code> to better support metrics for alternative allocator implementations.</li>
</ul>
<p><a name="1-0-x-docker-timeout-flag"></a></p>
<ul>
<li>The <code>--docker_stop_timeout</code> agent flag is deprecated.</li>
</ul>
<p><a name="1-0-x-executorinfo"></a></p>
<ul>
<li>The ExecutorInfo.source field is deprecated in favor of ExecutorInfo.labels.</li>
</ul>
<p><a name="1-0-x-slave"></a></p>
<ul>
<li>Mesos 1.0 deprecates the 'slave' keyword in favor of 'agent' in a number of places
<ul>
<li>Deprecated flags with keyword 'slave' in favor of 'agent'.</li>
<li>Deprecated sandbox links with 'slave' keyword in the WebUI.</li>
<li>Deprecated <code>slave</code> subcommand for mesos-cli.</li>
</ul>
</li>
</ul>
<p><a name="1-0-x-workdir"></a></p>
<ul>
<li>Mesos 1.0 removes the default value for the agent's <code>work_dir</code> command-line flag. This flag is now required; the agent will exit immediately if it is not provided.</li>
</ul>
<p><a name="1-0-x-registry-strict"></a></p>
<ul>
<li>Mesos 1.0 disables support for the master's <code>registry_strict</code> command-line flag. If this flag is specified, the master will exit immediately. Note that this flag was previously marked as experimental and not recommended for production use.</li>
</ul>
<p><a name="1-0-x-credentials-file"></a></p>
<ul>
<li>Mesos 1.0 deprecates the use of plain text credential files in favor of JSON-formatted credential files.</li>
</ul>
<p><a name="1-0-x-persistent-volume"></a></p>
<ul>
<li>When a persistent volume is destroyed, Mesos will now remove any data that was stored on the volume from the filesystem of the appropriate agent. In prior versions of Mesos, destroying a volume would not delete data (this was a known missing feature that has now been implemented).</li>
</ul>
<p><a name="1-0-x-status-code"></a></p>
<ul>
<li>Mesos 1.0 changes the HTTP status code of the following endpoints from <code>200 OK</code> to <code>202 Accepted</code>:
<ul>
<li><code>/reserve</code></li>
<li><code>/unreserve</code></li>
<li><code>/create-volumes</code></li>
<li><code>/destroy-volumes</code></li>
</ul>
</li>
</ul>
<p><a name="1-0-x-v1-commandinfo"></a></p>
<ul>
<li>Added <code>output_file</code> field to CommandInfo.URI in Scheduler API and v1 Scheduler HTTP API.</li>
</ul>
<p><a name="1-0-x-scheduler-proto"></a></p>
<ul>
<li>Changed Call and Event Type enums in scheduler.proto from required to optional for the purpose of backwards compatibility.</li>
</ul>
<p><a name="1-0-x-executor-proto"></a></p>
<ul>
<li>Changed Call and Event Type enums in executor.proto from required to optional for the purpose of backwards compatibility.</li>
</ul>
<p><a name="1-0-x-nonterminal"></a></p>
<ul>
<li>Added non-terminal task metadata to the container resource usage information.</li>
</ul>
<p><a name="1-0-x-observe-endpoint"></a></p>
<ul>
<li>Deleted the /observe HTTP endpoint.</li>
</ul>
<p><a name="1-0-x-quota-acls"></a></p>
<ul>
<li>The <code>SetQuota</code> and <code>RemoveQuota</code> ACLs have been deprecated. To replace these, a new ACL <code>UpdateQuota</code> have been introduced. In addition, a new ACL <code>GetQuota</code> have been added; these control which principals are allowed to query quota information for which roles. These changes affect the <code>--acls</code> flag for the local authorizer in the following ways:
<ul>
<li>The <code>update_quotas</code> ACL cannot be used in combination with either the <code>set_quotas</code> or <code>remove_quotas</code> ACL. The local authorizer will produce an error in such a case;</li>
<li>When upgrading a Mesos cluster that uses the <code>set_quotas</code> or <code>remove_quotas</code> ACLs, the operator should first upgrade the Mesos binaries. At this point, the deprecated ACLs will still be enforced. After the upgrade has been verified, the operator should replace deprecated values for <code>set_quotas</code> and <code>remove_quotas</code> with equivalent values for <code>update_quotas</code>;</li>
<li>If desired, the operator can use the <code>get_quotas</code> ACL after the upgrade to control which principals are allowed to query quota information.</li>
</ul>
</li>
</ul>
<p><a name="1-0-x-authorizer"></a></p>
<ul>
<li>Mesos 1.0 contains a number of authorizer changes that particularly effect custom authorizer modules:
<ul>
<li>The authorizer interface has been refactored in order to decouple the ACL definition language from the interface. It additionally includes the option of retrieving <code>ObjectApprover</code>. An <code>ObjectApprover</code> can be used to synchronously check authorizations for a given object and is hence useful when authorizing a large number of objects and/or large objects (which need to be copied using request-based authorization). NOTE: This is a <strong>breaking change</strong> for authorizer modules.</li>
<li>Authorization-based HTTP endpoint filtering enables operators to restrict which parts of the cluster state a user is authorized to see. Consider for example the <code>/state</code> master endpoint: an operator can now authorize users to only see a subset of the running frameworks, tasks, or executors.</li>
<li>The <code>subject</code> and <code>object</code> fields in the authorization::Request protobuf message have been changed to be optional. If these fields are not set, the request should only be allowed for ACLs with <code>ANY</code> semantics. NOTE: This is a semantic change for authorizer modules.</li>
</ul>
</li>
</ul>
<p><a name="1-0-x-allocator"></a></p>
<ul>
<li>Namespace and header file of <code>Allocator</code> has been moved to be consistent with other packages.</li>
</ul>
<p><a name="1-0-x-fetcher-user"></a></p>
<ul>
<li>When a task is run as a particular user, the fetcher now fetches files as that user also. Note, this means that filesystem permissions for that user will be enforced when fetching local files.</li>
</ul>
<p><a name="1-0-x-http-authentication-flags"></a></p>
<ul>
<li>The <code>--authenticate_http</code> flag has been deprecated in favor of <code>--authenticate_http_readwrite</code>. Setting <code>--authenticate_http_readwrite</code> will now enable authentication for all endpoints which previously had authentication support. These happen to be the endpoints which allow modification of the cluster state, or &quot;read-write&quot; endpoints. Note that <code>/logging/toggle</code>, <code>/profiler/start</code>, <code>/profiler/stop</code>, <code>/maintenance/schedule</code>, <code>/machine/up</code>, and <code>/machine/down</code> previously did not have authentication support, but in 1.0 if either <code>--authenticate_http</code> or <code>--authenticate_http_readwrite</code> is set, those endpoints will now require authentication. A new flag has also been introduced, <code>--authenticate_http_readonly</code>, which enables authentication for endpoints which support authentication and do not allow modification of the state of the cluster, like <code>/state</code> or <code>/flags</code>.</li>
</ul>
<p><a name="1-0-x-endpoint-authorization"></a></p>
<ul>
<li>
<p>Mesos 1.0 introduces authorization support for several HTTP endpoints. Note that some of these endpoints are used by the web UI, and thus using the web UI in a cluster with authorization enabled will require that ACLs be set appropriately. Please refer to the <a href="authorization.html">authorization documentation</a> for details.</p>
</li>
<li>
<p>The endpoints with coarse-grained authorization enabled are:</p>
<ul>
<li><code>/files/debug</code></li>
<li><code>/logging/toggle</code></li>
<li><code>/metrics/snapshot</code></li>
<li><code>/slave(id)/containers</code></li>
<li><code>/slave(id)/monitor/statistics</code></li>
</ul>
</li>
<li>
<p>If the defined ACLs used <code>permissive: false</code>, the listed HTTP endpoints will stop working unless ACLs for the <code>get_endpoints</code> actions are defined.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-027x-to-028x"><a class="header" href="#upgrading-from-027x-to-028x">Upgrading from 0.27.x to 0.28.x</a></h2>
<p><a name="0-28-x-resource-precision"></a></p>
<ul>
<li>Mesos 0.28 only supports three decimal digits of precision for scalar resource values. For example, frameworks can reserve &quot;0.001&quot; CPUs but more fine-grained reservations (e.g., &quot;0.0001&quot; CPUs) are no longer supported (although they did not work reliably in prior versions of Mesos anyway). Internally, resource math is now done using a fixed-point format that supports three decimal digits of precision, and then converted to/from floating point for input and output, respectively. Frameworks that do their own resource math and manipulate fractional resources may observe differences in roundoff error and numerical precision.</li>
</ul>
<p><a name="0-28-x-autherization-acls"></a></p>
<ul>
<li>Mesos 0.28 changes the definitions of two ACLs used for authorization. The objects of the <code>ReserveResources</code> and <code>CreateVolume</code> ACLs have been changed to <code>roles</code>. In both cases, principals can now be authorized to perform these operations for particular roles. This means that by default, a framework or operator can reserve resources/create volumes for any role. To restrict this behavior, <a href="authorization.html">ACLs can be added</a> to the master which authorize principals to reserve resources/create volumes for specified roles only. Previously, frameworks could only reserve resources for their own role; this behavior can be preserved by configuring the <code>ReserveResources</code> ACLs such that the framework's principal is only authorized to reserve for the framework's role. <strong>NOTE</strong> This renders existing <code>ReserveResources</code> and <code>CreateVolume</code> ACL definitions obsolete; if you are authorizing these operations, your ACL definitions should be updated.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-026x-to-027x"><a class="header" href="#upgrading-from-026x-to-027x">Upgrading from 0.26.x to 0.27.x</a></h2>
<p><a name="0-27-x-implicit-roles"></a></p>
<ul>
<li>Mesos 0.27 introduces the concept of <em>implicit roles</em>. In previous releases, configuring roles required specifying a static whitelist of valid role names on master startup (via the <code>--roles</code> flag). In Mesos 0.27, if <code>--roles</code> is omitted, <em>any</em> role name can be used; controlling which principals are allowed to register as which roles should be done using <a href="authorization.html">ACLs</a>. The role whitelist functionality is still supported but is deprecated.</li>
</ul>
<p><a name="0-27-x-allocator-api"></a></p>
<ul>
<li>The Allocator API has changed due to the introduction of implicit roles. Custom allocator implementations will need to be updated. See <a href="https://issues.apache.org/jira/browse/MESOS-4000">MESOS-4000</a> for more information.</li>
</ul>
<p><a name="0-27-x-executor-lost-callback"></a></p>
<ul>
<li>The <code>executorLost</code> callback in the Scheduler interface will now be called whenever the agent detects termination of a custom executor. This callback was never called in previous versions, so please make sure any framework schedulers can now safely handle this callback. Note that this callback may not be reliably delivered.</li>
</ul>
<p><a name="0-27-x-isolator-api"></a></p>
<ul>
<li>The isolator <code>prepare</code> interface has been changed slightly. Instead of keeping adding parameters to the <code>prepare</code> interface, we decide to use a protobuf (<code>ContainerConfig</code>). Also, we renamed <code>ContainerPrepareInfo</code> to <code>ContainerLaunchInfo</code> to better capture the purpose of this struct. See <a href="https://issues.apache.org/jira/browse/MESOS-4240">MESOS-4240</a> and <a href="https://issues.apache.org/jira/browse/MESOS-4282">MESOS-4282</a> for more information. If you are an isolator module writer, you will have to adjust your isolator module according to the new interface and re-compile with 0.27.</li>
</ul>
<p><a name="0-27-x-acl-shutdown-flag"></a></p>
<ul>
<li>
<p>ACLs.shutdown_frameworks has been deprecated in favor of the new ACLs.teardown_frameworks. This affects the <code>--acls</code> master flag for the local authorizer.</p>
</li>
<li>
<p>Reserved resources are now accounted for in the DRF role sorter. Previously unaccounted reservations will influence the weighted DRF sorter. If role weights were explicitly set, they may need to be adjusted in order to account for the reserved resources in the cluster.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-025x-to-026x"><a class="header" href="#upgrading-from-025x-to-026x">Upgrading from 0.25.x to 0.26.x</a></h2>
<p><a name="0-26-x-taskstatus-reason"></a></p>
<ul>
<li>
<p>The names of some TaskStatus::Reason enums have been changed. But the tag numbers remain unchanged, so it is backwards compatible. Frameworks using the new version might need to do some compile time adjustments:</p>
<ul>
<li>REASON_MEM_LIMIT -&gt; REASON_CONTAINER_LIMITATION_MEMORY</li>
<li>REASON_EXECUTOR_PREEMPTED -&gt; REASON_CONTAINER_PREEMPTED</li>
</ul>
</li>
</ul>
<p><a name="0-26-x-credential-protobuf"></a></p>
<ul>
<li>The <code>Credential</code> protobuf has been changed. <code>Credential</code> field <code>secret</code> is now a string, it used to be bytes. This will affect framework developers and language bindings ought to update their generated protobuf with the new version. This fixes JSON based credentials file support.</li>
</ul>
<p><a name="0-26-x-state-endpoint"></a></p>
<ul>
<li>The <code>/state</code> endpoints on master and agent will no longer include <code>data</code> fields as part of the JSON models for <code>ExecutorInfo</code> and <code>TaskInfo</code> out of consideration for memory scalability (see <a href="https://issues.apache.org/jira/browse/MESOS-3794">MESOS-3794</a> and <a href="http://www.mail-archive.com/dev@mesos.apache.org/msg33536.html">this email thread</a>).
<ul>
<li>On master, the affected <code>data</code> field was originally found via <code>frameworks[*].executors[*].data</code>.</li>
<li>On agents, the affected <code>data</code> field was originally found via <code>executors[*].tasks[*].data</code>.</li>
</ul>
</li>
</ul>
<p><a name="0-26-x-network-info-protobuf"></a></p>
<ul>
<li>The <code>NetworkInfo</code> protobuf has been changed. The fields <code>protocol</code> and <code>ip_address</code> are now deprecated. The new field <code>ip_addresses</code> subsumes the information provided by them.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-024x-to-025x"><a class="header" href="#upgrading-from-024x-to-025x">Upgrading from 0.24.x to 0.25.x</a></h2>
<p><a name="0-25-x-json-endpoints"></a></p>
<ul>
<li>
<p>The following endpoints will be deprecated in favor of new endpoints. Both versions will be available in 0.25 but the deprecated endpoints will be removed in a subsequent release.</p>
<p>For master endpoints:</p>
<ul>
<li>/state.json becomes /state</li>
<li>/tasks.json becomes /tasks</li>
</ul>
<p>For agent endpoints:</p>
<ul>
<li>/state.json becomes /state</li>
<li>/monitor/statistics.json becomes /monitor/statistics</li>
</ul>
<p>For both master and agent:</p>
<ul>
<li>/files/browse.json becomes /files/browse</li>
<li>/files/debug.json becomes /files/debug</li>
<li>/files/download.json becomes /files/download</li>
<li>/files/read.json becomes /files/read</li>
</ul>
</li>
</ul>
<p><a name="0-25-x-scheduler-bindings"></a></p>
<ul>
<li>The C++/Java/Python scheduler bindings have been updated. In particular, the driver can make a suppressOffers() call to stop receiving offers (until reviveOffers() is called).</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-023x-to-024x"><a class="header" href="#upgrading-from-023x-to-024x">Upgrading from 0.23.x to 0.24.x</a></h2>
<ul>
<li>
<p>Support for live upgrading a driver based scheduler to HTTP based (experimental) scheduler has been added.</p>
</li>
<li>
<p>Master now publishes its information in ZooKeeper in JSON (instead of protobuf). Make sure schedulers are linked against &gt;= 0.23.0 libmesos before upgrading the master.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-022x-to-023x"><a class="header" href="#upgrading-from-022x-to-023x">Upgrading from 0.22.x to 0.23.x</a></h2>
<ul>
<li>
<p>The 'stats.json' endpoints for masters and agents have been removed. Please use the 'metrics/snapshot' endpoints instead.</p>
</li>
<li>
<p>The '/master/shutdown' endpoint is deprecated in favor of the new '/master/teardown' endpoint.</p>
</li>
<li>
<p>In order to enable decorator modules to remove metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks in Mesos 0.23.0. Please refer to the modules documentation for more details.</p>
</li>
<li>
<p>Agent ping timeouts are now configurable on the master via <code>--slave_ping_timeout</code> and <code>--max_slave_ping_timeouts</code>. Agents should be upgraded to 0.23.x before changing these flags.</p>
</li>
<li>
<p>A new scheduler driver API, <code>acceptOffers</code>, has been introduced. This is a more general version of the <code>launchTasks</code> API, which allows the scheduler to accept an offer and specify a list of operations (Offer.Operation) to perform using the resources in the offer. Currently, the supported operations include LAUNCH (launching tasks), RESERVE (making dynamic reservations), UNRESERVE (releasing dynamic reservations), CREATE (creating persistent volumes) and DESTROY (releasing persistent volumes). Similar to the <code>launchTasks</code> API, any unused resources will be considered declined, and the specified filters will be applied on all unused resources.</p>
</li>
<li>
<p>The Resource protobuf has been extended to include more metadata for supporting persistence (DiskInfo), dynamic reservations (ReservationInfo) and oversubscription (RevocableInfo). You must not combine two Resource objects if they have different metadata.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Rebuild and install any modules so that upgraded masters/agents can use them.</li>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library / jar / egg (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg (if necessary).</li>
</ol>
<h2 id="upgrading-from-021x-to-022x"><a class="header" href="#upgrading-from-021x-to-022x">Upgrading from 0.21.x to 0.22.x</a></h2>
<ul>
<li>
<p>Agent checkpoint flag has been removed as it will be enabled for all
agents. Frameworks must still enable checkpointing during registration to take advantage
of checkpointing their tasks.</p>
</li>
<li>
<p>The stats.json endpoints for masters and agents have been deprecated.
Please refer to the metrics/snapshot endpoint.</p>
</li>
<li>
<p>The C++/Java/Python scheduler bindings have been updated. In particular, the driver can be constructed with an additional argument that specifies whether to use implicit driver acknowledgements. In <code>statusUpdate</code>, the <code>TaskStatus</code> now includes a UUID to make explicit acknowledgements possible.</p>
</li>
<li>
<p>The Authentication API has changed slightly in this release to support additional authentication mechanisms. The change from 'string' to 'bytes' for AuthenticationStartMessage.data has no impact on C++ or the over-the-wire representation, so it only impacts pure language bindings for languages like Java and Python that use different types for UTF-8 strings vs. byte arrays.</p>
<p>message AuthenticationStartMessage {
required string mechanism = 1;
optional bytes data = 2;
}</p>
</li>
<li>
<p>All Mesos arguments can now be passed using file:// to read them out of a file (either an absolute or relative path). The --credentials, --whitelist, and any flags that expect JSON backed arguments (such as --modules) behave as before, although support for just passing an absolute path for any JSON flags rather than file:// has been deprecated and will produce a warning (and the absolute path behavior will be removed in a future release).</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers:</li>
</ol>
<ul>
<li>For Java schedulers, link the new native library against the new JAR. The JAR contains API above changes. A 0.21.0 JAR will work with a 0.22.0 libmesos. A 0.22.0 JAR will work with a 0.21.0 libmesos if explicit acks are not being used. 0.22.0 and 0.21.0 are inter-operable at the protocol level between the master and the scheduler.</li>
<li>For Python schedulers, upgrade to use a 0.22.0 egg. If constructing <code>MesosSchedulerDriverImpl</code> with <code>Credentials</code>, your code must be updated to pass the <code>implicitAcknowledgements</code> argument before <code>Credentials</code>. You may run a 0.21.0 Python scheduler against a 0.22.0 master, and vice versa.</li>
</ul>
<ol start="4">
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library / jar / egg.</li>
</ol>
<h2 id="upgrading-from-020x-to-021x"><a class="header" href="#upgrading-from-020x-to-021x">Upgrading from 0.20.x to 0.21.x</a></h2>
<ul>
<li>Disabling agent checkpointing has been deprecated; the agent --checkpoint flag has been deprecated and will be removed in a future release.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library (mesos jar upgrade not necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-019x-to-020x"><a class="header" href="#upgrading-from-019x-to-020x">Upgrading from 0.19.x to 0.20.x.</a></h2>
<ul>
<li>
<p>The Mesos API has been changed slightly in this release. The CommandInfo has been changed (see below), which makes launching a command more flexible. The 'value' field has been changed from <em>required</em> to <em>optional</em>. However, it will not cause any issue during the upgrade (since the existing schedulers always set this field).</p>
<pre><code>  message CommandInfo {
    ...
    // There are two ways to specify the command:
    // 1) If 'shell == true', the command will be launched via shell
    //    (i.e., /bin/sh -c 'value'). The 'value' specified will be
    //    treated as the shell command. The 'arguments' will be ignored.
    // 2) If 'shell == false', the command will be launched by passing
    //    arguments to an executable. The 'value' specified will be
    //    treated as the filename of the executable. The 'arguments'
    //    will be treated as the arguments to the executable. This is
    //    similar to how POSIX exec families launch processes (i.e.,
    //    execlp(value, arguments(0), arguments(1), ...)).
    optional bool shell = 6 [default = true];
    optional string value = 3;
    repeated string arguments = 7;
    ...
  }
</code></pre>
</li>
<li>
<p>The Python bindings are also changing in this release. There are now sub-modules which allow you to use either the interfaces and/or the native driver.</p>
<ul>
<li><code>import mesos.native</code> for the native drivers</li>
<li><code>import mesos.interface</code> for the stub implementations and protobufs</li>
</ul>
<p>To ensure a smooth upgrade, we recommend to upgrade your python framework and executor first. You will be able to either import using the new configuration or the old. Replace the existing imports with something like the following:</p>
<p>try:
from mesos.native import MesosExecutorDriver, MesosSchedulerDriver
from mesos.interface import Executor, Scheduler
from mesos.interface import mesos_pb2
except ImportError:
from mesos import Executor, MesosExecutorDriver, MesosSchedulerDriver, Scheduler
import mesos_pb2</p>
</li>
<li>
<p>If you're using a pure language binding, please ensure that it sends status update acknowledgements through the master before upgrading.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library (install the latest mesos jar and python egg if necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library (install the latest mesos jar and python egg if necessary).</li>
</ol>
<h2 id="upgrading-from-018x-to-019x"><a class="header" href="#upgrading-from-018x-to-019x">Upgrading from 0.18.x to 0.19.x.</a></h2>
<ul>
<li>
<p>There are new required flags on the master (<code>--work_dir</code> and <code>--quorum</code>) to support the <em>Registrar</em> feature, which adds replicated state on the masters.</p>
</li>
<li>
<p>No required upgrade ordering across components.</p>
</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the schedulers by linking the latest native library (mesos jar upgrade not necessary).</li>
<li>Restart the schedulers.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0170-to-018x"><a class="header" href="#upgrading-from-0170-to-018x">Upgrading from 0.17.0 to 0.18.x.</a></h2>
<ul>
<li>This upgrade requires a system reboot for agents that use Linux cgroups for isolation.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries then perform one of the following two steps, depending on if cgroups isolation is used:</li>
</ol>
<ul>
<li>[no cgroups]
<ul>
<li>Restart the agents. The &quot;--isolation&quot; flag has changed and &quot;process&quot; has been deprecated in favor of &quot;posix/cpu,posix/mem&quot;.</li>
</ul>
</li>
<li>[cgroups]
<ul>
<li>Change from a single mountpoint for all controllers to separate mountpoints for each controller, e.g., /sys/fs/cgroup/memory/ and /sys/fs/cgroup/cpu/.</li>
<li>The suggested configuration is to mount a tmpfs filesystem to /sys/fs/cgroup and to let the agent mount the required controllers. However, the agent will also use previously mounted controllers if they are appropriately mounted under &quot;--cgroups_hierarchy&quot;.</li>
<li>It has been observed that unmounting and remounting of cgroups from the single to separate configuration is unreliable and a reboot into the new configuration is strongly advised. Restart the agents after reboot.</li>
<li>The &quot;--cgroups_hierarchy&quot; now defaults to &quot;/sys/fs/cgroup&quot;. The &quot;--cgroups_root&quot; flag default remains &quot;mesos&quot;.</li>
<li>The &quot;--isolation&quot; flag has changed and &quot;cgroups&quot; has been deprecated in favor of &quot;cgroups/cpu,cgroups/mem&quot;.</li>
<li>The &quot;--cgroup_subsystems&quot; flag is no longer required and will be ignored.</li>
</ul>
</li>
</ul>
<ol start="5">
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0160-to-0170"><a class="header" href="#upgrading-from-0160-to-0170">Upgrading from 0.16.0 to 0.17.0.</a></h2>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0150-to-0160"><a class="header" href="#upgrading-from-0150-to-0160">Upgrading from 0.15.0 to 0.16.0.</a></h2>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0140-to-0150"><a class="header" href="#upgrading-from-0140-to-0150">Upgrading from 0.14.0 to 0.15.0.</a></h2>
<ul>
<li>Schedulers should implement the new <code>reconcileTasks</code> driver method.</li>
<li>Schedulers should call the new <code>MesosSchedulerDriver</code> constructor that takes <code>Credential</code> to authenticate.</li>
<li>--authentication=false (default) allows both authenticated and unauthenticated frameworks to register.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries.</li>
<li>Restart the masters with --credentials pointing to credentials of the framework(s).</li>
<li>Install the new agent binaries and restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.
Restart the masters with --authentication=true.</li>
</ol>
<p>NOTE: After the restart unauthenticated frameworks <em>will not</em> be allowed to register.</p>
<h2 id="upgrading-from-0130-to-0140"><a class="header" href="#upgrading-from-0130-to-0140">Upgrading from 0.13.0 to 0.14.0.</a></h2>
<ul>
<li>/vars endpoint has been removed.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
<li>Install the new agent binaries.</li>
<li>Restart the agents after adding --checkpoint flag to enable checkpointing.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Set FrameworkInfo.checkpoint in the scheduler if checkpointing is desired (recommended).</li>
<li>Restart the schedulers.</li>
<li>Restart the masters (to get rid of the cached FrameworkInfo).</li>
<li>Restart the agents (to get rid of the cached FrameworkInfo).</li>
</ol>
<h2 id="upgrading-from-0120-to-0130"><a class="header" href="#upgrading-from-0120-to-0130">Upgrading from 0.12.0 to 0.13.0.</a></h2>
<ul>
<li>cgroups_hierarchy_root agent flag is renamed as cgroups_hierarchy</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new master binaries and restart the masters.</li>
<li>Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).</li>
<li>Restart the schedulers.</li>
<li>Install the new agent binaries.</li>
<li>Restart the agents.</li>
<li>Upgrade the executors by linking the latest native library and mesos jar (if necessary).</li>
</ol>
<h2 id="upgrading-from-0110-to-0120"><a class="header" href="#upgrading-from-0110-to-0120">Upgrading from 0.11.0 to 0.12.0.</a></h2>
<ul>
<li>If you are a framework developer, you will want to examine the new 'source' field in the ExecutorInfo protobuf. This will allow you to take further advantage of the resource monitoring.</li>
</ul>
<p>In order to upgrade a running cluster:</p>
<ol>
<li>Install the new agent binaries and restart the agents.</li>
<li>Install the new master binaries and restart the masters.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="downgrade-mesos"><a class="header" href="#downgrade-mesos">Downgrade Mesos</a></h1>
<p>This document serves as a guide for users who wish to downgrade from an
existing Mesos cluster to a previous version. This usually happens when
rolling back from problematic upgrades. Mesos provides compatibility
between any 1.x and 1.y versions of masters/agents as long as new features
are not used. Since Mesos 1.8, we introduced a check for minimum capabilities
on the master. If a backwards incompatible feature is used, a corresponding
minimum capability entry will be persisted to the registry. If an old master
(that does not possess the capability) tries to recover from the registry
(e.g. when rolling back), an error message will be printed containing the
missing capabilities. This document lists the detailed information regarding
these minimum capabilities and remediation for downgrade errors.</p>
<h2 id="list-of-master-minimum-capabilities"><a class="header" href="#list-of-master-minimum-capabilities">List of Master Minimum Capabilities</a></h2>
<table class="table table-striped">
<thead>
<tr><th>Capability</th><th>Description</th>
</thead>
<tr>
  <td>
    <code>AGENT_DRAINING</code>
  </td>
  <td>
    This capability is required when any agent is marked for draining
    or deactivated.  These states were added in Mesos 1.9 and are
    triggered by using the <code>DRAIN_AGENT</code> or
    <code>DEACTIVATE_AGENT</code> operator APIs.
    <br/>
    To remove this minimum capability requirement:
    <ol>
      <li>
        Stop the master downgrade and return to the more recent version.
      </li>
      <li>
        Find all agents that are marked for draining or deactivated.
        This can be done by using the <code>GET_AGENTS</code> operator
        API and checking the <code>deactivated</code> boolean field of
        each agent.  All draining agents will also be deactivated.
      </li>
      <li>
        Use the <code>REACTIVATE_AGENT</code> operator API for each
        deactivated agent.
      </li>
    </ol>
  </td>
</tr>
<tr>
  <td>
    <code>QUOTA_V2</code>
  </td>
  <td>
    This capability is required when quota is configured in Mesos 1.9 or
    higher. When that happens, the newly configured quota will be persisted
    in the <code>quota_configs</code> field in the registry which requires this
    capability to decode.
    <br/>
    To remove this minimum capability requirement:
    <ol>
      <li>
        Stop the master downgrade and return to the more recent version.
      </li>
      <li>
        Use the <code>/registrar(id)/registry</code> endpoint to read the
        registry content and identify roles listed under the
        <code>quota_configs</code> field.
      </li>
      <li>
        Reset those roles' quota back to default (no guarantees and no limits).
        This will remove the roles from the <code>quota_configs</code> field.
        Once <code>quota_configs</code> becomes empty, the capability
        requirement will be removed.
      </li>
    </ol>
  </td>
</tr>
</table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logging"><a class="header" href="#logging">Logging</a></h1>
<p>Mesos handles the logs of each Mesos component differently depending on the
degree of control Mesos has over the source code of the component.</p>
<p>Roughly, these categories are:</p>
<ul>
<li><a href="logging.html#Internal">Internal</a> - Master and Agent.</li>
<li><a href="logging.html#Containers">Containers</a> - Executors and Tasks.</li>
<li>External - Components launched outside of Mesos, like
Frameworks and <a href="high-availability.html">ZooKeeper</a>.  These are expected to
implement their own logging solution.</li>
</ul>
<h2 id="internal"><a class="header" href="#internal"><a name="Internal"></a>Internal</a></h2>
<p>The Mesos Master and Agent use the
<a href="https://github.com/google/glog">Google's logging library</a>.
For information regarding the command-line options used to configure this
library, see the
<a href="configuration/master-and-agent.html#logging-options">configuration documentation</a>.
Google logging options that are not explicitly mentioned there can be
configured via environment variables.</p>
<p>Both Master and Agent also expose a <a href="endpoints/logging/toggle.html">/logging/toggle</a>
HTTP endpoint which temporarily toggles verbose logging:</p>
<pre><code>POST &lt;ip:port&gt;/logging/toggle?level=[1|2|3]&amp;duration=VALUE
</code></pre>
<p>The effect is analogous to setting the <code>GLOG_v</code> environment variable prior
to starting the Master/Agent, except the logging level will revert to the
original level after the given duration.</p>
<h2 id="containers"><a class="header" href="#containers"><a name="Containers"></a>Containers</a></h2>
<p>For background, see <a href="containerizers.html">the containerizer documentation</a>.</p>
<p>Mesos does not assume any structured logging for entities running inside
containers.  Instead, Mesos will store the stdout and stderr of containers
into plain files (&quot;stdout&quot; and &quot;stderr&quot;) located inside
<a href="sandbox.html#where-is-it">the sandbox</a>.</p>
<p>In some cases, the default Container logger behavior of Mesos is not ideal:</p>
<ul>
<li>Logging may not be standardized across containers.</li>
<li>Logs are not easily aggregated.</li>
<li>Log file sizes are not managed.  Given enough time, the &quot;stdout&quot; and &quot;stderr&quot;
files can fill up the Agent's disk.</li>
</ul>
<h2 id="containerlogger-module"><a class="header" href="#containerlogger-module"><code>ContainerLogger</code> Module</a></h2>
<p>The <code>ContainerLogger</code> module was introduced in Mesos 0.27.0 and aims to address
the shortcomings of the default logging behavior for containers.  The module
can be used to change how Mesos redirects the stdout and stderr of containers.</p>
<p>The <a href="https://github.com/apache/mesos/blob/master/include/mesos/slave/container_logger.hpp">interface for a <code>ContainerLogger</code> can be found here</a>.</p>
<p>Mesos comes with two <code>ContainerLogger</code> modules:</p>
<ul>
<li>The <code>SandboxContainerLogger</code> implements the existing logging behavior as
a <code>ContainerLogger</code>.  This is the default behavior.</li>
<li>The <code>LogrotateContainerLogger</code> addresses the problem of unbounded log file
sizes.</li>
</ul>
<h3 id="logrotatecontainerlogger"><a class="header" href="#logrotatecontainerlogger"><code>LogrotateContainerLogger</code></a></h3>
<p>The <code>LogrotateContainerLogger</code> constrains the total size of a container's
stdout and stderr files.  The module does this by rotating log files based
on the parameters to the module.  When a log file reaches its specified
maximum size, it is renamed by appending a <code>.N</code> to the end of the filename,
where <code>N</code> increments each rotation.  Older log files are deleted when the
specified maximum number of files is reached.</p>
<h4 id="invoking-the-module"><a class="header" href="#invoking-the-module">Invoking the module</a></h4>
<p>The <code>LogrotateContainerLogger</code> can be loaded by specifying the library
<code>liblogrotate_container_logger.so</code> in the
<a href="modules.html#Invoking"><code>--modules</code> flag</a> when starting the Agent and by
setting the <code>--container_logger</code> Agent flag to
<code>org_apache_mesos_LogrotateContainerLogger</code>.</p>
<h4 id="module-parameters"><a class="header" href="#module-parameters">Module parameters</a></h4>
<table class="table table-striped">
  <thead>
    <tr>
      <th width="30%">
        Key
      </th>
      <th>
        Explanation
      </th>
    </tr>
  </thead>
<tr>
    <td>
      <code>max_stdout_size</code>/<code>max_stderr_size</code>
    </td>
    <td>
      Maximum size, in bytes, of a single stdout/stderr log file.
      When the size is reached, the file will be rotated.
<pre><code>  Defaults to 10 MB.  Minimum size of 1 (memory) page, usually around 4 KB.
&lt;/td&gt;
</code></pre>
</tr>
<tr>
    <td>
      <code>logrotate_stdout_options</code>/
      <code>logrotate_stderr_options</code>
    </td>
    <td>
      Additional config options to pass into <code>logrotate</code> for stdout.
      This string will be inserted into a <code>logrotate</code> configuration
      file. i.e. For "stdout":
      <pre>
/path/to/stdout {
  [logrotate_stdout_options]
  size [max_stdout_size]
}</pre>
      NOTE: The <code>size</code> option will be overridden by this module.
    </td>
  </tr>
<tr>
    <td>
      <code>environment_variable_prefix</code>
    </td>
    <td>
      Prefix for environment variables meant to modify the behavior of
      the logrotate logger for the specific container being launched.
      The logger will look for four prefixed environment variables in the
      container's <code>CommandInfo</code>'s <code>Environment</code>:
      <ul>
        <li><code>MAX_STDOUT_SIZE</code></li>
        <li><code>LOGROTATE_STDOUT_OPTIONS</code></li>
        <li><code>MAX_STDERR_SIZE</code></li>
        <li><code>LOGROTATE_STDERR_OPTIONS</code></li>
      </ul>
      If present, these variables will overwrite the global values set
      via module parameters.
<pre><code>  Defaults to &lt;code&gt;CONTAINER_LOGGER_&lt;/code&gt;.
&lt;/td&gt;
</code></pre>
</tr>
<tr>
    <td>
      <code>launcher_dir</code>
    </td>
    <td>
      Directory path of Mesos binaries.
      The <code>LogrotateContainerLogger</code> will find the
      <code>mesos-logrotate-logger</code> binary under this directory.
<pre><code>  Defaults to &lt;code&gt;/usr/local/libexec/mesos&lt;/code&gt;.
&lt;/td&gt;
</code></pre>
</tr>
<tr>
    <td>
      <code>logrotate_path</code>
    </td>
    <td>
      If specified, the <code>LogrotateContainerLogger</code> will use the
      specified <code>logrotate</code> instead of the system's
      <code>logrotate</code>.  If <code>logrotate</code> is not found, then
      the module will exit with an error.
    </td>
  </tr>
</table>
<h4 id="how-it-works"><a class="header" href="#how-it-works">How it works</a></h4>
<ol>
<li>Every time a container starts up, the <code>LogrotateContainerLogger</code>
starts up companion subprocesses of the <code>mesos-logrotate-logger</code> binary.</li>
<li>The module instructs Mesos to redirect the container's stdout/stderr
to the <code>mesos-logrotate-logger</code>.</li>
<li>As the container outputs to stdout/stderr, <code>mesos-logrotate-logger</code> will
pipe the output into the &quot;stdout&quot;/&quot;stderr&quot; files.  As the files grow,
<code>mesos-logrotate-logger</code> will call <code>logrotate</code> to keep the files strictly
under the configured maximum size.</li>
<li>When the container exits, <code>mesos-logrotate-logger</code> will finish logging before
exiting as well.</li>
</ol>
<p>The <code>LogrotateContainerLogger</code> is designed to be resilient across Agent
failover.  If the Agent process dies, any instances of <code>mesos-logrotate-logger</code>
will continue to run.</p>
<h3 id="writing-a-custom-containerlogger"><a class="header" href="#writing-a-custom-containerlogger">Writing a Custom <code>ContainerLogger</code></a></h3>
<p>For basics on module writing, see <a href="modules.html">the modules documentation</a>.</p>
<p>There are several caveats to consider when designing a new <code>ContainerLogger</code>:</p>
<ul>
<li>Logging by the <code>ContainerLogger</code> should be resilient to Agent failover.
If the Agent process dies (which includes the <code>ContainerLogger</code> module),
logging should continue.  This is usually achieved by using subprocesses.</li>
<li>When containers shut down, the <code>ContainerLogger</code> is not explicitly notified.
Instead, encountering <code>EOF</code> in the container's stdout/stderr signifies
that the container has exited.  This provides a stronger guarantee that the
<code>ContainerLogger</code> has seen all the logs before exiting itself.</li>
<li>The <code>ContainerLogger</code> should not assume that containers have been launched
with any specific <code>ContainerLogger</code>.  The Agent may be restarted with a
different <code>ContainerLogger</code>.</li>
<li>Each <a href="containerizers.html">containerizer</a> running on an Agent uses its own
instance of the <code>ContainerLogger</code>.  This means more than one <code>ContainerLogger</code>
may be running in a single Agent.  However, each Agent will only run a single
type of <code>ContainerLogger</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-observability-metrics"><a class="header" href="#mesos-observability-metrics">Mesos Observability Metrics</a></h1>
<p>This document describes the observability metrics provided by Mesos master and
agent nodes. This document also provides some initial guidance on which metrics
you should monitor to detect abnormal situations in your cluster.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Mesos master and agent nodes report a set of statistics and metrics that enable
cluster operators to monitor resource usage and detect abnormal situations early. The
information reported by Mesos includes details about available resources, used
resources, registered frameworks, active agents, and task state. You can use
this information to create automated alerts and to plot different metrics over
time inside a monitoring dashboard.</p>
<p>Metric information is not persisted to disk at either master or agent
nodes, which means that metrics will be reset when masters and agents
are restarted. Similarly, if the current leading master fails and a new
leading master is elected, metrics at the new master will be reset.</p>
<h2 id="metric-types"><a class="header" href="#metric-types">Metric Types</a></h2>
<p>Mesos provides two different kinds of metrics: counters and gauges.</p>
<p><strong>Counters</strong> keep track of discrete events and are monotonically increasing. The
value of a metric of this type is always a natural number. Examples include the
number of failed tasks and the number of agent registrations. For some metrics
of this type, the rate of change is often more useful than the value itself.</p>
<p><strong>Gauges</strong> represent an instantaneous sample of some magnitude. Examples include
the amount of used memory in the cluster and the number of connected agents. For
some metrics of this type, it is often useful to determine whether the value is
above or below a threshold for a sustained period of time.</p>
<p>The tables in this document indicate the type of each available metric.</p>
<h2 id="master-nodes"><a class="header" href="#master-nodes">Master Nodes</a></h2>
<p>Metrics from each master node are available via the
<a href="endpoints/metrics/snapshot.html">/metrics/snapshot</a> master endpoint.  The response
is a JSON object that contains metrics names and values as key-value pairs.</p>
<h3 id="observability-metrics"><a class="header" href="#observability-metrics">Observability metrics</a></h3>
<p>This section lists all available metrics from Mesos master nodes grouped by
category.</p>
<h4 id="resources"><a class="header" href="#resources">Resources</a></h4>
<p>The following metrics provide information about the total resources available in
the cluster and their current usage. High resource usage for sustained periods
of time may indicate that you need to add capacity to your cluster or that a
framework is misbehaving.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/cpus_percent</code>
  </td>
  <td>Percentage of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_used</code>
  </td>
  <td>Number of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_total</code>
  </td>
  <td>Number of CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_revocable_total</code>
  </td>
  <td>Number of revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/cpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_percent</code>
  </td>
  <td>Percentage of allocated disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_used</code>
  </td>
  <td>Allocated disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_total</code>
  </td>
  <td>Disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_revocable_total</code>
  </td>
  <td>Revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/disk_revocable_used</code>
  </td>
  <td>Allocated revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_percent</code>
  </td>
  <td>Percentage of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_used</code>
  </td>
  <td>Number of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_total</code>
  </td>
  <td>Number of GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_revocable_total</code>
  </td>
  <td>Number of revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/gpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_percent</code>
  </td>
  <td>Percentage of allocated memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_used</code>
  </td>
  <td>Allocated memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_total</code>
  </td>
  <td>Memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_revocable_total</code>
  </td>
  <td>Revocable memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/mem_revocable_used</code>
  </td>
  <td>Allocated revocable memory in MB</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="master"><a class="header" href="#master">Master</a></h4>
<p>The following metrics provide information about whether a master is currently
elected and how long it has been running. A cluster with no elected master
for sustained periods of time indicates a malfunctioning cluster. This
points to either leadership election issues (so check the connection to
ZooKeeper) or a flapping Master process. A low uptime value indicates that the
master has restarted recently.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/elected</code>
  </td>
  <td>Whether this is the elected master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/uptime_secs</code>
  </td>
  <td>Uptime in seconds</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="system"><a class="header" href="#system">System</a></h4>
<p>The following metrics provide information about the resources available on this
master node and their current usage. High resource usage in a master node for
sustained periods of time may degrade the performance of the cluster.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>system/cpus_total</code>
  </td>
  <td>Number of CPUs available in this master node</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_15min</code>
  </td>
  <td>Load average for the past 15 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_5min</code>
  </td>
  <td>Load average for the past 5 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_1min</code>
  </td>
  <td>Load average for the past minute</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_free_bytes</code>
  </td>
  <td>Free memory in bytes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_total_bytes</code>
  </td>
  <td>Total memory in bytes</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="agents"><a class="header" href="#agents">Agents</a></h4>
<p>The following metrics provide information about agent events, agent counts, and
agent states. A low number of active agents may indicate that agents are
unhealthy or that they are not able to connect to the elected master.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/slave_registrations</code>
  </td>
  <td>Number of agents that were able to cleanly re-join the cluster and
      connect back to the master after the master is disconnected.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals</code>
  </td>
  <td>Number of agent removed for various reasons, including maintenance</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_reregistrations</code>
  </td>
  <td>Number of agent re-registrations</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_unreachable_scheduled</code>
  </td>
  <td>Number of agents which have failed their health check and are scheduled
      to be marked unreachable. They will not be marked unreachable immediately due to the Agent
      Removal Rate-Limit, but <code>master/slave_unreachable_completed</code>
      will start increasing as they do get removed.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_unreachable_canceled</code>
  </td>
  <td>Number of times that an agent was due to be marked unreachable but this
      transition was cancelled. This happens when the agent removal rate limit
      is enabled and the agent sends a <code>PONG</code> response message to the
      master before the rate limit allows the agent to be marked unreachable.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_unreachable_completed</code>
  </td>
  <td>Number of agents that were marked as unreachable because they failed
      health checks. These are agents which were not heard from despite the
      agent-removal rate limit, and have been marked as unreachable in the
      master's agent registry.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slaves_active</code>
  </td>
  <td>Number of active agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_connected</code>
  </td>
  <td>Number of connected agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_disconnected</code>
  </td>
  <td>Number of disconnected agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_inactive</code>
  </td>
  <td>Number of inactive agents</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/slaves_unreachable</code>
  </td>
  <td>Number of unreachable agents. Unreachable agents are periodically
      garbage collected from the registry, which will cause this value to
      decrease.</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="frameworks"><a class="header" href="#frameworks">Frameworks</a></h4>
<p>The following metrics provide information about the registered frameworks in the
cluster. No active or connected frameworks may indicate that a scheduler is not
registered or that it is misbehaving.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/frameworks_active</code>
  </td>
  <td>Number of active frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks_connected</code>
  </td>
  <td>Number of connected frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks_disconnected</code>
  </td>
  <td>Number of disconnected frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks_inactive</code>
  </td>
  <td>Number of inactive frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/outstanding_offers</code>
  </td>
  <td>Number of outstanding resource offers</td>
  <td>Gauge</td>
</tr>
</table>
<p>The following metrics are added for each framework which registers with the
master, in order to provide detailed information about the behavior of the
framework. The framework name is percent-encoded before creating these metrics;
the actual name can be recovered by percent-decoding.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/subscribed</code>
  </td>
  <td>Whether or not this framework is currently subscribed</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/calls</code>
  </td>
  <td>Total number of calls sent by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/calls/&lt;CALL_TYPE&gt;</code>
  </td>
  <td>Number of each type of call sent by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/events</code>
  </td>
  <td>Total number of events sent to this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/events/&lt;EVENT_TYPE&gt;</code>
  </td>
  <td>Number of each type of event sent to this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/operations</code>
  </td>
  <td>Total number of offer operations performed by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/operations/&lt;OPERATION_TYPE&gt;</code>
  </td>
  <td>Number of each type of offer operation performed by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/tasks/active/&lt;TASK_STATE&gt;</code>
  </td>
  <td>Number of this framework's tasks currently in each active task state</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/tasks/terminal/&lt;TASK_STATE&gt;</code>
  </td>
  <td>Number of this framework's tasks which have transitioned into each terminal task state</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/sent</code>
  </td>
  <td>Number of offers sent to this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/accepted</code>
  </td>
  <td>Number of offers accepted by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/declined</code>
  </td>
  <td>Number of offers explicitly declined by this framework</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/offers/rescinded</code>
  </td>
  <td>Number of offers sent to this framework which were subsequently rescinded</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/frameworks/&lt;ENCODED_FRAMEWORK_NAME&gt;/&lt;FRAMEWORK_ID&gt;/roles/&lt;ROLE_NAME&gt;/suppressed</code>
  </td>
  <td>For each of the framework's subscribed roles, whether or not offers for that role are currently suppressed</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="tasks"><a class="header" href="#tasks">Tasks</a></h4>
<p>The following metrics provide information about active and terminated tasks. A
high rate of lost tasks may indicate that there is a problem with the cluster.
The task states listed here match those of the task state machine.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/tasks_error</code>
  </td>
  <td>Number of tasks that were invalid</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_failed</code>
  </td>
  <td>Number of failed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_finished</code>
  </td>
  <td>Number of finished tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_killed</code>
  </td>
  <td>Number of killed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_killing</code>
  </td>
  <td>Number of tasks currently being killed</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_lost</code>
  </td>
  <td>Number of lost tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/tasks_running</code>
  </td>
  <td>Number of running tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_staging</code>
  </td>
  <td>Number of staging tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_starting</code>
  </td>
  <td>Number of starting tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/tasks_unreachable</code>
  </td>
  <td>Number of unreachable tasks</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="operations"><a class="header" href="#operations">Operations</a></h4>
<p>The following metrics provide information about offer operations on the master.</p>
<p>Below, <code>OPERATION_TYPE</code> refers to any one of <code>reserve</code>, <code>unreserve</code>, <code>create</code>,
<code>destroy</code>, <code>grow_volume</code>, <code>shrink_volume</code>, <code>create_disk</code> or <code>destroy_disk</code>.</p>
<p>NOTE: The counter for terminal operation states can over-count over time. In
particular if an agent contained unacknowledged terminal status updates when
it was marked gone or marked unreachable, these operations will be double-counted
as both their original state and <code>OPERATION_GONE</code>/<code>OPERATION_UNREACHABLE</code>.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/operations/total</code>
  </td>
  <td>Total number of operations known to this master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations in the given non-terminal state (`pending`, `recovering` or `unreachable`)</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations in the given terminal state (`finished`, `error`, `dropped` or `gone_by_operator`)</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_TYPE&gt;/total</code>
  </td>
  <td>Total number of operations with the given type known to this master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_TYPE&gt;/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations with the given type in the given non-terminal state (`pending`, `recovering` or `unreachable`)</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operations/&lt;OPERATION_TYPE&gt;/&lt;OPERATION_STATE&gt;</code>
  </td>
  <td>Number of operations with the given type in the given state (`finished`, `error`, `dropped` or `gone_by_operator`)</td>
  <td>Counter</td>
</tr>
</table>
<h4 id="messages"><a class="header" href="#messages">Messages</a></h4>
<p>The following metrics provide information about messages between the master and
the agents and between the framework and the executors. A high rate of dropped
messages may indicate that there is a problem with the network.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/invalid_executor_to_framework_messages</code>
  </td>
  <td>Number of invalid executor to framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_framework_to_executor_messages</code>
  </td>
  <td>Number of invalid framework to executor messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_operation_status_update_acknowledgements</code>
  </td>
  <td>Number of invalid operation status update acknowledgements</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_status_update_acknowledgements</code>
  </td>
  <td>Number of invalid status update acknowledgements</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/invalid_status_updates</code>
  </td>
  <td>Number of invalid status updates</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/dropped_messages</code>
  </td>
  <td>Number of dropped messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_authenticate</code>
  </td>
  <td>Number of authentication messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_deactivate_framework</code>
  </td>
  <td>Number of framework deactivation messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_decline_offers</code>
  </td>
  <td>Number of offers declined</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_executor_to_framework</code>
  </td>
  <td>Number of executor to framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_exited_executor</code>
  </td>
  <td>Number of terminated executor messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_framework_to_executor</code>
  </td>
  <td>Number of messages from a framework to an executor</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_kill_task</code>
  </td>
  <td>Number of kill task messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_launch_tasks</code>
  </td>
  <td>Number of launch task messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_operation_status_update_acknowledgement</code>
  </td>
  <td>Number of operation status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reconcile_operations</code>
  </td>
  <td>Number of reconcile operations messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reconcile_tasks</code>
  </td>
  <td>Number of reconcile task messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_register_framework</code>
  </td>
  <td>Number of framework registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_register_slave</code>
  </td>
  <td>Number of agent registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reregister_framework</code>
  </td>
  <td>Number of framework re-registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_reregister_slave</code>
  </td>
  <td>Number of agent re-registration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_resource_request</code>
  </td>
  <td>Number of resource request messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_revive_offers</code>
  </td>
  <td>Number of offer revival messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_status_update</code>
  </td>
  <td>Number of status update messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_status_update_acknowledgement</code>
  </td>
  <td>Number of status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_unregister_framework</code>
  </td>
  <td>Number of framework unregistration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_unregister_slave</code>
  </td>
  <td>Number of agent unregistration messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/messages_update_slave</code>
  </td>
  <td>Number of update agent messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/recovery_slave_removals</code>
  </td>
  <td>Number of agents not reregistered during master failover</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals/reason_registered</code>
  </td>
  <td>Number of agents removed when new agents registered at the same address</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals/reason_unhealthy</code>
  </td>
  <td>Number of agents failed due to failed health checks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/slave_removals/reason_unregistered</code>
  </td>
  <td>Number of agents unregistered</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_framework_to_executor_messages</code>
  </td>
  <td>Number of valid framework to executor messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_operation_status_update_acknowledgements</code>
  </td>
  <td>Number of valid operation status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_status_update_acknowledgements</code>
  </td>
  <td>Number of valid status update acknowledgement messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_status_updates</code>
  </td>
  <td>Number of valid status update messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/task_lost/source_master/reason_invalid_offers</code>
  </td>
  <td>Number of tasks lost due to invalid offers</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/task_lost/source_master/reason_slave_removed</code>
  </td>
  <td>Number of tasks lost due to agent removal</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/task_lost/source_slave/reason_executor_terminated</code>
  </td>
  <td>Number of tasks lost due to executor termination</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>master/valid_executor_to_framework_messages</code>
  </td>
  <td>Number of valid executor to framework messages</td>
  <td>Counter</td>
</tr>
</table>
<h4 id="event-queue"><a class="header" href="#event-queue">Event queue</a></h4>
<p>The following metrics provide information about different types of events in the
event queue.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>master/event_queue_dispatches</code>
  </td>
  <td>Number of dispatches in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/event_queue_http_requests</code>
  </td>
  <td>Number of HTTP requests in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/event_queue_messages</code>
  </td>
  <td>Number of messages in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>master/operator_event_stream_subscribers</code>
  </td>
  <td>Number of subscribers to the operator event stream</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="registrar"><a class="header" href="#registrar">Registrar</a></h4>
<p>The following metrics provide information about read and write latency to the
agent registrar.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>registrar/state_fetch_ms</code>
  </td>
  <td>Registry read latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms</code>
  </td>
  <td>Registry write latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/max</code>
  </td>
  <td>Maximum registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/min</code>
  </td>
  <td>Minimum registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p50</code>
  </td>
  <td>Median registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p90</code>
  </td>
  <td>90th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p95</code>
  </td>
  <td>95th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p99</code>
  </td>
  <td>99th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p999</code>
  </td>
  <td>99.9th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/state_store_ms/p9999</code>
  </td>
  <td>99.99th percentile registry write latency in ms</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="replicated-log"><a class="header" href="#replicated-log">Replicated log</a></h4>
<p>The following metrics provide information about the replicated log underneath
the registrar, which is the persistent store for masters.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>registrar/log/recovered</code>
  </td>
  <td>
    Whether the replicated log for the registrar has caught up with the other
    masters in the cluster. A cluster is operational as long as a quorum of
    "recovered" masters is available in the cluster.
  </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>registrar/log/ensemble_size</code>
  </td>
  <td>
    The number of masters in the ensemble (cluster) that the current master
    communicates with (including itself) to form the replicated log quorum.
    It's imperative that this number is always less than `--quorum * 2` to
    prevent split-brain. It's also important that it should be greater than
    or equal to `--quorum` to maintain availability.
  </td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="allocator"><a class="header" href="#allocator">Allocator</a></h4>
<p>The following metrics provide information about performance
and resource allocations in the allocator.</p>
<table class="table table-stripped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms</code>
  </td>
  <td>Time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/count</code>
  </td>
  <td>Number of allocation algorithm time measurements in the window</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/max</code>
  </td>
  <td>Maximum time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/min</code>
  </td>
  <td>Minimum time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p50</code>
  </td>
  <td>Median time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p90</code>
  </td>
  <td>90th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p95</code>
  </td>
  <td>95th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p99</code>
  </td>
  <td>99th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p999</code>
  </td>
  <td>99.9th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_ms/p9999</code>
  </td>
  <td>99.99th percentile of time spent in allocation algorithm in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_runs</code>
  </td>
  <td>Number of times the allocation algorithm has run</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms</code>
  </td>
  <td>Allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/count</code>
  </td>
  <td>Number of allocation batch latency measurements in the window</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/max</code>
  </td>
  <td>Maximum allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/min</code>
  </td>
  <td>Minimum allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p50</code>
  </td>
  <td>Median allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p90</code>
  </td>
  <td>90th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p95</code>
  </td>
  <td>95th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p99</code>
  </td>
  <td>99th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p999</code>
  </td>
  <td>99.9th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/allocation_run_latency_ms/p9999</code>
  </td>
  <td>99.99th percentile allocation batch latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/roles/<i>&lt;role&gt;</i>/shares/dominant</code>
  </td>
  <td>Dominant <i>resource</i> share for the <i>role</i>, exposed as a percentage (0.0-1.0)</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/event_queue_dispatches</code>
  </td>
  <td>Number of dispatch events in the event queue</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/offer_filters/roles/<i>&lt;role&gt;</i>/active</code>
  </td>
  <td>Number of active offer filters for all frameworks within the <i>role</i></td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/quota/roles/<i>&lt;role&gt;</i>/resources/<i>&lt;resource&gt;</i>/offered_or_allocated</code>
  </td>
  <td>Amount of <i>resource</i>s considered offered or allocated towards
      a <i>role</i>'s quota guarantee</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/quota/roles/<i>&lt;role&gt;</i>/resources/<i>&lt;resource&gt;</i>/guarantee</code>
  </td>
  <td>Amount of <i>resource</i>s guaranteed for a <i>role</i> via quota</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/cpus/offered_or_allocated</code>
  </td>
  <td>Number of CPUs offered or allocated</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/cpus/total</code>
  </td>
  <td>Number of CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/disk/offered_or_allocated</code>
  </td>
  <td>Allocated or offered disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/disk/total</code>
  </td>
  <td>Total disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/mem/offered_or_allocated</code>
  </td>
  <td>Allocated or offered memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>allocator/mesos/resources/mem/total</code>
  </td>
  <td>Total memory in MB</td>
  <td>Gauge</td>
</tr>
</table>
<h3 id="basic-alerts"><a class="header" href="#basic-alerts">Basic Alerts</a></h3>
<p>This section lists some examples of basic alerts that you can use to detect
abnormal situations in a cluster.</p>
<h4 id="masteruptime_secs-is-low"><a class="header" href="#masteruptime_secs-is-low">master/uptime_secs is low</a></h4>
<p>The master has restarted.</p>
<h4 id="masteruptime_secs--60-for-sustained-periods-of-time"><a class="header" href="#masteruptime_secs--60-for-sustained-periods-of-time">master/uptime_secs &lt; 60 for sustained periods of time</a></h4>
<p>The cluster has a flapping master node.</p>
<h4 id="mastertasks_lost-is-increasing-rapidly"><a class="header" href="#mastertasks_lost-is-increasing-rapidly">master/tasks_lost is increasing rapidly</a></h4>
<p>Tasks in the cluster are disappearing. Possible causes include hardware
failures, bugs in one of the frameworks, or bugs in Mesos.</p>
<h4 id="masterslaves_active-is-low"><a class="header" href="#masterslaves_active-is-low">master/slaves_active is low</a></h4>
<p>Agents are having trouble connecting to the master.</p>
<h4 id="mastercpus_percent--09-for-sustained-periods-of-time"><a class="header" href="#mastercpus_percent--09-for-sustained-periods-of-time">master/cpus_percent &gt; 0.9 for sustained periods of time</a></h4>
<p>Cluster CPU utilization is close to capacity.</p>
<h4 id="mastermem_percent--09-for-sustained-periods-of-time"><a class="header" href="#mastermem_percent--09-for-sustained-periods-of-time">master/mem_percent &gt; 0.9 for sustained periods of time</a></h4>
<p>Cluster memory utilization is close to capacity.</p>
<h4 id="masterelected-is-0-for-sustained-periods-of-time"><a class="header" href="#masterelected-is-0-for-sustained-periods-of-time">master/elected is 0 for sustained periods of time</a></h4>
<p>No master is currently elected.</p>
<h2 id="agent-nodes"><a class="header" href="#agent-nodes">Agent Nodes</a></h2>
<p>Metrics from each agent node are available via the
<a href="endpoints/metrics/snapshot.html">/metrics/snapshot</a> agent endpoint.  The response
is a JSON object that contains metrics names and values as key-value pairs.</p>
<h3 id="observability-metrics-1"><a class="header" href="#observability-metrics-1">Observability Metrics</a></h3>
<p>This section lists all available metrics from Mesos agent nodes grouped by
category.</p>
<h4 id="resources-1"><a class="header" href="#resources-1">Resources</a></h4>
<p>The following metrics provide information about the total resources available in
the agent and their current usage.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>containerizer/fetcher/cache_size_total_bytes</code>
  </td>
  <td>The configured maximum size of the fetcher cache in bytes. This value is
  constant for the life of the Mesos agent.</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/fetcher/cache_size_used_bytes</code>
  </td>
  <td>The current amount of data stored in the fetcher cache in bytes.</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>gc/path_removals_failed</code>
  </td>
  <td>Number of times the agent garbage collection process has failed to remove a sandbox path.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>gc/path_removals_pending</code>
  </td>
  <td>Number of sandbox paths that are currently pending agent garbage collection.</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>gc/path_removals_succeeded</code>
  </td>
  <td>Number of sandbox paths the agent successfully removed.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_percent</code>
  </td>
  <td>Percentage of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_used</code>
  </td>
  <td>Number of allocated CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_total</code>
  </td>
  <td>Number of CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_revocable_total</code>
  </td>
  <td>Number of revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/cpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable CPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_percent</code>
  </td>
  <td>Percentage of allocated disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_used</code>
  </td>
  <td>Allocated disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_total</code>
  </td>
  <td>Disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_percent</code>
  </td>
  <td>Percentage of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_used</code>
  </td>
  <td>Number of allocated GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_total</code>
  </td>
  <td>Number of GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_revocable_total</code>
  </td>
  <td>Number of revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/gpus_revocable_used</code>
  </td>
  <td>Number of allocated revocable GPUs</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_percent</code>
  </td>
  <td>Percentage of allocated memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable disk space</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_revocable_total</code>
  </td>
  <td>Revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/disk_revocable_used</code>
  </td>
  <td>Allocated revocable disk space in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_used</code>
  </td>
  <td>Allocated memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_total</code>
  </td>
  <td>Memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_revocable_percent</code>
  </td>
  <td>Percentage of allocated revocable memory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_revocable_total</code>
  </td>
  <td>Revocable memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/mem_revocable_used</code>
  </td>
  <td>Allocated revocable memory in MB</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>volume_gid_manager/volume_gids_total</code>
  </td>
  <td>Number of gids configured for volume gid manager</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>volume_gid_manager/volume_gids_free</code>
  </td>
  <td>Number of free gids available for volume gid manager</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="agent"><a class="header" href="#agent">Agent</a></h4>
<p>The following metrics provide information about whether an agent is currently
registered with a master and for how long it has been running.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>slave/registered</code>
  </td>
  <td>Whether this agent is registered with a master</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/uptime_secs</code>
  </td>
  <td>Uptime in seconds</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="system-1"><a class="header" href="#system-1">System</a></h4>
<p>The following metrics provide information about the agent system.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>system/cpus_total</code>
  </td>
  <td>Number of CPUs available</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_15min</code>
  </td>
  <td>Load average for the past 15 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_5min</code>
  </td>
  <td>Load average for the past 5 minutes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/load_1min</code>
  </td>
  <td>Load average for the past minute</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_free_bytes</code>
  </td>
  <td>Free memory in bytes</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>system/mem_total_bytes</code>
  </td>
  <td>Total memory in bytes</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="executors"><a class="header" href="#executors">Executors</a></h4>
<p>The following metrics provide information about the executor instances running
on the agent.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>containerizer/mesos/container_destroy_errors</code>
  </td>
  <td>Number of containers destroyed due to launch errors</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>containerizer/fetcher/task_fetches_succeeded</code>
  </td>
  <td>Total number of times the Mesos fetcher successfully fetched all the URIs for a task.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>containerizer/fetcher/task_fetches_failed</code>
  </td>
  <td>Number of times the Mesos fetcher failed to fetch all the URIs for a task.</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/container_launch_errors</code>
  </td>
  <td>Number of container launch errors</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/executors_preempted</code>
  </td>
  <td>Number of executors destroyed due to preemption</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/frameworks_active</code>
  </td>
  <td>Number of active frameworks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executor_directory_max_allowed_age_secs</code>
  </td>
  <td>Maximum allowed age in seconds to delete executor directory</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executors_registering</code>
  </td>
  <td>Number of executors registering</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executors_running</code>
  </td>
  <td>Number of executors running</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/executors_terminated</code>
  </td>
  <td>Number of terminated executors</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/executors_terminating</code>
  </td>
  <td>Number of terminating executors</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/recovery_errors</code>
  </td>
  <td>Number of errors encountered during agent recovery</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/recovery_time_secs</code>
  </td>
  <td>Agent recovery time in seconds. This value is only available after agent
  recovery succeeded and remains constant for the life of the Mesos agent.</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="tasks-1"><a class="header" href="#tasks-1">Tasks</a></h4>
<p>The following metrics provide information about active and terminated tasks.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>slave/tasks_failed</code>
  </td>
  <td>Number of failed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_finished</code>
  </td>
  <td>Number of finished tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_killed</code>
  </td>
  <td>Number of killed tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_lost</code>
  </td>
  <td>Number of lost tasks</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_running</code>
  </td>
  <td>Number of running tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_staging</code>
  </td>
  <td>Number of staging tasks</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>slave/tasks_starting</code>
  </td>
  <td>Number of starting tasks</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="messages-1"><a class="header" href="#messages-1">Messages</a></h4>
<p>The following metrics provide information about messages between the agents and
the master it is registered with.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>slave/invalid_framework_messages</code>
  </td>
  <td>Number of invalid framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/invalid_status_updates</code>
  </td>
  <td>Number of invalid status updates</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/valid_framework_messages</code>
  </td>
  <td>Number of valid framework messages</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>slave/valid_status_updates</code>
  </td>
  <td>Number of valid status updates</td>
  <td>Counter</td>
</tr>
</table>
<h4 id="containerizers"><a class="header" href="#containerizers">Containerizers</a></h4>
<p>The following metrics provide information about both Mesos and Docker
containerizers.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms</code>
  </td>
  <td>Docker containerizer image pull latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/count</code>
  </td>
  <td>Number of Docker containerizer image pulls</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/max</code>
  </td>
  <td>Maximum Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/min</code>
  </td>
  <td>Minimum Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p50</code>
  </td>
  <td>Median Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p90</code>
  </td>
  <td>90th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p95</code>
  </td>
  <td>95th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p99</code>
  </td>
  <td>99th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p999</code>
  </td>
  <td>99.9th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/docker/image_pull_ms/p9999</code>
  </td>
  <td>99.99th percentile Docker containerizer image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/disk/project_ids_free</code>
  </td>
  <td>Number of free project IDs available to the XFS Disk isolator</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/disk/project_ids_total</code>
  </td>
  <td>Number of project IDs configured for the XFS Disk isolator</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms</code>
  </td>
  <td>Mesos containerizer docker image pull latency in ms </td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/count</code>
  </td>
  <td>Number of Mesos containerizer docker image pulls</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/max</code>
  </td>
  <td>Maximum Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/min</code>
  </td>
  <td>Minimum Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p50</code>
  </td>
  <td>Median Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p90</code>
  </td>
  <td>90th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p95</code>
  </td>
  <td>95th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p99</code>
  </td>
  <td>99th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p999</code>
  </td>
  <td>99.9th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>containerizer/mesos/provisioner/docker_store/image_pull_ms/p9999</code>
  </td>
  <td>99.99th percentile Mesos containerizer docker image pull latency in ms</td>
  <td>Gauge</td>
</tr>
</table>
<h4 id="resource-providers"><a class="header" href="#resource-providers">Resource Providers</a></h4>
<p>The following metrics provide information about ongoing and completed
<a href="operations.html">operations</a> that apply to resources provided by a
<a href="resource-provider.html">resource provider</a> with the given <em>type</em> and <em>name</em>. In
the following metrics, the <em>operation</em> placeholder refers to the name of a
particular operation type, which is described in the list of
<a href="monitoring.html#supported-operation-types">supported operation types</a>.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/pending</code>
  </td>
  <td>Number of ongoing <i>operation</i>s</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/finished</code>
  </td>
  <td>Number of finished <i>operation</i>s</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/failed</code>
  </td>
  <td>Number of failed <i>operation</i>s</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/operations/<i>&lt;operation&gt;</i>/dropped</code>
  </td>
  <td>Number of dropped <i>operation</i>s</td>
  <td>Counter</td>
</tr>
</table>
<h5 id="supported-operation-types"><a class="header" href="#supported-operation-types">Supported Operation Types</a></h5>
<p>Since the supported operation types may vary among different resource providers,
the following is a comprehensive list of operation types and the corresponding
resource providers that support them. Note that the name column is for the
<em>operation</em> placeholder in the above metrics.</p>
<table class="table table-striped">
<thead>
<tr><th>Type</th><th>Name</th><th>Supported Resource Provider Types</th>
</thead>
<tr>
  <td><code><a href="reservation.html">RESERVE</a></code></td>
  <td><code>reserve</code></td>
  <td>All</td>
</tr>
<tr>
  <td><code><a href="reservation.html">UNRESERVE</a></code></td>
  <td><code>unreserve</code></td>
  <td>All</td>
</tr>
<tr>
  <td><code><a href="persistent-volume.html#-offer-operation-create-">CREATE</a></code></td>
  <td><code>create</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
<tr>
  <td><code><a href="persistent-volume.html#-offer-operation-destroy-">DESTROY</a></code></td>
  <td><code>destroy</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
<tr>
  <td><code><a href="csi.html#-create_disk-operation">CREATE_DISK</a></code></td>
  <td><code>create_disk</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
<tr>
  <td><code><a href="csi.html#-destroy_disk-operation">DESTROY_DISK</a></code></td>
  <td><code>destroy_disk</code></td>
  <td><code>org.apache.mesos.rp.local.storage</code></td>
</tr>
</table>
<p>For example, cluster operators can monitor the number of successful
<code>CREATE_VOLUME</code> operations that are applied to the resource provider with type
<code>org.apache.mesos.rp.local.storage</code> and name <code>lvm</code> through the
<code>resource_providers/org.apache.mesos.rp.local.storage.lvm/operations/create_disk/finished</code>
metric.</p>
<h4 id="csi-plugins"><a class="header" href="#csi-plugins">CSI Plugins</a></h4>
<p>Storage resource providers in Mesos are backed by
<a href="csi.html#standalone-containers-for-csi-plugins">CSI plugins</a> running in
<a href="standalone-container.html">standalone containers</a>. To monitor the health of these
CSI plugins for a storage resource provider with <em>type</em> and <em>name</em>, the
following metrics provide information about plugin terminations and ongoing and
completed CSI calls made to the plugin.</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/container_terminations</code>
  </td>
  <td>Number of terminated CSI plugin containers</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_pending</code>
  </td>
  <td>Number of ongoing CSI calls</td>
  <td>Gauge</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_finished</code>
  </td>
  <td>Number of successful CSI calls</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_failed</code>
  </td>
  <td>Number of failed CSI calls</td>
  <td>Counter</td>
</tr>
<tr>
  <td>
  <code>resource_providers/<i>&lt;type&gt;</i>.<i>&lt;name&gt;</i>/csi_plugin/rpcs_cancelled</code>
  </td>
  <td>Number of cancelled CSI calls</td>
  <td>Counter</td>
</tr>
</table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-new-cli"><a class="header" href="#the-new-cli">The new CLI</a></h1>
<p>The new Mesos Command Line Interface provides one executable Python 3
script to run all default commands and additional custom plugins.</p>
<p>Two of the subcommands available allow you to debug running containers:</p>
<ul>
<li><code>mesos task exec</code>, to run a command in a running task's container.</li>
<li><code>mesos task attach</code>, to attach your local terminal to a running task
and stream its input/output.</li>
</ul>
<h2 id="building-the-cli"><a class="header" href="#building-the-cli">Building the CLI</a></h2>
<p>For now, the Mesos CLI is still under development and not built as part
of a standard Mesos distribution.</p>
<p>However, the CLI can be built using <a href="configuration/autotools.html">Autotools</a> and
<a href="configuration/cmake.html">Cmake options</a>. If necessary, check the options
described in the linked pages to set Python 3 before starting a build.</p>
<p>The result of this build will be a <code>mesos</code> binary that can be executed.</p>
<h2 id="using-the-cli"><a class="header" href="#using-the-cli">Using the CLI</a></h2>
<p>Using the CLI without building Mesos is also possible. To do so, activate
the CLI virtual environment by following the steps described below:</p>
<pre><code>$ cd src/python/cli_new/
$ PYTHON=python3 ./bootstrap
$ source activate
$ mesos
</code></pre>
<p>Calling <code>mesos</code> will then run the CLI and calling <code>mesos-cli-tests</code> will
run the integration tests.</p>
<h2 id="configuring-the-cli"><a class="header" href="#configuring-the-cli">Configuring the CLI</a></h2>
<p>The CLI uses a configuration file to know where the masters of the cluster are
as well as list any plugins that should be used in addition to the default ones
provided.</p>
<p>The configuation file, located by default at <code>~/.mesos/config.toml</code>, looks
like this:</p>
<pre><code># The `plugins` array lists the absolute paths of the
# plugins you want to add to the CLI.
plugins = [
  &quot;&lt;/absolute/path/to/plugin-1/directory&gt;&quot;,
  &quot;&lt;/absolute/path/to/plugin-2/directory&gt;&quot;
]

# The `master` field is either composed of an `address` field
# or a `zookeeper` field, but not both. For example:
[master]
  address = &quot;10.10.0.30:5050&quot;
  # The `zookeeper` field has an `addresses` array and a `path` field.
  # [master.zookeeper]
  #   addresses = [
  #     &quot;10.10.0.31:5050&quot;,
  #     &quot;10.10.0.32:5050&quot;,
  #     &quot;10.10.0.33:5050&quot;
  #   ]
  #   path = &quot;/mesos&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operational-guide"><a class="header" href="#operational-guide">Operational Guide</a></h1>
<h2 id="using-a-process-supervisor"><a class="header" href="#using-a-process-supervisor">Using a process supervisor</a></h2>
<p>Mesos uses a &quot;<a href="https://en.wikipedia.org/wiki/Fail-fast">fail-fast</a>&quot; approach to error handling: if a serious error occurs, Mesos will typically exit rather than trying to continue running in a possibly erroneous state. For example, when Mesos is configured for <a href="high-availability.html">high availability</a>, the leading master will abort itself when it discovers it has been partitioned away from the Zookeeper quorum. This is a safety precaution to ensure the previous leader doesn't continue communicating in an unsafe state.</p>
<p>To ensure that such failures are handled appropriately, production deployments of Mesos typically use a <em>process supervisor</em> (such as systemd or supervisord) to detect when Mesos processes exit. The supervisor can be configured to restart the failed process automatically and/or to notify the cluster operator to investigate the situation.</p>
<h2 id="changing-the-master-quorum"><a class="header" href="#changing-the-master-quorum">Changing the master quorum</a></h2>
<p>The master leverages a <a href="replicated-log-internals.html">Paxos-based replicated log</a> as its storage backend (<code>--registry=replicated_log</code> is the only storage backend currently supported). Each master participates in the ensemble as a log replica. The <code>--quorum</code> flag determines a majority of the masters.</p>
<p>The following table shows the tolerance to master failures for each quorum size:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">Masters</th><th style="text-align: right">Quorum Size</th><th style="text-align: right">Failure Tolerance</th></tr></thead><tbody>
<tr><td style="text-align: right">1</td><td style="text-align: right">1</td><td style="text-align: right">0</td></tr>
<tr><td style="text-align: right">3</td><td style="text-align: right">2</td><td style="text-align: right">1</td></tr>
<tr><td style="text-align: right">5</td><td style="text-align: right">3</td><td style="text-align: right">2</td></tr>
<tr><td style="text-align: right">...</td><td style="text-align: right">...</td><td style="text-align: right">...</td></tr>
<tr><td style="text-align: right">2N - 1</td><td style="text-align: right">N</td><td style="text-align: right">N - 1</td></tr>
</tbody></table>
</div>
<p>It is recommended to run with 3 or 5 masters, when desiring high availability.</p>
<h3 id="note"><a class="header" href="#note">NOTE</a></h3>
<p>When configuring the quorum, it is essential to ensure that there are only so many masters running as specified in the table above. If additional masters are running, this violates the quorum and the log may be corrupted! As a result, it is recommended to gate the running of the master process with something that enforces a static whitelist of the master hosts. See <a href="https://issues.apache.org/jira/browse/MESOS-1546">MESOS-1546</a> for adding a safety whitelist within Mesos itself.</p>
<p>For online reconfiguration of the log, see: <a href="https://issues.apache.org/jira/browse/MESOS-683">MESOS-683</a>.</p>
<h3 id="increasing-the-quorum-size"><a class="header" href="#increasing-the-quorum-size">Increasing the quorum size</a></h3>
<p>As the size of a cluster grows, it may be desired to increase the quorum size for additional fault tolerance.</p>
<p>The following steps indicate how to increment the quorum size, using 3 -&gt; 5 masters as an example (quorum size 2 -&gt; 3):</p>
<ol>
<li>Initially, 3 masters are running with <code>--quorum=2</code></li>
<li>Restart the original 3 masters with <code>--quorum=3</code></li>
<li>Start 2 additional masters with <code>--quorum=3</code></li>
</ol>
<p>To increase the quorum by N, repeat this process to increment the quorum size N times.</p>
<p>NOTE: Currently, moving out of a single master setup requires wiping the replicated log
state and starting fresh. This will wipe all persistent data (e.g., agents, maintenance
information, quota information, etc). To move from 1 master to 3 masters:</p>
<ol>
<li>Stop the standalone master.</li>
<li>Remove the replicated log data (<code>replicated_log</code> under the <code>--work_dir</code>).</li>
<li>Start the original master and two new masters with <code>--quorum=2</code></li>
</ol>
<h3 id="decreasing-the-quorum-size"><a class="header" href="#decreasing-the-quorum-size">Decreasing the quorum size</a></h3>
<p>The following steps indicate how to decrement the quorum size, using 5 -&gt; 3 masters as an example (quorum size 3 -&gt; 2):</p>
<ol>
<li>Initially, 5 masters are running with <code>--quorum=3</code></li>
<li>Remove 2 masters from the cluster, ensure they will not be restarted (see NOTE section above). Now 3 masters are running with <code>--quorum=3</code></li>
<li>Restart the 3 masters with <code>--quorum=2</code></li>
</ol>
<p>To decrease the quorum by N, repeat this process to decrement the quorum size N times.</p>
<h3 id="replacing-a-master"><a class="header" href="#replacing-a-master">Replacing a master</a></h3>
<p>Please see the NOTE section above. So long as the failed master is guaranteed to not re-join the ensemble, it is safe to start a new master <em>with an empty log</em> and allow it to catch up.</p>
<h2 id="external-access-for-mesos-master"><a class="header" href="#external-access-for-mesos-master">External access for Mesos master</a></h2>
<p>If the default IP (or the command line arg <code>--ip</code>) is an internal IP, then external entities such as framework schedulers will be unable to reach the master. To address that scenario, an externally accessible IP:port can be setup via the <code>--advertise_ip</code> and <code>--advertise_port</code> command line arguments of <code>mesos-master</code>. If configured, external entities such as framework schedulers interact with the advertise_ip:advertise_port from where the request needs to be proxied to the internal IP:port on which the Mesos master is listening.</p>
<h2 id="http-requests-to-non-leading-master"><a class="header" href="#http-requests-to-non-leading-master">HTTP requests to non-leading master</a></h2>
<p>HTTP requests to some master endpoints (e.g., <a href="endpoints/master/state.html">/state</a>, <a href="endpoints/master/machine/down.html">/machine/down</a>) can only be answered by the leading master. Such requests made to a non-leading master will result in either a <code>307 Temporary Redirect</code> (with the location of the leading master) or <code>503 Service Unavailable</code> (if the master does not know who the current leader is).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-fetcher"><a class="header" href="#mesos-fetcher">Mesos Fetcher</a></h1>
<p>Mesos 0.23.0 introduced experimental support for the Mesos <em>fetcher cache</em>.</p>
<p>In this context we loosely regard the term &quot;downloading&quot; as to include copying
from local file systems.</p>
<h2 id="what-is-the-mesos-fetcher"><a class="header" href="#what-is-the-mesos-fetcher">What is the Mesos fetcher?</a></h2>
<p>The Mesos fetcher is a mechanism to download resources into the <a href="sandbox.html">sandbox
directory</a> of a task in preparation of running
the task. As part of a TaskInfo message, the framework ordering the task's
execution provides a list of <code>CommandInfo::URI</code> protobuf values, which becomes
the input to the Mesos fetcher.</p>
<p>The Mesos fetcher can copy files from a local filesytem and it also natively
supports the HTTP, HTTPS, FTP and FTPS protocols. If the requested URI is based
on some other protocol, then the fetcher tries to utilise a local Hadoop client
and hence supports any protocol supported by the Hadoop client, e.g., HDFS, S3.
See the agent <a href="configuration/agent.html">configuration documentation</a>
for how to configure the agent with a path to the Hadoop client.</p>
<p>By default, each requested URI is downloaded directly into the sandbox directory
and repeated requests for the same URI leads to downloading another copy of the
same resource. Alternatively, the fetcher can be instructed to cache URI
downloads in a dedicated directory for reuse by subsequent downloads.</p>
<p>The Mesos fetcher mechanism comprises of these two parts:</p>
<ol>
<li>
<p>The agent-internal Fetcher Process (in terms of libprocess) that controls and
coordinates all fetch actions. Every agent instance has exactly one internal
fetcher instance that is used by every kind of containerizer.</p>
</li>
<li>
<p>The external program <code>mesos-fetcher</code> that is invoked by the former. It
performs all network and disk operations except file deletions and file size
queries for cache-internal bookkeeping. It is run as an external OS process in
order to shield the agent process from I/O-related hazards. It takes
instructions in form of an environment variable containing a JSON object with
detailed fetch action descriptions.</p>
</li>
</ol>
<h2 id="the-fetch-procedure"><a class="header" href="#the-fetch-procedure">The fetch procedure</a></h2>
<p>Frameworks launch tasks by calling the scheduler driver method <code>launchTasks()</code>,
passing <code>CommandInfo</code> protobuf structures as arguments. This type of structure
specifies (among other things) a command and a list of URIs that need to be
&quot;fetched&quot; into the sandbox directory on the agent node as a precondition for
task execution. Hence, when the agent receives a request to launch a task, it
calls upon its fetcher, first, to provision the specified resources into the
sandbox directory. If fetching fails, the task is not started and the reported
task status is <code>TASK_FAILED</code>.</p>
<p>All URIs requested for a given task are fetched sequentially in a single
invocation of mesos-fetcher. Here, avoiding download concurrency reduces the
risk of bandwidth issues somewhat. However, multiple fetch operations can be
active concurrently due to multiple task launch requests.</p>
<h3 id="the-uri-protobuf-structure"><a class="header" href="#the-uri-protobuf-structure">The URI protobuf structure</a></h3>
<p>Before mesos-fetcher is started, the specific fetch actions to be performed for
each URI are determined based on the following protobuf structure. (See
<code>include/mesos/mesos.proto</code> for more details.)</p>
<pre><code>message CommandInfo {
  message URI {
    required string value = 1;
    optional bool executable = 2;
    optional bool extract = 3 [default = true];
    optional bool cache = 4;
    optional string output_file = 5;
  }
  ...
  optional string user = 5;
}
</code></pre>
<p>The field &quot;value&quot; contains the URI.</p>
<p>If the &quot;executable&quot; field is &quot;true&quot;, the &quot;extract&quot; field is ignored and
has no effect.</p>
<p>If the &quot;cache&quot; field is true, the fetcher cache is to be used for the URI.</p>
<p>If the &quot;output_file&quot; field is set, the fetcher will use that name for the copy
stored in the sandbox directory. &quot;output_file&quot; may contain a directory
component, in which case the path described must be a relative path.</p>
<h3 id="specifying-a-user-name"><a class="header" href="#specifying-a-user-name">Specifying a user name</a></h3>
<p>The framework may pass along a user name that becomes a fetch parameter. This
causes its executors and tasks to run under a specific user. However, if the
&quot;user&quot; field in the CommandInfo structure is specified, it takes precedence for
the affected task.</p>
<p>If a user name is specified either way, the fetcher first validates that it is
in fact a valid user name on the agent. If it is not, fetching fails right here.
Otherwise, the sandbox directory is assigned to the specified user as owner
(using <code>chown</code>) at the end of the fetch procedure, before task execution begins.</p>
<p>The user name in play has an important effect on caching.  Caching is managed on
a per-user base, i.e. the combination of user name and &quot;uri&quot; uniquely
identifies a cacheable fetch result. If no user name has been specified, this
counts for the cache as a separate user, too. Thus cache files for each valid
user are segregated from all others, including those without a specified user.</p>
<p>This means that the exact same URI will be downloaded and cached multiple times
if different users are indicated.</p>
<h3 id="executable-fetch-results"><a class="header" href="#executable-fetch-results">Executable fetch results</a></h3>
<p>By default, fetched files are not executable.</p>
<p>If the field &quot;executable&quot; is set to &quot;true&quot;, the fetch result will be changed to
be executable (by &quot;chmod&quot;) for every user. This happens at the end of the fetch
procedure, in the sandbox directory only. It does not affect any cache file.</p>
<h3 id="archive-extraction"><a class="header" href="#archive-extraction">Archive extraction</a></h3>
<p>If the &quot;extract&quot; field is &quot;true&quot;, which is the default, then files with
a recognized extension that hints at packed or compressed archives are unpacked
in the sandbox directory. These file extensions are recognized:</p>
<ul>
<li>.tar, .tar.gz, .tar.bz2, .tar.xz</li>
<li>.gz, .tgz, .tbz2, .txz, .zip</li>
</ul>
<p>In case the cache is bypassed, both the archive and the unpacked results will be
found together in the sandbox. In case a cache file is unpacked, only the
extraction result will be found in the sandbox.</p>
<p>The &quot;output_file&quot; field is useful here for cases where the URI ends with query
parameters, since these will otherwise end up in the file copied to the sandbox
and will subsequently fail to be recognized as archives.</p>
<h3 id="bypassing-the-cache"><a class="header" href="#bypassing-the-cache">Bypassing the cache</a></h3>
<p>By default, the URI field &quot;cache&quot; is not present. If this is the case or its
value is &quot;false&quot; the fetcher downloads directly into the sandbox directory.</p>
<p>The same also happens dynamically as a fallback strategy if anything goes wrong
when preparing a fetch operation that involves the cache. In this case, a
warning message is logged. Possible fallback conditions are:</p>
<ul>
<li>The server offering the URI does not respond or reports an error.</li>
<li>The URI's download size could not be determined.</li>
<li>There is not enough space in the cache, even after attempting to evict files.</li>
</ul>
<h3 id="fetching-through-the-cache"><a class="header" href="#fetching-through-the-cache">Fetching through the cache</a></h3>
<p>If the URI's &quot;cache&quot; field has the value &quot;true&quot;, then the fetcher cache is in
effect. If a URI is encountered for the first time (for the same user), it is
first downloaded into the cache, then copied to the sandbox directory from
there. If the same URI is encountered again, and a corresponding cache file is
resident in the cache or still en route into the cache, then downloading is
omitted and the fetcher proceeds directly to copying from the cache. Competing
requests for the same URI simply wait upon completion of the first request that
occurs. Thus every URI is downloaded at most once (per user) as long as it is
cached.</p>
<p>Every cache file stays resident for an unspecified amount of time and can be
removed at the fetcher's discretion at any moment, except while it is in direct
use:</p>
<ul>
<li>It is still being downloaded by this fetch procedure.</li>
<li>It is still being downloaded by a concurrent fetch procedure for a different
task.</li>
<li>It is being copied or extracted from the cache.</li>
</ul>
<p>Once a cache file has been removed, the related URI will thereafter be treated
as described above for the first encounter.</p>
<p>Unfortunately, there is no mechanism to refresh a cache entry in the current
experimental version of the fetcher cache. A future feature may force updates
based on checksum queries to the URI.</p>
<p>Recommended practice for now:</p>
<p>The framework should start using a fresh unique URI whenever the resource's
content has changed.</p>
<h3 id="determining-resource-sizes"><a class="header" href="#determining-resource-sizes">Determining resource sizes</a></h3>
<p>Before downloading a resource to the cache, the fetcher first determines the
size of the expected resource. It uses these methods depending on the nature of
the URI.</p>
<ul>
<li>Local file sizes are probed with systems calls (that follow symbolic links).</li>
<li>HTTP/HTTPS URIs are queried for the &quot;content-length&quot; field in the header. This
is performed by <code>curl</code>. The reported asset size must be greater than zero or
the URI is deemed invalid.</li>
<li>FTP/FTPS is not supported at the time of writing.</li>
<li>Everything else is queried by the local HDFS client.</li>
</ul>
<p>If any of this reports an error, the fetcher then falls back on bypassing the
cache as described above.</p>
<p>WARNING: Only URIs for which download sizes can be queried up front and for
which accurate sizes are reported reliably are eligible for any fetcher cache
involvement. If actual cache file sizes exceed the physical capacity of the
cache directory in any way, all further agent behavior is completely
unspecified. Do not use any cache feature with any URI for which you have any
doubts!</p>
<p>To mitigate this problem, cache files that have been found to be larger than
expected are deleted immediately after downloading and delivering the
requested content to the sandbox. Thus exceeding total capacity at least
does not accumulate over subsequent fetcher runs.</p>
<p>If you know for sure that size aberrations are within certain limits you can
specify a cache directory size that is sufficiently smaller than your actual
physical volume and fetching should work.</p>
<p>In case of cache files that are smaller then expected, the cache will
dynamically adjust its own bookkeeping according to actual sizes.</p>
<h3 id="cache-eviction"><a class="header" href="#cache-eviction">Cache eviction</a></h3>
<p>After determining the prospective size of a cache file and before downloading
it, the cache attempts to ensure that at least as much space as is needed for
this file is available and can be written into. If this is immediately the case,
the requested amount of space is simply marked as reserved. Otherwise, missing
space is freed up by &quot;cache eviction&quot;. This means that the cache removes files
at its own discretion until the given space target is met or exceeded.</p>
<p>The eviction process fails if too many files are in use and therefore not
evictable or if the cache is simply too small. Either way, the fetcher then
falls back on bypassing the cache for the given URI as described above.</p>
<p>If multiple evictions happen concurrently, each of them is pursuing its own
separate space goals. However, leftover freed up space from one effort is
automatically awarded to others.</p>
<h2 id="http-and-socks-proxy-settings"><a class="header" href="#http-and-socks-proxy-settings">HTTP and SOCKS proxy settings</a></h2>
<p>Sometimes it is desirable to use a proxy to download the file. The Mesos
fetcher uses libcurl internally for downloading content from
HTTP/HTTPS/FTP/FTPS servers, and libcurl can use a proxy automatically if
certain environment variables are set.</p>
<p>The respective environment variable name is <code>[protocol]_proxy</code>, where
<code>protocol</code> can be one of socks4, socks5, http, https.</p>
<p>For example, the value of the <code>http_proxy</code> environment variable would be used
as the proxy for fetching http contents, while <code>https_proxy</code> would be used for
fetching https contents. Pay attention that these variable names must be
entirely in lower case.</p>
<p>The value of the proxy variable is of the format
<code>[protocol://][user:password@]machine[:port]</code>, where <code>protocol</code> can be one of
socks4, socks5, http, https.</p>
<p>FTP/FTPS requests with a proxy also make use of an HTTP/HTTPS proxy. Even
though in general this constrains the available FTP protocol operations,
everything the fetcher uses is supported.</p>
<p>Your proxy settings can be placed in <code>/etc/default/mesos-slave</code>. Here is an
example:</p>
<pre><code>export http_proxy=https://proxy.example.com:3128
export https_proxy=https://proxy.example.com:3128
</code></pre>
<p>The fetcher will pick up these environment variable settings since the utility
program <code>mesos-fetcher</code> which it employs is a child of mesos-agent.</p>
<p>For more details, please check the
<a href="http://curl.haxx.se/libcurl/c/libcurl-tutorial.html">libcurl manual</a>.</p>
<h2 id="agent-flags"><a class="header" href="#agent-flags">Agent flags</a></h2>
<p>It is highly recommended to set these flags explicitly to values other than
their defaults or to not use the fetcher cache in production.</p>
<ul>
<li>&quot;fetcher_cache_size&quot;, default value: enough for testing.</li>
<li>&quot;fetcher_cache_dir&quot;, default value: somewhere inside the directory specified
by the &quot;work_dir&quot; flag, which is OK for testing.</li>
</ul>
<p>Recommended practice:</p>
<ul>
<li>Use a separate volume as fetcher cache. Do not specify a directory as fetcher
cache directory that competes with any other contributor for the underlying
volume's space.</li>
<li>Set the cache directory size flag of the agent to less than your actual cache
volume's physical size. Use a safety margin, especially if you do not know
for sure if all frameworks are going to be compliant.</li>
</ul>
<p>Ultimate remedy:</p>
<p>You can disable the fetcher cache entirely on each agent by setting its
&quot;fetcher_cache_size&quot; flag to zero bytes.</p>
<h2 id="future-features"><a class="header" href="#future-features">Future Features</a></h2>
<p>The following features would be relatively easy to implement additionally.</p>
<ul>
<li>Perform cache updates based on resource check sums. For example, query the md5
field in HTTP headers to determine when a resource at a URL has changed.</li>
<li>Respect HTTP cache-control directives.</li>
<li>Enable caching for ftp/ftps.</li>
<li>Use symbolic links or bind mounts to project cached resources into the
sandbox, read-only.</li>
<li>Have a choice whether to copy the extracted archive into the sandbox.</li>
<li>Have a choice whether to delete the archive after extraction bypassing the
cache.</li>
<li>Make the segregation of cache files by user optional.</li>
<li>Extract content while downloading when bypassing the cache.</li>
<li>Prefetch resources for subsequent tasks. This can happen concurrently with
running the present task, right after fetching its own resources.</li>
</ul>
<h2 id="implementation-details-2"><a class="header" href="#implementation-details-2">Implementation Details</a></h2>
<p>The <a href="fetcher-cache-internals.html">Mesos Fetcher Cache Internals</a> describes how the fetcher cache is implemented.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Domains and Regions
layout: documentation</h2>
<h1 id="regions-and-fault-domains"><a class="header" href="#regions-and-fault-domains">Regions and Fault Domains</a></h1>
<p>Starting with Mesos 1.5, it is possible to place Mesos masters and agents into
<em>domains</em>, which are logical groups of machines that share some characteristics.</p>
<p>Currently, fault domains are the only supported type of domains, which are
groups of machines with similar failure characteristics.</p>
<p>A fault domain is a 2 level hierarchy of regions and zones. The mapping from
fault domains to physical infrastructure is up to the operator to configure,
although it is recommended that machines in the same zones have low latency to
each other.</p>
<p>In cloud environments, regions and zones can be mapped to the &quot;region&quot; and
&quot;availability zone&quot; concepts exposed by most cloud providers, respectively.
In on-premise deployments, regions and zones can be mapped to data centers and
racks, respectively.</p>
<p>Schedulers may prefer to place network-intensive workloads in the same domain,
as this may improve performance. Conversely, a single failure that affects a
host in a domain may be more likely to affect other hosts in the same domain;
hence, schedulers may prefer to place workloads that require high availability
in multiple domains. For example, all the hosts in a single rack might lose
power or network connectivity simultaneously.</p>
<p>The <code>--domain</code> flag can be used to specify the fault domain of a master or
agent node. The value of this flag must be a file path or a JSON dictionary
with the key <code>fault_domain</code> and subkeys <code>region</code> and <code>zone</code> mapping to
arbitrary strings:</p>
<pre><code>mesos-master --domain='{&quot;fault_domain&quot;: {&quot;region&quot;: {&quot;name&quot;:&quot;eu&quot;}, &quot;zone&quot;: { &quot;name&quot;:&quot;rack1&quot;}}}'

mesos-agent  --domain='{&quot;fault_domain&quot;: {&quot;region&quot;: {&quot;name&quot;:&quot;eu&quot;}, &quot;zone&quot;: {&quot;name&quot;:&quot;rack2&quot;}}}'
</code></pre>
<p>Frameworks can learn about the domain of an agent by inspecting the <code>domain</code>
field in the received offer, which contains a <code>DomainInfo</code> that has the
same structure as the JSON dictionary above.</p>
<h1 id="constraints"><a class="header" href="#constraints">Constraints</a></h1>
<p>When configuring fault domains for the masters and agents, the following
constraints must be obeyed:</p>
<ul>
<li>
<p>If a mesos master is not configured with a domain, it will reject connection
attempts from agents with a domain.</p>
<p>This is done because the master is not able to determine whether or not the
agent would be remote in this case.</p>
</li>
<li>
<p>Agents with no configured domain are assumed to be in the same domain as the
master.</p>
<p>If this behaviour isn't desired, the <code>--require_agent_domain</code> flag on the
master can be used to enforce that domains are configured on all agents by
having the master reject all registration attempts by agents without a
configured domain.</p>
</li>
<li>
<p>If one master is configured with a domain, all other masters must be in the
same &quot;region&quot; to avoid cross-region quorum writes. It is recommended to put
them in different zones within that region for high availability.</p>
</li>
<li>
<p>The default DRF resource allocator will only offer resources from agents in
the same region as the master. To receive offers from all regions, a
framework must set the <code>REGION_AWARE</code> capability bit in its FrameworkInfo.</p>
</li>
</ul>
<h1 id="example"><a class="header" href="#example">Example</a></h1>
<p>A short example will serve to illustrate these concepts. WayForward Technologies
runs a successful website that allows users to purchase things that they want
to have.</p>
<p>To do this, it owns a data center in San Francisco, in which it runs a number of
custom Mesos frameworks. All agents within the data center are configured with
the same region <code>sf</code>, and the individual racks inside the data center are used
as zones.</p>
<p>The three mesos masters are placed in different server racks in the data center,
which gives them enough isolation to withstand events like a whole rack losing
power or network connectivity but still have low-enough latency for
quorum writes.</p>
<p>One of the provided services is a real-time view of the company's inventory.
The framework providing this service is placing all of its tasks in the same
zone as the database server, to take advantage of the high-speed, low-latency
link so it can always display the latest results.</p>
<p>During peak hours, it might happen that the computing power required to operate
the website exceeds the capacity of the data center. To avoid unnecessary
hardware purchases, WayForward Technologies contracted with a third-party cloud
provider TPC. The machines from this provider are placed in a different
region <code>tpc</code>, and the zones are configured to correspond to the availability
zones provided by TPC. All relevant frameworks are updated with the
<code>REGION_AWARE</code> bit in their <code>FrameworkInfo</code> and their scheduling logic is
updated so that they can schedule tasks in the cloud if required.</p>
<p>Non-region aware frameworks will now only receive offers from agents within
the data center, where the master nodes reside. Region-aware frameworks are
supposed to know when and if they should place their tasks in the data center
or with the cloud provider.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance-profiling"><a class="header" href="#performance-profiling">Performance Profiling</a></h1>
<p>This document over time will be home to various guides on how to use various profiling tools to do performance analysis of Mesos.</p>
<h2 id="flamescope"><a class="header" href="#flamescope">Flamescope</a></h2>
<p><a href="https://github.com/Netflix/flamescope">Flamescope</a> is a visualization tool for exploring different time ranges as <a href="https://github.com/brendangregg/FlameGraph">flamegraphs</a>. In order to use the tool, you first need to obtain stack traces, here's how to obtain a 60 second recording of the mesos master process at 100 hertz using Linux perf:</p>
<pre><code>$ sudo perf record --freq=100 --no-inherit --call-graph dwarf -p &lt;mesos-master-pid&gt; -- sleep 60
$ sudo perf script --header | c++filt &gt; mesos-master.stacks
$ gzip mesos-master.stacks
</code></pre>
<p>If you'd like to solicit help in analyzing the performance data, upload the <code>mesos-master.stacks.gz</code> to a publicly accessible location and file with <code>dev@mesos.apache.org</code> for analysis, or send the file over <a href="http://mesos.slack.com">slack</a> to the #performance channel.</p>
<p>Alternatively, to do the analysis yourself, place mesos-master.stacks into the <code>examples</code> folder of a flamescope git checkout.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-profiling-with-mesos-and-jemalloc"><a class="header" href="#memory-profiling-with-mesos-and-jemalloc">Memory Profiling with Mesos and Jemalloc</a></h1>
<p>On Linux systems, Mesos is able to leverage the memory-profiling capabilities of
the <a href="http://jemalloc.net">jemalloc</a> general-purpose allocator to provide
powerful debugging tools for investigating memory-related issues.</p>
<p>These include detailed real-time statistics of the current memory usage, as well
as information about the location and frequency of individual allocations.</p>
<p>This generally works by having libprocess detect at runtime whether the current
process is using jemalloc as its memory allocator, and if so enable a number of
HTTP endpoints described below that allow operators to generate the desired data
at runtime.</p>
<p><a name="requirements"></a></p>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<p>A prerequisite for memory profiling is a suitable allocator. Currently only
jemalloc is supported, which can be connected via one of the following ways.</p>
<p>The recommended method is to specify the <code>--enable-jemalloc-allocator</code>
compile-time flag, which causes the <code>mesos-master</code> and <code>mesos-agent</code> binaries
to be statically linked against a bundled version of jemalloc that will be
compiled with the correct compile-time flags.</p>
<p>Alternatively and analogous to other bundled dependencies of Mesos, it is of
course also possible to use a <em>suitable</em> custom version of jemalloc with the
<code>--with-jemalloc=&lt;/path-to-jemalloc&gt;</code> flag.</p>
<p><strong>NOTE:</strong> Suitable here means that jemalloc should have been built with the
<code>--enable-stats</code> and <code>--enable-prof</code> flags, and that the string
<code>prof:true;prof_active:false</code> is part of the malloc configuration. The latter
condition can be satisfied either at configuration or at run-time, see the
section on <code>MALLOC_CONF</code> below.</p>
<p>The third way is to use the <code>LD_PRELOAD</code> mechanism to preload a <code>libjemalloc.so</code>
shared library that is present on the system at runtime. The <code>MemoryProfiler</code>
class in libprocess will automatically detect this and enable its memory
profiling support.</p>
<p>The generated profile dumps will be written to a random directory under <code>TMPDIR</code>
if set, otherwise in a subdirectory of <code>/tmp</code>.</p>
<p>Finally, note that since jemalloc was designed to be used in highly concurrent
allocation scenarios, it can improve performance over the default system
allocator. In this case, it can be beneficial to build Mesos with jemalloc even
if there is no intention to use the memory profiling functionality.</p>
<h2 id="usage-1"><a class="header" href="#usage-1">Usage</a></h2>
<p>There are two independent sets of data that can be collected from jemalloc:
memory statistics and heap profiling information.</p>
<p>Using any of the endpoints described below
<a href="memory-profiling.html#requirements">requires the jemalloc allocator</a> and starting the <code>mesos-agent</code>
or <code>mesos-master</code> binary with the option <code>--memory_profiling=true</code> (or setting
the environment variable <code>LIBPROCESS_MEMORY_PROFILING=true</code> for other binaries
using libprocess).</p>
<h3 id="memory-statistics"><a class="header" href="#memory-statistics">Memory Statistics</a></h3>
<p>The <code>/statistics</code> endpoint returns exact statistics about the memory usage in
JSON format, for example the number of bytes currently allocated and the size
distribution of these allocations.</p>
<p>It takes no parameters and will return the results in JSON format:</p>
<pre><code>http://example.org:5050/memory-profiler/statistics
</code></pre>
<p>Be aware that the returned JSON is quite large, so when accessing this endpoint
from a terminal, it is advisable to redirect the results into a file.</p>
<h3 id="heap-profiling"><a class="header" href="#heap-profiling">Heap Profiling</a></h3>
<p>The profiling done by jemalloc works by sampling from the calls to <code>malloc()</code>
according to a configured probability distribution, and storing stack traces for
the sampled calls in a separate memory area. These can then be dumped into files
on the filesystem, so-called heap profiles.</p>
<p>To start a profiling run one would access the <code>/start</code> endpoint:</p>
<pre><code>http://example.org:5050/memory-profiler/start?duration=5mins
</code></pre>
<p>followed by downloading one of the generated files described below after the
duration has elapsed. The remaining time of the current profiling run can be
verified via the <code>/state</code> endpoint:</p>
<pre><code>http://example.org:5050/memory-profiler/state
</code></pre>
<p>Since profiling information is stored process-global by jemalloc, only a single
concurrent profiling run is allowed. Additionally, only the results of the most
recently finished run are stored on disk.</p>
<p>The profile collection can also be stopped early with the <code>/stop</code> endpoint:</p>
<pre><code>http://example.org:5050/memory-profiler/stop
</code></pre>
<p>To analyze the generated profiling data, the results are offered in three
different formats.</p>
<h4 id="raw-profile"><a class="header" href="#raw-profile">Raw profile</a></h4>
<pre><code>http://example.org:5050/memory-profiler/download/raw
</code></pre>
<p>This returns a file in a plain text format containing the raw backtraces
collected, i.e., lists of memory addresses. It can be interactively analyzed
and rendered using the <code>jeprof</code> tool provided by the jemalloc project. For more
information on this file format, check out <a href="http://jemalloc.net/jemalloc.3.html#heap_profile_format">the official jemalloc
documentation</a>.</p>
<h4 id="symbolized-profile"><a class="header" href="#symbolized-profile">Symbolized profile</a></h4>
<pre><code>http://example.org:5050/memory-profiler/download/text
</code></pre>
<p>This is similar to the raw format above, except that <code>jeprof</code> is called on the
host machine to attempt to read symbol information from the current binary and
replace raw memory addresses in the profile by human-readable symbol names.</p>
<p>Usage of this endpoint requires that <code>jeprof</code> is present on the host machine
and on the <code>PATH</code>, and no useful information will be generated unless the binary
contains symbol information.</p>
<h4 id="call-graph"><a class="header" href="#call-graph">Call graph</a></h4>
<pre><code>http://example.org:5050/memory-profiler/download/graph
</code></pre>
<p>This endpoint returns an image in SVG format that shows a graphical
representation of the samples backtraces.</p>
<p>Usage of this endpoint requires that <code>jeprof</code> and <code>dot</code> are present on the host
machine and on the <code>PATH</code> of mesos, and no useful information will be generated
unless the binary contains symbol information.</p>
<h4 id="overview-2"><a class="header" href="#overview-2">Overview</a></h4>
<p>Which of these is needed will depend on the circumstances of the application
deployment and of the bug that is investigated.</p>
<p>For example, the call graph presents information in a visual, immediately useful
form, but is difficult to filter and post-process if non-default output options
are desired.</p>
<p>On the other hand, in many debian-like environments symbol information is by
default stripped from binaries to save space and shipped in separate packages.
In such an environment, if it is not permitted to install additional packages on
the host running Mesos, one would store the raw profiles and enrich them with
symbol information locally.</p>
<h2 id="jeprof-installation"><a class="header" href="#jeprof-installation">Jeprof Installation</a></h2>
<p>As described above, the <code>/download/text</code> and <code>/download/graph</code> endpoints require
the <code>jeprof</code> program installed on the host system. Where possible, it is
recommended to install <code>jeprof</code> through the system package manager, where it is
usually packaged alongside with jemalloc itself.</p>
<p>Alternatively, a copy of the script can be found under
<code>3rdparty/jemalloc-5.0.1/bin/jeprof</code> in the build directory, or can be
downloaded directly from the internet using a command like:</p>
<pre><code>$ curl https://raw.githubusercontent.com/jemalloc/jemalloc/dev/bin/jeprof.in | sed s/@jemalloc_version@/5.0.1/ &gt;jeprof
</code></pre>
<p>Note that <code>jeprof</code> is just a perl script that post-processes the raw profiles.
It has no connection to the jemalloc library besides being distributed in the
same package. In particular, it is generally not required to have matching
versions of jemalloc and <code>jeprof</code>.</p>
<p>If <code>jeprof</code> is installed manually, one also needs to take care to install the
necessary dependencies. In particular, this include the <code>perl</code> interpreter to
execute the script itself and the <code>dot</code> binary to generate graph files.</p>
<h2 id="command-line-usage"><a class="header" href="#command-line-usage">Command-line Usage</a></h2>
<p>In some circumstances, it might be desired to automate the downloading of heap
profiles by writing a simple script. A simple example for how this might look
like this:</p>
<pre><code>#!/bin/bash

SECONDS=600
HOST=example.org:5050

curl ${HOST}/memory-profiler/start?duration=${SECONDS}
sleep $((${SECONDS} + 1))
wget ${HOST}/memory-profiler/download/raw
</code></pre>
<p>A more sophisticated script would additionally store the <code>id</code> value returned by
the call to <code>/start</code> and pass it as a paremter to <code>/download</code>, to ensure that a
new run was not started in the meantime.</p>
<h2 id="using-the-malloc_conf-interface"><a class="header" href="#using-the-malloc_conf-interface">Using the <code>MALLOC_CONF</code> Interface</a></h2>
<p>The jemalloc allocator provides a native interface to control the memory
profiling behaviour. The usual way to provide settings through this interface is
by setting the environment variable <code>MALLOC_CONF</code>.</p>
<p><strong>NOTE:</strong> If libprocess detects that memory profiling was started through
<code>MALLOC_CONF</code>, it will reject starting a profiling run of its own to avoid
interference.</p>
<p>The <code>MALLOC_CONF</code> interface provides a number of options that are not exposed by
libprocess, like generating heap profiles automatically after a certain amount
of memory has been allocated, or whenever memory usage reaches a new high-water
mark. The full list of settings is described on the
<a href="http://jemalloc.net/jemalloc.3.html">jemalloc man page</a>.</p>
<p>On the other hand, features like starting and stopping the profiling at runtime
or getting the information provided by the <code>/statistics</code> endpoint can not be
achieved through the <code>MALLOC_CONF</code> interface.</p>
<p>For example, to create a dump automatically for every 1 GiB worth of recorded
allocations, one might use the configuration:</p>
<pre><code>MALLOC_CONF=&quot;prof:true,prof_prefix:/path/to/folder,lg_prof_interval=20&quot;
</code></pre>
<p>To debug memory allocations during early startup, profiling can be activated
before accessing the <code>/start</code> endpoint:</p>
<pre><code>MALLOC_CONF=&quot;prof:true,prof_active:true&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesos-attributes--resources"><a class="header" href="#mesos-attributes--resources">Mesos Attributes &amp; Resources</a></h1>
<p>Mesos has two basic methods to describe the agents that comprise a cluster. One of these is managed by the Mesos master, the other is simply passed onwards to the frameworks using the cluster.</p>
<h2 id="types"><a class="header" href="#types">Types</a></h2>
<p>The types of values that are supported by Attributes and Resources in Mesos are scalar, ranges, sets and text.</p>
<p>The following are the definitions of these types:</p>
<pre><code>scalar : floatValue

floatValue : ( intValue ( &quot;.&quot; intValue )? ) | ...

intValue : [0-9]+

range : &quot;[&quot; rangeValue ( &quot;,&quot; rangeValue )* &quot;]&quot;

rangeValue : scalar &quot;-&quot; scalar

set : &quot;{&quot; text ( &quot;,&quot; text )* &quot;}&quot;

text : [a-zA-Z0-9_/.-]
</code></pre>
<h2 id="attributes"><a class="header" href="#attributes">Attributes</a></h2>
<p>Attributes are key-value pairs (where value is optional) that Mesos passes along when it sends offers to frameworks. An attribute value supports three different <em>types</em>: scalar, range or text.</p>
<pre><code>attributes : attribute ( &quot;;&quot; attribute )*

attribute : text &quot;:&quot; ( scalar | range | text )
</code></pre>
<p>Note that setting multiple attributes corresponding to the same key is highly
discouraged (and might be disallowed in future), as this complicates attribute-
based filtering of offers, both on schedulers side and on the Mesos side.</p>
<h2 id="resources-2"><a class="header" href="#resources-2">Resources</a></h2>
<p>Mesos can manage three different <em>types</em> of resources: scalars, ranges, and sets.  These are used to represent the different resources that a Mesos agent has to offer.  For example, a scalar resource type could be used to represent the amount of memory on an agent. Scalar resources are represented using floating point numbers to allow fractional values to be specified (e.g., &quot;1.5 CPUs&quot;). Mesos only supports three decimal digits of precision for scalar resources (e.g., reserving &quot;1.5123 CPUs&quot; is considered equivalent to reserving &quot;1.512 CPUs&quot;). For GPUs, Mesos only supports whole number values.</p>
<p>Resources can be specified either with a JSON array or a semicolon-delimited string of key-value pairs.  If, after examining the examples below, you have questions about the format of the JSON, inspect the <code>Resource</code> protobuf message definition in <code>include/mesos/mesos.proto</code>.</p>
<p>As JSON:</p>
<pre><code>[
  {
    &quot;name&quot;: &quot;&lt;resource_name&gt;&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: &lt;resource_value&gt;
    }
  },
  {
    &quot;name&quot;: &quot;&lt;resource_name&gt;&quot;,
    &quot;type&quot;: &quot;RANGES&quot;,
    &quot;ranges&quot;: {
      &quot;range&quot;: [
        {
          &quot;begin&quot;: &lt;range_beginning&gt;,
          &quot;end&quot;: &lt;range_ending&gt;
        },
        ...
      ]
    }
  },
  {
    &quot;name&quot;: &quot;&lt;resource_name&gt;&quot;,
    &quot;type&quot;: &quot;SET&quot;,
    &quot;set&quot;: {
      &quot;item&quot;: [
        &quot;&lt;first_item&gt;&quot;,
        ...
      ]
    },
    &quot;role&quot;: &quot;&lt;role_name&gt;&quot;
  },
  ...
]
</code></pre>
<p>As a list of key-value pairs:</p>
<pre><code>resources : resource ( &quot;;&quot; resource )*

resource : key &quot;:&quot; ( scalar | range | set )

key : text ( &quot;(&quot; resourceRole &quot;)&quot; )?

resourceRole : text | &quot;*&quot;
</code></pre>
<p>Note that <code>resourceRole</code> must be a valid role name; see the <a href="roles.html">roles</a> documentation for details.</p>
<h2 id="predefined-uses--conventions"><a class="header" href="#predefined-uses--conventions">Predefined Uses &amp; Conventions</a></h2>
<p>There are several kinds of resources that have predefined behavior:</p>
<ul>
<li><code>cpus</code></li>
<li><code>gpus</code></li>
<li><code>disk</code></li>
<li><code>mem</code></li>
<li><code>ports</code></li>
</ul>
<p>Note that <code>disk</code> and <code>mem</code> resources are specified in megabytes. The master's user interface will convert resource values into a more human-readable format: for example, the value <code>15000</code> will be displayed as <code>14.65GB</code>.</p>
<p>An agent without <code>cpus</code> and <code>mem</code> resources will not have its resources advertised to any frameworks.</p>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<p>By default, Mesos will try to autodetect the resources available at the local machine when <code>mesos-agent</code> starts up. Alternatively, you can explicitly configure which resources an agent should make available.</p>
<p>Here are some examples of how to configure the resources at a Mesos agent:</p>
<pre><code>--resources='cpus:24;gpus:2;mem:24576;disk:409600;ports:[21000-24000,30000-34000];bugs(debug_role):{a,b,c}'

--resources='[{&quot;name&quot;:&quot;cpus&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:24}},{&quot;name&quot;:&quot;gpus&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:2}},{&quot;name&quot;:&quot;mem&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:24576}},{&quot;name&quot;:&quot;disk&quot;,&quot;type&quot;:&quot;SCALAR&quot;,&quot;scalar&quot;:{&quot;value&quot;:409600}},{&quot;name&quot;:&quot;ports&quot;,&quot;type&quot;:&quot;RANGES&quot;,&quot;ranges&quot;:{&quot;range&quot;:[{&quot;begin&quot;:21000,&quot;end&quot;:24000},{&quot;begin&quot;:30000,&quot;end&quot;:34000}]}},{&quot;name&quot;:&quot;bugs&quot;,&quot;type&quot;:&quot;SET&quot;,&quot;set&quot;:{&quot;item&quot;:[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]},&quot;role&quot;:&quot;debug_role&quot;}]'
</code></pre>
<p>Or given a file <code>resources.txt</code> containing the following:</p>
<pre><code>[
  {
    &quot;name&quot;: &quot;cpus&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 24
    }
  },
  {
    &quot;name&quot;: &quot;gpus&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 2
    }
  },
  {
    &quot;name&quot;: &quot;mem&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 24576
    }
  },
  {
    &quot;name&quot;: &quot;disk&quot;,
    &quot;type&quot;: &quot;SCALAR&quot;,
    &quot;scalar&quot;: {
      &quot;value&quot;: 409600
    }
  },
  {
    &quot;name&quot;: &quot;ports&quot;,
    &quot;type&quot;: &quot;RANGES&quot;,
    &quot;ranges&quot;: {
      &quot;range&quot;: [
        {
          &quot;begin&quot;: 21000,
          &quot;end&quot;: 24000
        },
        {
          &quot;begin&quot;: 30000,
          &quot;end&quot;: 34000
        }
      ]
    }
  },
  {
    &quot;name&quot;: &quot;bugs&quot;,
    &quot;type&quot;: &quot;SET&quot;,
    &quot;set&quot;: {
      &quot;item&quot;: [
        &quot;a&quot;,
        &quot;b&quot;,
        &quot;c&quot;
      ]
    },
    &quot;role&quot;: &quot;debug_role&quot;
  }
]
</code></pre>
<p>You can do:</p>
<pre><code>$ path/to/mesos-agent --resources=file:///path/to/resources.txt ...
</code></pre>
<p>In this case, we have five resources of three different types: scalars, a range, and a set.  There are scalars called <code>cpus</code>, <code>gpus</code>, <code>mem</code> and <code>disk</code>, a range called <code>ports</code>, and a set called <code>bugs</code>. <code>bugs</code> is assigned to the role <code>debug_role</code>, while the other resources do not specify a role and are thus assigned to the default role.</p>
<p>Note: the &quot;default role&quot; can be set by the <code>--default_role</code> flag.</p>
<ul>
<li>scalar called <code>cpus</code>, with the value <code>24</code></li>
<li>scalar called <code>gpus</code>, with the value <code>2</code></li>
<li>scalar called <code>mem</code>, with the value <code>24576</code></li>
<li>scalar called <code>disk</code>, with the value <code>409600</code></li>
<li>range called <code>ports</code>, with values <code>21000</code> through <code>24000</code> and <code>30000</code> through <code>34000</code> (inclusive)</li>
<li>set called <code>bugs</code>, with the values <code>a</code>, <code>b</code> and <code>c</code>, assigned to the role <code>debug_role</code></li>
</ul>
<p>To configure the attributes of a Mesos agent, you can use the <code>--attributes</code> command-line flag of <code>mesos-agent</code>:</p>
<pre><code>--attributes='rack:abc;zone:west;os:centos5;level:10;keys:[1000-1500]'
</code></pre>
<p>That will result in configuring the following five attributes:</p>
<ul>
<li><code>rack</code> with text value <code>abc</code></li>
<li><code>zone</code> with text value <code>west</code></li>
<li><code>os</code> with text value <code>centos5</code></li>
<li><code>level</code> with scalar value 10</li>
<li><code>keys</code> with range value <code>1000</code> through <code>1500</code> (inclusive)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Roles
layout: documentation</h2>
<h1 id="roles"><a class="header" href="#roles">Roles</a></h1>
<p>Many modern host-level operating systems (e.g. Linux, BSDs, etc) support
multiple users. Similarly, Mesos is a multi-user cluster management system,
with the expectation of a single Mesos cluster managing an organization's
resources and servicing the organization's users.</p>
<p>As such, Mesos has to address a number of requirements related to resource
management:</p>
<ul>
<li>Fair sharing of the resources amongst users</li>
<li>Providing resource guarantees to users (e.g. quota, priorities, isolation)</li>
<li>Providing accurate resource accounting
<ul>
<li>How many resources are allocated / utilized / etc?</li>
<li>Per-user accounting</li>
</ul>
</li>
</ul>
<p>In Mesos, we refer to these &quot;users&quot; as <strong>roles</strong>. More precisely, a <strong>role</strong>
within Mesos refers to a resource consumer within the cluster. This resource
consumer could represent a user within an organization, but it could also
represent a team, a group, a service, a framework, etc.</p>
<p>Schedulers subscribe to one or more roles in order to receive resources and
schedule work on behalf of the resource consumer(s) they are servicing.</p>
<p>Some examples of resource allocation guarantees that Mesos provides:</p>
<ul>
<li>Guaranteeing that a role is allocated a specified amount of resources
(via <a href="quota.html">quota</a>).</li>
<li>Ensuring that some (or all) of the resources on a particular agent
are allocated to a particular role (via <a href="reservation.html">reservations</a>).</li>
<li>Ensuring that resources are fairly shared between roles
(via <a href="https://www.cs.berkeley.edu/~alig/papers/drf.pdf">DRF</a>).</li>
<li>Expressing that some roles should receive a higher relative share of the
cluster (via <a href="weights.html">weights</a>).</li>
</ul>
<h2 id="roles-and-access-control"><a class="header" href="#roles-and-access-control">Roles and access control</a></h2>
<p>There are two ways to control which roles a framework is allowed to subscribe
to. First, ACLs can be used to specify which framework principals can subscribe
to which roles. For more information, see the <a href="authorization.html">authorization</a>
documentation.</p>
<p>Second, a <em>role whitelist</em> can be configured by passing the <code>--roles</code> flag to
the Mesos master at startup. This flag specifies a comma-separated list of role
names. If the whitelist is specified, only roles that appear in the whitelist
can be used. To change the whitelist, the Mesos master must be restarted. Note
that in a high-availability deployment of Mesos, you should take care to ensure
that all Mesos masters are configured with the same whitelist.</p>
<p>In Mesos 0.26 and earlier, you should typically configure <em>both</em> ACLs and the
whitelist, because in these versions of Mesos, any role that does not appear in
the whitelist cannot be used.</p>
<p>In Mesos 0.27, this behavior has changed: if <code>--roles</code> is not specified, the
whitelist permits <em>any role name</em> to be used. Hence, in Mesos 0.27, the
recommended practice is to only use ACLs to define which roles can be used; the
<code>--roles</code> command-line flag is deprecated.</p>
<h2 id="associating-frameworks-with-roles"><a class="header" href="#associating-frameworks-with-roles">Associating frameworks with roles</a></h2>
<p>A framework specifies which roles it would like to subscribe to when it
subscribes with the master. This is done via the <code>roles</code> field in
<code>FrameworkInfo</code>. A framework can also change which roles it is
subscribed to by reregistering with an updated <code>FrameworkInfo</code>.</p>
<p>As a user, you can typically specify which role(s) a framework will
subscribe to when you start the framework. How to do this depends on the
user interface of the framework you're using. For example, a single user
scheduler might take a <code>--mesos_role</code> command-line flag and a multi-user
scheduler might take a <code>--mesos-roles</code> command-line flag or sync with
the organization's LDAP system to automatically adjust which roles it is
subscribed to as the organization's structure changes.</p>
<h3 id="subscribing-to-multiple-roles"><a class="header" href="#subscribing-to-multiple-roles">Subscribing to multiple roles</a></h3>
<p>As noted above, a framework can subscribe to multiple roles
simultaneously. Frameworks that want to do this must opt-in to the
<code>MULTI_ROLE</code> capability.</p>
<p>When a framework is offered resources, those resources are associated
with exactly <em>one</em> of the roles it has subscribed to; the framework can
determine which role an offer is for by consulting the
<code>allocation_info.role</code> field in the <code>Offer</code> or the
<code>allocation_info.role</code> field in each offered <code>Resource</code> (in the current
implementation, all the resources in a single <code>Offer</code> will be allocated
to the same role).</p>
<p><a id="roles-multiple-frameworks"></a></p>
<h3 id="multiple-frameworks-in-the-same-role"><a class="header" href="#multiple-frameworks-in-the-same-role">Multiple frameworks in the same role</a></h3>
<p>Multiple frameworks can be subscribed to the same role. This can be useful:
for example, one framework can create a persistent volume and write data to
it. Once the task that writes data to the persistent volume has finished,
the volume will be offered to other frameworks subscribed to the same role;
this might give a second (&quot;consumer&quot;) framework the opportunity to launch a
task that reads the data produced by the first (&quot;producer&quot;) framework.</p>
<p>However, configuring multiple frameworks to use the same role should be done
with caution, because all the frameworks will have access to any resources that
have been reserved for that role. For example, if a framework stores sensitive
information on a persistent volume, that volume might be offered to a different
framework subscribed to the same role. Similarly, if one framework creates a
persistent volume, another framework subscribed to the same role might &quot;steal&quot;
the volume and use it to launch a task of its own. In general, multiple
frameworks sharing the same role should be prepared to collaborate with one
another to ensure that role-specific resources are used appropriately.</p>
<h2 id="associating-resources-with-roles"><a class="header" href="#associating-resources-with-roles">Associating resources with roles</a></h2>
<p>A resource is assigned to a role using a <em>reservation</em>. Resources can either be
reserved <em>statically</em> (when the agent that hosts the resource is started) or
<em>dynamically</em>: frameworks and operators can specify that a certain resource
should subsequently be reserved for use by a given role. For more information,
see the <a href="reservation.html">reservation</a> documentation.</p>
<h2 id="default-role"><a class="header" href="#default-role">Default role</a></h2>
<p>The role named <code>*</code> is special. Unreserved resources are currently represented
as having the special <code>*</code> role (the idea being that <code>*</code> matches any role). By
default, all the resources at an agent node are unreserved (this can be changed
via the <code>--default_role</code> command-line flag when starting the agent).</p>
<p>In addition, when a framework registers without providing a
<code>FrameworkInfo.role</code>, it is assigned to the <code>*</code> role. In Mesos 1.3, frameworks
should use the <code>FrameworkInfo.roles</code> field, which does not assign a default of
<code>*</code>, but frameworks can still specify <code>*</code> explicitly if desired. Frameworks
and operators cannot make reservations to the <code>*</code> role.</p>
<h2 id="invalid-role-names"><a class="header" href="#invalid-role-names">Invalid role names</a></h2>
<p>A role name must be a valid directory name, so it cannot:</p>
<ul>
<li>Be an empty string</li>
<li>Be <code>.</code> or <code>..</code></li>
<li>Start with <code>-</code></li>
<li>Contain any slash, backspace, or whitespace character</li>
</ul>
<h2 id="roles-and-resource-allocation"><a class="header" href="#roles-and-resource-allocation">Roles and resource allocation</a></h2>
<p>By default, the Mesos master uses weighted Dominant Resource Fairness (wDRF) to
allocate resources. In particular, this implementation of wDRF first identifies
which <em>role</em> is furthest below its fair share of the role's dominant resource.
Each of the frameworks subscribed to that role are then offered additional
resources in turn.</p>
<p>The resource allocation process can be customized by assigning
<em><a href="weights.html">weights</a></em> to roles: a role with a weight of 2 will be allocated
twice the fair share of a role with a weight of 1. By default, every role has a
weight of 1. Weights can be configured using the
<a href="endpoints/master/weights.html">/weights</a> operator endpoint, or else using the
deprecated <code>--weights</code> command-line flag when starting the Mesos master.</p>
<h2 id="roles-and-quota"><a class="header" href="#roles-and-quota">Roles and quota</a></h2>
<p>In order to guarantee that a role is allocated a specific amount of resources,
quota can be specified via the <a href="endpoints/master/quota.html">/quota</a> endpoint.</p>
<p>The resource allocator will first attempt to satisfy the quota requirements,
before fairly sharing the remaining resources. For more information, see the
<a href="quota.html">quota</a> documentation.</p>
<h2 id="role-vs-principal"><a class="header" href="#role-vs-principal">Role vs. Principal</a></h2>
<p>A principal identifies an entity that interacts with Mesos; principals are
similar to user names. For example, frameworks supply a principal when they
register with the Mesos master, and operators provide a principal when using the
operator HTTP endpoints. An entity may be required to
<a href="authentication.html">authenticate</a> with its principal in order to prove its
identity, and the principal may be used to <a href="authorization.html">authorize</a> actions
performed by an entity, such as <a href="reservation.html">resource reservation</a> and
<a href="persistent-volume.html">persistent volume</a> creation/destruction.</p>
<p>Roles, on the other hand, are used exclusively for resource allocation, as
covered above.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Weights
layout: documentation</h2>
<h1 id="weights"><a class="header" href="#weights">Weights</a></h1>
<p>In Mesos, <strong>weights</strong> can be used to control the relative share of cluster
resources that is offered to different <a href="roles.html">roles</a>.</p>
<p>In Mesos 0.28 and earlier, weights can only be configured by specifying
the <code>--weights</code> command-line flag when starting the Mesos master. If a
role does not have a weight specified in the <code>--weights</code> flag, then the default
value (1.0) will be used. Weights cannot be changed without updating the flag
and restarting all Mesos masters.</p>
<p>Mesos 1.0 contains a <a href="endpoints/master/weights.html">/weights</a> operator endpoint
that allows weights to be changed at runtime. The <code>--weights</code> command-line flag
is deprecated.</p>
<h1 id="operator-http-endpoint"><a class="header" href="#operator-http-endpoint">Operator HTTP Endpoint</a></h1>
<p>The master <code>/weights</code> HTTP endpoint enables operators to configure weights. The
endpoint currently offers a REST-like interface and supports the following operations:</p>
<ul>
<li><a href="weights.html#putRequest">Updating</a> weights with PUT.</li>
<li><a href="weights.html#getRequest">Querying</a> the currently set weights with GET.</li>
</ul>
<p>The endpoint can optionally use authentication and authorization. See the
<a href="authentication.html">authentication guide</a> for details.</p>
<p><a name="putRequest"></a></p>
<h2 id="update"><a class="header" href="#update">Update</a></h2>
<p>The operator can update the weights by sending an HTTP PUT request to the <code>/weights</code>
endpoint.</p>
<p>An example request to the <code>/weights</code> endpoint could look like this (using the
JSON file below):</p>
<pre><code>$ curl -d @weights.json -X PUT http://&lt;master-ip&gt;:&lt;port&gt;/weights
</code></pre>
<p>For example, to set a weight of <code>2.0</code> for <code>role1</code> and set a weight of <code>3.5</code>
for <code>role2</code>, the operator can use the following <code>weights.json</code>:</p>
<pre><code>    [
      {
        &quot;role&quot;: &quot;role1&quot;,
        &quot;weight&quot;: 2.0
      },
      {
        &quot;role&quot;: &quot;role2&quot;,
        &quot;weight&quot;: 3.5
      }
    ]
</code></pre>
<p>If the master is configured with an explicit <a href="roles.html">role whitelist</a>, the
request is only valid if all specified roles exist in the role whitelist.</p>
<p>Weights are now persisted in the registry on cluster bootstrap and after any
updates.  Once the weights are persisted in the registry, any Mesos master that
subsequently starts with <code>--weights</code> still specified will emit a warning and use
the registry value instead.</p>
<p>The operator will receive one of the following HTTP response codes:</p>
<ul>
<li><code>200 OK</code>: Success (the update request was successful).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., invalid JSON, non-positive weights).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
</ul>
<p><a name="getRequest"></a></p>
<h2 id="query"><a class="header" href="#query">Query</a></h2>
<p>The operator can query the configured weights by sending an HTTP GET request
to the <code>/weights</code> endpoint.</p>
<pre><code>$ curl -X GET http://&lt;master-ip&gt;:&lt;port&gt;/weights
</code></pre>
<p>The response message body includes a JSON representation of the current
configured weights, for example:</p>
<pre><code>    [
      {
        &quot;role&quot;: &quot;role2&quot;,
        &quot;weight&quot;: 3.5
      },
      {
        &quot;role&quot;: &quot;role1&quot;,
        &quot;weight&quot;: 2.0
      }
    ]
</code></pre>
<p>The operator will receive one of the following HTTP response codes:</p>
<ul>
<li><code>200 OK</code>: Success.</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Quota
layout: documentation</h2>
<h1 id="quota"><a class="header" href="#quota">Quota</a></h1>
<p>When multiple users are sharing a cluster, the operator may want to set limits
on how many resources each user can use. Quota addresses this need and allows
operators to set these limits on a per-role basis.</p>
<ul>
<li><a href="quota.html#supported-resources">Supported Resources</a></li>
<li><a href="quota.html#updating-quotas">Setting Quotas</a></li>
<li><a href="quota.html#viewing-quotas">Viewing Quotas</a></li>
<li><a href="quota.html#deprecated-quota-guarantees">Deprecated: Quota Guarantees</a></li>
<li><a href="quota.html#implementation-notes">Implementation Notes</a></li>
</ul>
<h2 id="supported-resources"><a class="header" href="#supported-resources">Supported Resources</a></h2>
<p>The following resources have quota support:</p>
<ul>
<li><code>cpus</code></li>
<li><code>mem</code></li>
<li><code>disk</code></li>
<li><code>gpus</code></li>
<li>any custom resource of type <code>SCALAR</code></li>
</ul>
<p>The following resources do not have quota support:</p>
<ul>
<li><code>ports</code></li>
<li>any custom resource of type <code>RANGES</code> or <code>SET</code></li>
</ul>
<h2 id="updating-quotas"><a class="header" href="#updating-quotas">Updating Quotas</a></h2>
<p>By default, every role has no resource limits. To modify the resource limits
for one or more roles, the v1 API <code>UPDATE_QUOTA</code> call is used. Note that this
call applies the update in an all-or-nothing manner, so that if one of the
role's quota updates is invalid or unauthorized, the entire request will not
go through.</p>
<p>Example:</p>
<pre><code>curl --request POST \
     --url http://&lt;master-ip&gt;:&lt;master-port&gt;/api/v1/ \
     --header 'Content-Type: application/json' \
     --data '{
               &quot;type&quot;: &quot;UPDATE_QUOTA&quot;,
               &quot;update_quota&quot;: {
                 &quot;force&quot;: false,
                 &quot;quota_configs&quot;: [
                   {
                     &quot;role&quot;: &quot;dev&quot;,
                     &quot;limits&quot;: {
                       &quot;cpus&quot;: { &quot;value&quot;: 10 },
                       &quot;mem&quot;:  { &quot;value&quot;: 2048 },
                       &quot;disk&quot;: { &quot;value&quot;: 4096 }
                     }
                   },
                   {
                     &quot;role&quot;: &quot;test&quot;,
                     &quot;limits&quot;: {
                       &quot;cpus&quot;: { &quot;value&quot;: 1 },
                       &quot;mem&quot;:  { &quot;value&quot;: 256 },
                       &quot;disk&quot;: { &quot;value&quot;: 512 }
                     }
                   }
                 ]
               }
             }'
</code></pre>
<ul>
<li>Note that the request will be denied if the current quota consumption is above
the provided limit. This check can be overriden by setting <code>force</code> to <code>true</code>.</li>
<li>Note that the master will attempt to rescind a sufficient number of offers to
ensure that the role cannot exceed its limits.</li>
</ul>
<h2 id="viewing-quotas"><a class="header" href="#viewing-quotas">Viewing Quotas</a></h2>
<h3 id="web-ui"><a class="header" href="#web-ui">Web UI</a></h3>
<p>The 'Roles' tab in the web ui displays resource accounting information for all
known roles. This includes the configured quota and the quota consumption.</p>
<h3 id="api"><a class="header" href="#api">API</a></h3>
<p>There are several endpoints for viewing quota related information.</p>
<p>The v1 API <code>GET_QUOTA</code> call will return the quota configuration:</p>
<pre><code>$ curl --request POST \
     --url http://&lt;master-ip&gt;:&lt;master-port&gt;/api/v1/ \
     --header 'Content-Type: application/json' \
     --header 'Accept: application/json' \
     --data '{ &quot;type&quot;: &quot;GET_QUOTA&quot; }'
</code></pre>
<p>Response:</p>
<pre><code>{
  &quot;type&quot;: &quot;GET_QUOTA&quot;,
  &quot;get_quota&quot;: {
    &quot;status&quot;: {
      &quot;infos&quot;: [
        {
          &quot;configs&quot; : [
            {
              &quot;role&quot;: &quot;dev&quot;,
              &quot;limits&quot;: {
                &quot;cpus&quot;: { &quot;value&quot;: 10.0 },
                &quot;mem&quot;:  { &quot;value&quot;: 2048.0 },
                &quot;disk&quot;: { &quot;value&quot;: 4096.0 }
              }
            },
            {
              &quot;role&quot;: &quot;test&quot;,
              &quot;limits&quot;: {
                &quot;cpus&quot;: { &quot;value&quot;: 1.0 },
                &quot;mem&quot;:  { &quot;value&quot;: 256.0 },
                &quot;disk&quot;: { &quot;value&quot;: 512.0 }
              }
            }
          ]
        }
      ]
    }
  }
}
</code></pre>
<p>To also view the quota consumption, use the <code>/roles</code> endpoint:</p>
<pre><code>$ curl http://&lt;master-ip&gt;:&lt;master-port&gt;/roles
</code></pre>
<p>Response</p>
<pre><code>{
  &quot;roles&quot;: [
    {
      &quot;name&quot;: &quot;dev&quot;,
      &quot;weight&quot;: 1.0,
      &quot;quota&quot;:
      {
        &quot;role&quot;: &quot;dev&quot;,
        &quot;limit&quot;: {
          &quot;cpus&quot;: 10.0,
          &quot;mem&quot;:  2048.0,
          &quot;disk&quot;: 4096.0
        },
        &quot;consumed&quot;: {
          &quot;cpus&quot;: 2.0,
          &quot;mem&quot;:  1024.0,
          &quot;disk&quot;: 2048.0
        }
      },
      &quot;allocated&quot;: {
        &quot;cpus&quot;: 2.0,
        &quot;mem&quot;:  1024.0,
        &quot;disk&quot;: 2048.0
      },
      &quot;offered&quot;: {},
      &quot;reserved&quot;: {
        &quot;cpus&quot;: 2.0,
        &quot;mem&quot;:  1024.0,
        &quot;disk&quot;: 2048.0
      },
      &quot;frameworks&quot;: []
    }
  ]
}
</code></pre>
<h3 id="quota-consumption"><a class="header" href="#quota-consumption">Quota Consumption</a></h3>
<p>A role's quota consumption consists of its allocations and reservations.
In other words, even if reservations are not allocated, they are included
in the quota consumption. Offered resources are not charged against quota.</p>
<h3 id="metrics"><a class="header" href="#metrics">Metrics</a></h3>
<p>The following metric keys are exposed for quota:</p>
<ul>
<li><code>allocator/mesos/quota/roles/&lt;role&gt;/resources/&lt;resource&gt;/guarantee</code></li>
<li><code>allocator/mesos/quota/roles/&lt;role&gt;/resources/&lt;resource&gt;/limit</code></li>
<li>A quota consumption metric will be added via
<a href="https://issues.apache.org/jira/browse/MESOS-9123">MESOS-9123</a>.</li>
</ul>
<h2 id="deprecated-quota-guarantees"><a class="header" href="#deprecated-quota-guarantees">Deprecated: Quota Guarantees</a></h2>
<p>Prior to Mesos 1.9, the quota related APIs only exposed quota &quot;guarantees&quot;
which ensured a minimum amount of resources would be available to a role.
Setting guarantees also set implicit quota limits. In Mesos 1.9+, quota
limits are now exposed directly per the above documentation.</p>
<p>Quota guarantees are now deprecated in favor of using only quota limits.
Enforcement of quota guarantees required that Mesos holds back enough
resources to meet all of the unsatisfied quota guarantees. Since Mesos is
moving towards an optimistic offer model (to improve multi-role / multi-
scheduler scalability, see
<a href="https://issues.apache.org/jira/browse/MESOS-1607">MESOS-1607</a>), it will
become no longer possible to enforce quota guarantees by holding back
resources. In such a model, quota limits are simple to enforce, but quota
guarantees would require a complex &quot;effective limit&quot; propagation model to
leave space for unsatisfied guarantees.</p>
<p>For these reasons, quota guarantees, while still functional in Mesos 1.9,
are now deprecated. A combination of limits and priority based preemption
will be simpler in an optimistic offer model.</p>
<p>For documentation on quota guarantees, please see the previous
documentation: <a href="https://github.com/apache/mesos/blob/1.8.0/docs/quota.md">https://github.com/apache/mesos/blob/1.8.0/docs/quota.md</a></p>
<h2 id="implementation-notes"><a class="header" href="#implementation-notes">Implementation Notes</a></h2>
<ul>
<li>Quota is not supported on nested roles (e.g. <code>eng/prod</code>).</li>
<li>During failover, in order to correctly enforce limits, the allocator
will be paused and will not issue offers until at least 80% agents
re-register or 10 minutes elapses. These parameters will be made
configurable: <a href="https://issues.apache.org/jira/browse/MESOS-4073">MESOS-4073</a></li>
<li>Quota is SUPPORTED for the default <code>*</code> role now <a href="https://issues.apache.org/jira/browse/MESOS-3938">MESOS-3938</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Reservation
layout: documentation</h2>
<h1 id="reservation"><a class="header" href="#reservation">Reservation</a></h1>
<p>Mesos provides mechanisms to <strong>reserve</strong> resources in specific slaves.
The concept was first introduced with <strong>static reservation</strong> in 0.14.0
which enabled operators to specify the reserved resources on slave startup.
This was extended with <strong>dynamic reservation</strong> in 0.23.0 which enabled operators
and authorized <strong>frameworks</strong> to dynamically reserve resources in the cluster.</p>
<p>In both types of reservations, resources are reserved for a <a href="roles.html"><strong>role</strong></a>.</p>
<p><a name="static-reservation"></a></p>
<h2 id="static-reservation"><a class="header" href="#static-reservation">Static Reservation</a></h2>
<p>An operator can configure a slave with resources reserved for a role.
The reserved resources are specified via the <code>--resources</code> flag.
For example, suppose we have 12 CPUs and 6144 MB of RAM available on a slave and
that we want to reserve 8 CPUs and 4096 MB of RAM for the <code>ads</code> role.
We start the slave like so:</p>
<pre><code>    $ mesos-slave \
      --master=&lt;ip&gt;:&lt;port&gt; \
      --resources=&quot;cpus:4;mem:2048;cpus(ads):8;mem(ads):4096&quot;
</code></pre>
<p>We now have 8 CPUs and 4096 MB of RAM reserved for <code>ads</code> on this slave.</p>
<p><strong>CAVEAT:</strong> In order to modify a static reservation, the operator must drain and
restart the slave with the new configuration specified in the
<code>--resources</code> flag.</p>
<p>It's often more convenient to specify the total resources available on
the slave as unreserved via the <code>--resources</code> flag and manage reservations
dynamically (see below) via the master HTTP endpoints. However static
reservation provides a way for the operator to more deterministically control
the reservations (roles, amount, principals) before the agent is exposed to the
master and frameworks. One use case is for the operator to dedicate entire
agents for specific roles.</p>
<h2 id="dynamic-reservation"><a class="header" href="#dynamic-reservation">Dynamic Reservation</a></h2>
<p>As mentioned in <a href="reservation.html#static-reservation">Static Reservation</a>, specifying
the reserved resources via the <code>--resources</code> flag makes the reservation static.
That is, statically reserved resources cannot be reserved for another role nor
be unreserved. Dynamic reservation enables operators and authorized frameworks
to reserve and unreserve resources after slave-startup.</p>
<ul>
<li><code>Offer::Operation::Reserve</code> and <code>Offer::Operation::Unreserve</code> messages are
available for <strong>frameworks</strong> to send back via the <code>acceptOffers</code> API as a
response to a resource offer.</li>
<li><code>/reserve</code> and <code>/unreserve</code> HTTP endpoints allow <strong>operators</strong> to manage
dynamic reservations through the master.</li>
</ul>
<p>In the following sections, we will walk through examples of each of the
interfaces described above.</p>
<p>If two dynamic reservations are made for the same role at a single slave (using
the same labels, if any; see below), the reservations will be combined by adding
together the resources reserved by each request. This will result in a single
reserved resource at the slave. Similarly, &quot;partial&quot; unreserve operations are
allowed: an unreserve operation can release some but not all of the resources at
a slave that have been reserved for a role. In this case, the unreserved
resources will be subtracted from the previous reservation and any remaining
resources will still be reserved.</p>
<p>Dynamic reservations cannot be unreserved if they are still being used by a
running task or if a <a href="persistent-volume.html">persistent volume</a> has been created
using the reserved resources. In the latter case, the volume should be destroyed
before unreserving the resources.</p>
<h2 id="authorization"><a class="header" href="#authorization">Authorization</a></h2>
<p>By default, frameworks and operators are authorized to reserve resources for
any role and to unreserve dynamically reserved resources.
<a href="authorization.html">Authorization</a> allows this behavior to be limited so that
resources can only be reserved for particular roles, and only particular
resources can be unreserved. For these operations to be authorized, the
framework or operator should provide a <code>principal</code> to identify itself. To use
authorization with reserve/unreserve operations, the Mesos master must be
configured with the appropriate ACLs. For more information, see the
<a href="authorization.html">authorization documentation</a>.</p>
<p>Similarly, agents by default can register with the master with resources that
are statically reserved for arbitrary roles.
With <a href="authorization.html">authorization</a>,
the master can be configured to use the <code>reserve_resources</code> ACL to check that
the agent's <code>principal</code> is allowed to statically reserve resources for specific
roles.</p>
<h2 id="reservation-labels"><a class="header" href="#reservation-labels">Reservation Labels</a></h2>
<p>Dynamic reservations can optionally include a list of <em>labels</em>, which are
arbitrary key-value pairs. Labels can be used to associate arbitrary metadata
with a resource reservation. For example, frameworks can use labels to identify
the intended purpose for a portion of the resources that have been reserved at a
given slave. Note that two reservations with different labels will not be
combined together into a single reservation, even if the reservations are at the
same slave and use the same role.</p>
<h2 id="reservation-refinement"><a class="header" href="#reservation-refinement">Reservation Refinement</a></h2>
<p>Hierarhical roles such as <code>eng/backend</code> enable the delegation of resources
down a hierarchy, and reservation refinement is the mechanism with which
<strong>reservations</strong> are delegated down the hierarchy. For example, a reservation
(static or dynamic) for <code>eng</code> can be <strong>refined</strong> to <code>eng/backend</code>. When such
a reservation is unreserved, they are returned to the previous owner. In this
case it would be returned to <code>eng</code>. Reservation refinements can also &quot;skip&quot;
levels. For example, <code>eng</code> can be <strong>refined</strong> directly to <code>eng/backend/db</code>.
Again, unreserving such a reservation is returned to its previous owner <code>eng</code>.</p>
<p><strong>NOTE:</strong> Frameworks need to enable the <code>RESERVATION_REFINEMENT</code> capability
in order to be offered, and to create refined reservations</p>
<h2 id="listing-reservations"><a class="header" href="#listing-reservations">Listing Reservations</a></h2>
<p>Information about the reserved resources at each slave in the cluster can be
found by querying the <a href="endpoints/master/slaves.html">/slaves</a> master endpoint
(under the <code>reserved_resources_full</code> key).</p>
<p>The same information can also be found in the <a href="endpoints/slave/state.html">/state</a>
endpoint on the agent (under the <code>reserved_resources_full</code> key). The agent
endpoint is useful to confirm if a reservation has been propagated to the
agent (which can fail in the event of network partition or master/agent
restarts).</p>
<h2 id="examples-3"><a class="header" href="#examples-3">Examples</a></h2>
<h3 id="framework-scheduler-api"><a class="header" href="#framework-scheduler-api">Framework Scheduler API</a></h3>
<p><a name="offer-operation-reserve"></a></p>
<h4 id="offeroperationreserve-without-reservation_refinement"><a class="header" href="#offeroperationreserve-without-reservation_refinement"><code>Offer::Operation::Reserve</code> (<strong>without</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework can reserve resources through the resource offer cycle. The
reservation role must match the offer's allocation role. Suppose we
receive a resource offer with 12 CPUs and 6144 MB of RAM unreserved, allocated
to role <code>&quot;engineering&quot;</code>.</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 12 },
          &quot;role&quot;: &quot;*&quot;,
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 6144 },
          &quot;role&quot;: &quot;*&quot;,
        }
      ]
    }
</code></pre>
<p>We can reserve 8 CPUs and 4096 MB of RAM by sending the following
<code>Offer::Operation</code> message. <code>Offer::Operation::Reserve</code> has a <code>resources</code> field
which we specify with the resources to be reserved. We must explicitly set the
resources' <code>role</code> field to the offer's allocation role. The required value of
the <code>principal</code> field depends on whether or not the framework provided a
principal when it registered with the master. If a principal was provided, then
the resources' <code>principal</code> field must be equal to the framework's principal.
If no principal was provided during registration, then the resources'
<code>principal</code> field can take any value, or can be left unset. Note that the
<code>principal</code> field determines the &quot;reserver principal&quot; when
<a href="authorization.html">authorization</a> is enabled, even if authentication is
disabled.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::RESERVE,
      &quot;reserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          }
        ]
      }
    }
</code></pre>
<p>If the reservation is successful, a subsequent resource offer will contain the
following reserved resources:</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
      ]
    }
</code></pre>
<h4 id="offeroperationunreserve-without-reservation_refinement"><a class="header" href="#offeroperationunreserve-without-reservation_refinement"><code>Offer::Operation::Unreserve</code> (<strong>without</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework can unreserve resources through the resource offer cycle.
In <a href="reservation.html#offer-operation-reserve">Offer::Operation::Reserve</a>, we reserved 8 CPUs
and 4096 MB of RAM on a particular slave for one of our subscribed roles
(e.g. <code>&quot;engineering&quot;</code>). The master will continue to only offer these reserved
resources to the reservation's <code>role</code>. Suppose we would like to unreserve
these resources. First, we receive a resource offer (copy/pasted from above):</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;role&quot;: &quot;engineering&quot;,
          &quot;reservation&quot;: {
            &quot;principal&quot;: &lt;framework_principal&gt;
          }
        },
      ]
    }
</code></pre>
<p>We can unreserve the 8 CPUs and 4096 MB of RAM by sending the following
<code>Offer::Operation</code> message. <code>Offer::Operation::Unreserve</code> has a <code>resources</code>
field which we can use to specify the resources to be unreserved.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::UNRESERVE,
      &quot;unreserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;role&quot;: &quot;engineering&quot;,
            &quot;reservation&quot;: {
              &quot;principal&quot;: &lt;framework_principal&gt;
            }
          }
        ]
      }
    }
</code></pre>
<p>The unreserved resources may now be offered to other frameworks.</p>
<p><a name="offer-operation-reserve-reservation-refinement"></a></p>
<h4 id="offeroperationreserve-with-reservation_refinement"><a class="header" href="#offeroperationreserve-with-reservation_refinement"><code>Offer::Operation::Reserve</code> (<strong>with</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework that wants to create a refined reservation needs to enable
the <code>RESERVATION_REFINEMENT</code> capability. Doing so will allow the framework
to use the <code>reservations</code> field in the <code>Resource</code> message in order to
<strong>push</strong> a refined reservation.</p>
<p>Since reserved resources are offered to any of the child roles under the role
for which they are reserved for, they can get <strong>allocated</strong> to say,
<code>&quot;engineering/backend&quot;</code> while being <strong>reserved</strong> for <code>&quot;engineering&quot;</code>.
It can then be refined to be <strong>reserved</strong> for <code>&quot;engineering/backend&quot;</code>.</p>
<p>Note that the refined reservation role must match the offer's allocation role.</p>
<p>Suppose we receive a resource offer with 12 CPUs and 6144 MB of RAM reserved to
<code>&quot;engineering&quot;</code>, allocated to role <code>&quot;engineering/backend&quot;</code>.</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 12 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            }
          ]
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 6144 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            }
          ]
        }
      ]
    }
</code></pre>
<p>Take note of the fact that <code>role</code> and <code>reservation</code> are not set, and that there
is a new field called <code>reservations</code> which represents the reservation state.
With <code>RESERVATION_REFINEMENT</code> enabled, the framework receives resources in this
new format where solely the <code>reservations</code> field is used for the reservation
state, rather than <code>role</code>/<code>reservation</code> pair from pre-<code>RESERVATION_REFINEMENT</code>.</p>
<p>We can reserve 8 CPUs and 4096 MB of RAM to <code>&quot;engineering/backend&quot;</code> by sending
the following <code>Offer::Operation</code> message. <code>Offer::Operation::Reserve</code> has
a <code>resources</code> field which we specify with the resources to be reserved.
We must <strong>push</strong> a new <code>ReservationInfo</code> message onto the back of
the <code>reservations</code> field. We must explicitly set the reservation's' <code>role</code> field
to the offer's allocation role. The optional value of the <code>principal</code> field
depends on whether or not the framework provided a principal when it registered
with the master. If a principal was provided, then the resources' <code>principal</code>
field must be equal to the framework's principal. If no principal was provided
during registration, then the resources' <code>principal</code> field can take any value,
or can be left unset.  Note that the <code>principal</code> field determines
the &quot;reserver principal&quot; when <a href="authorization.html">authorization</a> is enabled, even
if authentication is disabled.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::RESERVE,
      &quot;reserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          }
        ]
      }
    }
</code></pre>
<p>If the reservation is successful, a subsequent resource offer will contain the
following reserved resources:</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
      ]
    }
</code></pre>
<h4 id="offeroperationunreserve-with-reservation_refinement"><a class="header" href="#offeroperationunreserve-with-reservation_refinement"><code>Offer::Operation::Unreserve</code> (<strong>with</strong> <code>RESERVATION_REFINEMENT</code>)</a></h4>
<p>A framework can unreserve resources through the resource offer cycle.
In <a href="reservation.html#offer-operation-reserve-reservation-refinement">Offer::Operation::Reserve</a>,
we reserved 8 CPUs and 4096 MB of RAM on a particular slave for one of our
subscribed roles (i.e. <code>&quot;engineering/backend&quot;</code>), previously reserved for
<code>&quot;engineering&quot;</code>. When we unreserve these resources, they are returned to
<code>&quot;engineering&quot;</code>, by the last <code>ReservationInfo</code> added to
the <code>reservations</code> field being <strong>popped</strong>. First, we receive a resource offer
(copy/pasted from above):</p>
<pre><code>    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
      &quot;id&quot;: &lt;offer_id&gt;,
      &quot;framework_id&quot;: &lt;framework_id&gt;,
      &quot;slave_id&quot;: &lt;slave_id&gt;,
      &quot;hostname&quot;: &lt;hostname&gt;,
      &quot;resources&quot;: [
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
        {
          &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering&quot;,
              &quot;principal&quot;: &lt;principal&gt;,
            },
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;engineering/backend&quot;,
              &quot;principal&quot;: &lt;framework_principal&gt;,
            }
          ]
        },
      ]
    }
</code></pre>
<p>We can unreserve the 8 CPUs and 4096 MB of RAM by sending the following
<code>Offer::Operation</code> message. <code>Offer::Operation::Unreserve</code> has a <code>resources</code>
field which we can use to specify the resources to be unreserved.</p>
<pre><code>    {
      &quot;type&quot;: Offer::Operation::UNRESERVE,
      &quot;unreserve&quot;: {
        &quot;resources&quot;: [
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;cpus&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 8 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          },
          {
            &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering/backend&quot; },
            &quot;name&quot;: &quot;mem&quot;,
            &quot;type&quot;: &quot;SCALAR&quot;,
            &quot;scalar&quot;: { &quot;value&quot;: 4096 },
            &quot;reservations&quot;: [
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering&quot;,
                &quot;principal&quot;: &lt;principal&gt;,
              },
              {
                &quot;type&quot;: &quot;DYNAMIC&quot;,
                &quot;role&quot;: &quot;engineering/backend&quot;,
                &quot;principal&quot;: &lt;framework_principal&gt;,
              }
            ]
          },
        ]
      }
    }
</code></pre>
<p>The resources will now be reserved for <code>&quot;engineering&quot;</code> again, and may now be
offered to <code>&quot;engineering&quot;</code> role itself, or other roles under <code>&quot;engineering&quot;</code>.</p>
<h3 id="operator-http-endpoints"><a class="header" href="#operator-http-endpoints">Operator HTTP Endpoints</a></h3>
<p>As described above, dynamic reservations can be made by a framework scheduler,
typically in response to a resource offer. However, dynamic reservations can
also be created and deleted by sending HTTP requests to the <code>/reserve</code> and
<code>/unreserve</code> endpoints, respectively. This capability is intended for use by
operators and administrative tools.</p>
<h4 id="reserve-since-0250"><a class="header" href="#reserve-since-0250"><code>/reserve</code> (since 0.25.0)</a></h4>
<p>Suppose we want to reserve 8 CPUs and 4096 MB of RAM for the <code>ads</code> role on a
slave with id=<code>&lt;slave_id&gt;</code> (note that it is up to the user to find the ID of the
slave that hosts the desired resources; the request will fail if sufficient
unreserved resources cannot be found on the slave). In this case, the principal
that must be included in the <code>reservation</code> field of the reserved resources
depends on the status of HTTP authentication on the master. If HTTP
authentication is enabled, then the principal in the reservation should match
the authenticated principal provided in the request's HTTP headers. If HTTP
authentication is disabled, then the principal in the reservation can take any
value, or can be left unset. Note that the <code>principal</code> field determines the
&quot;reserver principal&quot; when <a href="authorization.html">authorization</a> is enabled, even if
HTTP authentication is disabled.</p>
<p>We send an HTTP POST request to the master's
<a href="endpoints/master/reserve.html">/reserve</a> endpoint like so:</p>
<pre><code>    $ curl -i \
      -u &lt;operator_principal&gt;:&lt;password&gt; \
      -d slaveId=&lt;slave_id&gt; \
      -d resources='[
        {
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;operator_principal&gt;,
            }
          ]
        },
        {
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;operator_principal&gt;,
            }
          ]
        }
      ]' \
      -X POST http://&lt;ip&gt;:&lt;port&gt;/master/reserve
</code></pre>
<p>The user receives one of the following HTTP responses:</p>
<ul>
<li><code>202 Accepted</code>: Request accepted (see below).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., missing parameters).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
<li><code>409 Conflict</code>: Insufficient resources to satisfy the reserve operation.</li>
</ul>
<p>This endpoint returns the 202 ACCEPTED HTTP status code, which indicates that
the reserve operation has been validated successfully by the master. The
request is then forwarded asynchronously to the Mesos slave where the reserved
resources are located. That asynchronous message may not be delivered or
reserving resources at the slave might fail, in which case no resources will be
reserved. To determine if a reserve operation has succeeded, the user can
examine the state of the appropriate Mesos slave (e.g., via the slave's
<a href="endpoints/slave/state.html">/state</a> HTTP endpoint).</p>
<h4 id="unreserve-since-0250"><a class="header" href="#unreserve-since-0250"><code>/unreserve</code> (since 0.25.0)</a></h4>
<p>Suppose we want to unreserve the resources that we dynamically reserved above.
We can send an HTTP POST request to the master's
<a href="endpoints/master/unreserve.html">/unreserve</a> endpoint like so:</p>
<pre><code>    $ curl -i \
      -u &lt;operator_principal&gt;:&lt;password&gt; \
      -d slaveId=&lt;slave_id&gt; \
      -d resources='[
        {
          &quot;name&quot;: &quot;cpus&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 8 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;reserver_principal&gt;,
            }
          ]
        },
        {
          &quot;name&quot;: &quot;mem&quot;,
          &quot;type&quot;: &quot;SCALAR&quot;,
          &quot;scalar&quot;: { &quot;value&quot;: 4096 },
          &quot;reservations&quot;: [
            {
              &quot;type&quot;: &quot;DYNAMIC&quot;,
              &quot;role&quot;: &quot;ads&quot;,
              &quot;principal&quot;: &lt;reserver_principal&gt;,
            }
          ]
        }
      ]' \
      -X POST http://&lt;ip&gt;:&lt;port&gt;/master/unreserve
</code></pre>
<p>Note that <code>reserver_principal</code> is the principal that was used to make the
reservation, while <code>operator_principal</code> is the principal that is attempting to
perform the unreserve operation---in some cases, these principals might be the
same. The <code>operator_principal</code> must be <a href="authorization.html">authorized</a> to
unreserve reservations made by <code>reserver_principal</code>.</p>
<p>The user receives one of the following HTTP responses:</p>
<ul>
<li><code>202 Accepted</code>: Request accepted (see below).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., missing parameters).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
<li><code>409 Conflict</code>: Insufficient resources to satisfy the unreserve operation.</li>
</ul>
<p>This endpoint returns the 202 ACCEPTED HTTP status code, which indicates that
the unreserve operation has been validated successfully by the master. The
request is then forwarded asynchronously to the Mesos slave where the reserved
resources are located. That asynchronous message may not be delivered or
unreserving resources at the slave might fail, in which case no resources will
be unreserved. To determine if an unreserve operation has succeeded, the user
can examine the state of the appropriate Mesos slave (e.g., via the slave's
<a href="endpoints/slave/state.html">/state</a> HTTP endpoint).</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Shared Persistent Volumes
layout: documentation</h2>
<h1 id="shared-persistent-volumes"><a class="header" href="#shared-persistent-volumes">Shared Persistent Volumes</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>By default, <a href="persistent-volume.html">persistent volumes</a> provide
<em>exclusive</em> access: once a task is launched using a persistent volume,
no other tasks can use that volume, and the volume will not appear in
any resource offers until the task that is using it has finished.</p>
<p>In some cases, it can be useful to share a volume between multiple tasks
running on the same agent. For example, this could be used to
efficiently share a large data set between multiple data analysis tasks.</p>
<h2 id="creating-shared-volumes"><a class="header" href="#creating-shared-volumes">Creating Shared Volumes</a></h2>
<p>Shared persistent volumes are created using the same workflow as normal
persistent volumes: by starting with a
<a href="reservation.html">reserved resource</a> and applying a <code>CREATE</code> operation,
either via the framework scheduler API or the
<a href="endpoints/master/create-volumes.html">/create-volumes</a> HTTP endpoint. To
create a shared volume, set the <code>shared</code> field during volume creation.</p>
<p>For example, suppose a framework subscribed to the <code>&quot;engineering&quot;</code> role
receives a resource offer containing 2048MB of dynamically reserved disk:</p>
<pre><code>{
  &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &quot;engineering&quot;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    }
  ]
}
</code></pre>
<p>The framework can create a shared persistent volume using this disk
resource via the following offer operation:</p>
<pre><code>{
  &quot;type&quot; : Offer::Operation::CREATE,
  &quot;create&quot;: {
    &quot;volumes&quot; : [
      {
        &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
        &quot;name&quot; : &quot;disk&quot;,
        &quot;type&quot; : &quot;SCALAR&quot;,
        &quot;scalar&quot; : { &quot;value&quot; : 2048 },
        &quot;role&quot; : &quot;engineering&quot;,
        &quot;reservation&quot; : {
          &quot;principal&quot; : &lt;framework_principal&gt;
        },
        &quot;disk&quot;: {
          &quot;persistence&quot;: {
            &quot;id&quot; : &lt;persistent_volume_id&gt;
          },
          &quot;volume&quot; : {
            &quot;container_path&quot; : &lt;container_path&gt;,
            &quot;mode&quot; : &lt;mode&gt;
          }
        },
        &quot;shared&quot; : {
        }
      }
    ]
  }
}
</code></pre>
<p>Note that the <code>shared</code> field has been set (to an empty JSON object),
which indicates that the <code>CREATE</code> operation will create a shared volume.</p>
<h2 id="using-shared-volumes"><a class="header" href="#using-shared-volumes">Using Shared Volumes</a></h2>
<p>To be eligible to receive resource offers that contain shared volumes, a
framework must enable the <code>SHARED_RESOURCES</code> capability in the
<code>FrameworkInfo</code> it provides when it registers with the master.
Frameworks that do <em>not</em> enable this capability will not be offered
shared resources.</p>
<p>When a framework receives a resource offer, it can determine whether a
volume is shared by checking if the <code>shared</code> field has been set. Unlike
normal persistent volumes, a shared volume that is in use by a task will
continue to be offered to the frameworks subscribed to the volume's role;
this gives those frameworks the opportunity to launch additional tasks
that can access the volume. A framework can also launch multiple tasks
that access the volume using a single <code>ACCEPT</code> call.</p>
<p>Note that Mesos does not provide any isolation or concurrency control
between the tasks that are sharing a volume. Framework developers should
ensure that tasks that access the same volume do not conflict with one
another. This can be done via careful application-level concurrency
control, or by ensuring that the tasks access the volume in a read-only
manner. Mesos provides support for read-only access to volumes: as
described in the <a href="persistent-volume.html">persistent volume</a>
documentation, tasks that are launched on a volume can specify a <code>mode</code>
of <code>&quot;RO&quot;</code> to use the volume in read-only mode.</p>
<h3 id="destroying-shared-volumes"><a class="header" href="#destroying-shared-volumes">Destroying Shared Volumes</a></h3>
<p>A persistent volume, whether shared or not, can only be destroyed if no
running or pending tasks have been launched using the volume. For
non-shared volumes, it is usually easy to determine when it is safe to
delete a volume. For shared volumes, the framework(s) that have launched
tasks using the volume typically need to coordinate to ensure (e.g., via
reference counting) that a volume is no longer being used before it is
destroyed.</p>
<h3 id="resource-allocation"><a class="header" href="#resource-allocation">Resource Allocation</a></h3>
<p>TODO: how do shared volumes influence resource allocation?</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li>
<p><a href="https://issues.apache.org/jira/browse/MESOS-3421">MESOS-3421</a>
contains additional information about the implementation of this
feature.</p>
</li>
<li>
<p>Talk at MesosCon Europe 2016 on August 31, 2016 entitled
&quot;<a href="http://schd.ws/hosted_files/mesosconeu2016/08/MesosConEurope2016PPVv1.0.pdf">Practical Persistent Volumes</a>&quot;.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Oversubscription
layout: documentation</h2>
<h1 id="oversubscription"><a class="header" href="#oversubscription">Oversubscription</a></h1>
<p>High-priority user-facing services are typically provisioned on large clusters
for peak load and unexpected load spikes. Hence, for most of time, the
provisioned resources remain underutilized. Oversubscription takes advantage of
temporarily unused resources to execute best-effort tasks such as background
analytics, video/image processing, chip simulations, and other low priority
jobs.</p>
<h2 id="how-does-it-work-1"><a class="header" href="#how-does-it-work-1">How does it work?</a></h2>
<p>Oversubscription was introduced in Mesos 0.23.0 and adds two new agent
components: a Resource Estimator and a Quality of Service (QoS) Controller,
alongside extending the existing resource allocator, resource monitor, and
Mesos agent. The new components and their interactions are illustrated below.</p>
<p><img src="images/oversubscription-overview.jpg" alt="Oversubscription overview" /></p>
<h3 id="resource-estimation"><a class="header" href="#resource-estimation">Resource estimation</a></h3>
<ul>
<li>
<p>(1) The first step is to identify the amount of oversubscribed resources.
The resource estimator taps into the resource monitor and periodically gets
usage statistics via <code>ResourceStatistic</code> messages. The resource estimator
applies logic based on the collected resource statistics to determine the
amount of oversubscribed resources. This can be a series of control algorithms
based on measured resource usage slack (allocated but unused resources) and
allocation slack.</p>
</li>
<li>
<p>(2) The agent keeps polling estimates from the resource estimator and tracks
the latest estimate.</p>
</li>
<li>
<p>(3) The agent will send the total amount of oversubscribed resources to the
master when the latest estimate is different from the previous estimate.</p>
</li>
</ul>
<h3 id="resource-tracking--scheduling-algorithm"><a class="header" href="#resource-tracking--scheduling-algorithm">Resource tracking &amp; scheduling algorithm</a></h3>
<ul>
<li>(4) The allocator keeps track of the oversubscribed resources separately
from regular resources and annotate those resources as <code>revocable</code>. It is up
to the resource estimator to determine which types of resources can be
oversubscribed. It is recommended only to oversubscribe <em>compressible</em>
resources such as cpu shares, bandwidth, etc.</li>
</ul>
<h3 id="frameworks-1"><a class="header" href="#frameworks-1">Frameworks</a></h3>
<ul>
<li>(5) Frameworks can choose to launch tasks on revocable resources by using
the regular <code>launchTasks()</code> API. To safe-guard frameworks that are not
designed to deal with preemption, only frameworks registering with the
<code>REVOCABLE_RESOURCES</code> capability set in its framework info will receive offers
with revocable resources.  Further more, revocable resources cannot be
dynamically reserved and persistent volumes should not be created on revocable
disk resources.</li>
</ul>
<h3 id="task-launch"><a class="header" href="#task-launch">Task launch</a></h3>
<ul>
<li>The revocable task is launched as usual when the <code>runTask</code> request is received
on the agent. The resources will still be marked as revocable and isolators
can take appropriate actions, if certain resources need to be setup differently
for revocable and regular tasks.</li>
</ul>
<blockquote>
<p>NOTE: If any resource used by a task or executor is
revocable, the whole container is treated as a revocable container and can
therefore be killed or throttled by the QoS Controller.</p>
</blockquote>
<h3 id="interference-detection"><a class="header" href="#interference-detection">Interference detection</a></h3>
<ul>
<li>(6) When the revocable task is running, it is important to constantly
monitor the original task running on those resources and guarantee
performance based on an SLA.  In order to react to detected interference, the
QoS controller needs to be able to kill or throttle running revocable tasks.</li>
</ul>
<h2 id="enabling-frameworks-to-use-oversubscribed-resources"><a class="header" href="#enabling-frameworks-to-use-oversubscribed-resources">Enabling frameworks to use oversubscribed resources</a></h2>
<p>Frameworks planning to use oversubscribed resources need to register with the
<code>REVOCABLE_RESOURCES</code> capability set:</p>
<pre><code class="language-{.cpp}">FrameworkInfo framework;
framework.set_name(&quot;Revocable framework&quot;);

framework.add_capabilities()-&gt;set_type(
    FrameworkInfo::Capability::REVOCABLE_RESOURCES);
</code></pre>
<p>From that point on, the framework will start to receive revocable resources in
offers.</p>
<blockquote>
<p>NOTE: That there is no guarantee that the Mesos cluster has oversubscription
enabled. If not, no revocable resources will be offered. See below for
instructions how to configure Mesos for oversubscription.</p>
</blockquote>
<h3 id="launching-tasks-using-revocable-resources"><a class="header" href="#launching-tasks-using-revocable-resources">Launching tasks using revocable resources</a></h3>
<p>Launching tasks using revocable resources is done through the existing
<code>launchTasks</code> API. Revocable resources will have the <code>revocable</code> field set. See
below for an example offer with regular and revocable resources.</p>
<pre><code class="language-{.json}">{
  &quot;id&quot;: &quot;20150618-112946-201330860-5050-2210-0000&quot;,
  &quot;framework_id&quot;: &quot;20141119-101031-201330860-5050-3757-0000&quot;,
  &quot;agent_id&quot;: &quot;20150618-112946-201330860-5050-2210-S1&quot;,
  &quot;hostname&quot;: &quot;foobar&quot;,
  &quot;resources&quot;: [
    {
      &quot;name&quot;: &quot;cpus&quot;,
      &quot;type&quot;: &quot;SCALAR&quot;,
      &quot;scalar&quot;: {
        &quot;value&quot;: 2.0
      },
      &quot;role&quot;: &quot;*&quot;
    }, {
      &quot;name&quot;: &quot;mem&quot;,
      &quot;type&quot;: &quot;SCALAR&quot;,
      &quot;scalar&quot;: {
        &quot;value&quot;: 512.0
      },
      &quot;role&quot;: &quot;*&quot;
    },
    {
      &quot;name&quot;: &quot;cpus&quot;,
      &quot;type&quot;: &quot;SCALAR&quot;,
      &quot;scalar&quot;: {
        &quot;value&quot;: 0.45
      },
      &quot;role&quot;: &quot;*&quot;,
      &quot;revocable&quot;: {}
    }
  ]
}
</code></pre>
<h2 id="writing-a-custom-resource-estimator"><a class="header" href="#writing-a-custom-resource-estimator">Writing a custom resource estimator</a></h2>
<p>The resource estimator estimates and predicts the total resources used on the
agent and informs the master about resources that can be oversubscribed. By
default, Mesos comes with a <code>noop</code> and a <code>fixed</code> resource estimator. The <code>noop</code>
estimator only provides an empty estimate to the agent and stalls, effectively
disabling oversubscription. The <code>fixed</code> estimator doesn't use the actual
measured slack, but oversubscribes the node with fixed resource amount (defined
via a command line flag).</p>
<p>The interface is defined below:</p>
<pre><code class="language-{.cpp}">class ResourceEstimator
{
public:
  // Initializes this resource estimator. This method needs to be
  // called before any other member method is called. It registers
  // a callback in the resource estimator. The callback allows the
  // resource estimator to fetch the current resource usage for each
  // executor on agent.
  virtual Try&lt;Nothing&gt; initialize(
      const lambda::function&lt;process::Future&lt;ResourceUsage&gt;()&gt;&amp; usage) = 0;

  // Returns the current estimation about the *maximum* amount of
  // resources that can be oversubscribed on the agent. A new
  // estimation will invalidate all the previously returned
  // estimations. The agent will be calling this method periodically
  // to forward it to the master. As a result, the estimator should
  // respond with an estimate every time this method is called.
  virtual process::Future&lt;Resources&gt; oversubscribable() = 0;
};
</code></pre>
<h2 id="writing-a-custom-qos-controller"><a class="header" href="#writing-a-custom-qos-controller">Writing a custom QoS controller</a></h2>
<p>The interface for implementing custom QoS Controllers is defined below:</p>
<pre><code class="language-{.cpp}">class QoSController
{
public:
  // Initializes this QoS Controller. This method needs to be
  // called before any other member method is called. It registers
  // a callback in the QoS Controller. The callback allows the
  // QoS Controller to fetch the current resource usage for each
  // executor on agent.
  virtual Try&lt;Nothing&gt; initialize(
      const lambda::function&lt;process::Future&lt;ResourceUsage&gt;()&gt;&amp; usage) = 0;

  // A QoS Controller informs the agent about corrections to carry
  // out, but returning futures to QoSCorrection objects. For more
  // information, please refer to mesos.proto.
  virtual process::Future&lt;std::list&lt;QoSCorrection&gt;&gt; corrections() = 0;
};
</code></pre>
<blockquote>
<p>NOTE The QoS Controller must not block <code>corrections()</code>. Back the QoS
Controller with its own libprocess actor instead.</p>
</blockquote>
<p>The QoS Controller informs the agent that particular corrective actions need to
be made. Each corrective action contains information about executor or task and
the type of action to perform.</p>
<p>Mesos comes with a <code>noop</code> and a <code>load</code> qos controller. The <code>noop</code> controller
does not provide any corrections, thus does not assure any quality of service
for regular tasks. The <code>load</code> controller is ensuring the total system load
doesn't exceed a configurable thresholds and as a result try to avoid the cpu
congestion on the node. If the load is above the thresholds controller evicts
all the revocable executors. These thresholds are configurable via two module
parameters <code>load_threshold_5min</code> and <code>load_threshold_15min</code>. They represent
standard unix load averages in the system. 1 minute system load is ignored,
since for oversubscription use case it can be a misleading signal.</p>
<pre><code class="language-{.proto}">message QoSCorrection {
  enum Type {
    KILL = 1; // Terminate an executor.
  }

  message Kill {
    optional FrameworkID framework_id = 1;
    optional ExecutorID executor_id = 2;
  }

  required Type type = 1;
  optional Kill kill = 2;
}
</code></pre>
<h2 id="configuring-oversubscription"><a class="header" href="#configuring-oversubscription">Configuring oversubscription</a></h2>
<p>Five new flags has been added to the agent:</p>
<table class="table table-striped">
  <thead>
    <tr>
      <th width="30%">
        Flag
      </th>
      <th>
        Explanation
      </th>
  </thead>
<tr>
    <td>
      --oversubscribed_resources_interval=VALUE
    </td>
    <td>
      The agent periodically updates the master with the current estimation
about the total amount of oversubscribed resources that are allocated and
available. The interval between updates is controlled by this flag. (default:
15secs)
    </td>
  </tr>
<tr>
    <td>
      --qos_controller=VALUE
    </td>
    <td>
      The name of the QoS Controller to use for oversubscription.
    </td>
  </tr>
<tr>
    <td>
      --qos_correction_interval_min=VALUE
    </td>
    <td>
      The agent polls and carries out QoS corrections from the QoS Controller
based on its observed performance of running tasks. The smallest interval
between these corrections is controlled by this flag. (default: 0ns)
    </td>
  </tr>
<tr>
    <td>
      --resource_estimator=VALUE
    </td>
    <td>
      The name of the resource estimator to use for oversubscription.
    </td>
  </tr>
</table>
<p>The <code>fixed</code> resource estimator is enabled as follows:</p>
<pre><code>--resource_estimator=&quot;org_apache_mesos_FixedResourceEstimator&quot;

--modules='{
  &quot;libraries&quot;: {
    &quot;file&quot;: &quot;/usr/local/lib64/libfixed_resource_estimator.so&quot;,
    &quot;modules&quot;: {
      &quot;name&quot;: &quot;org_apache_mesos_FixedResourceEstimator&quot;,
      &quot;parameters&quot;: {
        &quot;key&quot;: &quot;resources&quot;,
        &quot;value&quot;: &quot;cpus:14&quot;
      }
    }
  }
}'
</code></pre>
<p>In the example above, a fixed amount of 14 cpus will be offered as revocable
resources.</p>
<p>The <code>load</code> qos controller is enabled as follows:</p>
<pre><code>--qos_controller=&quot;org_apache_mesos_LoadQoSController&quot;

--qos_correction_interval_min=&quot;20secs&quot;

--modules='{
  &quot;libraries&quot;: {
    &quot;file&quot;: &quot;/usr/local/lib64/libload_qos_controller.so&quot;,
    &quot;modules&quot;: {
      &quot;name&quot;: &quot;org_apache_mesos_LoadQoSController&quot;,
      &quot;parameters&quot;: [
        {
          &quot;key&quot;: &quot;load_threshold_5min&quot;,
          &quot;value&quot;: &quot;6&quot;
        },
        {
	  &quot;key&quot;: &quot;load_threshold_15min&quot;,
	  &quot;value&quot;: &quot;4&quot;
        }
      ]
    }
  }
}'
</code></pre>
<p>In the example above, when standard unix system load average for 5 minutes will
be above 6, or for 15 minutes will be above 4 then agent will evict all the
<code>revocable</code> executors. <code>LoadQoSController</code> will be effectively run every 20
seconds.</p>
<p>To install a custom resource estimator and QoS controller, please refer to the
<a href="modules.html">modules documentation</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Authentication
layout: documentation</h2>
<h1 id="authentication"><a class="header" href="#authentication">Authentication</a></h1>
<p>Authentication permits only trusted entities to interact with a Mesos cluster. Authentication can be used by Mesos in three ways:</p>
<ol>
<li>To require that frameworks be authenticated in order to register with the master.</li>
<li>To require that agents be authenticated in order to register with the master.</li>
<li>To require that operators be authenticated to use many <a href="endpoints/index.html">HTTP endpoints</a>.</li>
</ol>
<p>Authentication is disabled by default. When authentication is enabled, operators
can configure Mesos to either use the default authentication module or to use a
<em>custom</em> authentication module.</p>
<p>The default Mesos authentication module uses the
<a href="http://asg.web.cmu.edu/sasl/">Cyrus SASL</a> library.  SASL is a flexible
framework that allows two endpoints to authenticate with each other using a
variety of methods. By default, Mesos uses
<a href="https://en.wikipedia.org/wiki/CRAM-MD5">CRAM-MD5</a> authentication.</p>
<h2 id="credentials-principals-and-secrets"><a class="header" href="#credentials-principals-and-secrets">Credentials, Principals, and Secrets</a></h2>
<p>When using the default CRAM-MD5 authentication method, an entity that wants to
authenticate with Mesos must provide a <em>credential</em>, which consists of a
<em>principal</em> and a <em>secret</em>. The principal is the identity that the entity would
like to use; the secret is an arbitrary string that is used to verify that
identity. Principals are similar to user names, while secrets are similar to
passwords.</p>
<p>Principals are used primarily for authentication and
<a href="authorization.html">authorization</a>; note that a principal is different from a
framework's <em>user</em>, which is the operating system account used by the agent to
run executors, and the framework's <em><a href="roles.html">roles</a></em>, which are used to
determine which resources a framework can use.</p>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>Authentication is configured by specifying command-line flags when starting the
Mesos master and agent processes. For more information, refer to the
<a href="configuration.html">configuration</a> documentation.</p>
<h3 id="master-1"><a class="header" href="#master-1">Master</a></h3>
<ul>
<li>
<p><code>--[no-]authenticate</code> - If <code>true</code>, only authenticated frameworks are allowed
to register. If <code>false</code> (the default), unauthenticated frameworks are also
allowed to register.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readonly</code> - If <code>true</code>, authentication is required to
make HTTP requests to the read-only HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-only endpoints are those which cannot be used to modify
the state of the cluster.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readwrite</code> - If <code>true</code>, authentication is required
to make HTTP requests to the read-write HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-write endpoints are those which can be used to modify the
state of the cluster.</p>
</li>
<li>
<p><code>--[no-]authenticate_agents</code> - If <code>true</code>, only authenticated agents are
allowed to register. If <code>false</code> (the default), unauthenticated agents are also
allowed to register.</p>
</li>
<li>
<p><code>--authentication_v0_timeout</code> - The timeout within which an authentication is
expected to complete against a v0 framework or agent. This does not apply to
the v0 or v1 HTTP APIs.(default: <code>15secs</code>)</p>
</li>
<li>
<p><code>--authenticators</code> - Specifies which authenticator module to use.  The default
is <code>crammd5</code>, but additional modules can be added using the <code>--modules</code>
option.</p>
</li>
<li>
<p><code>--http_authenticators</code> - Specifies which HTTP authenticator module to use.
The default is <code>basic</code> (basic HTTP authentication), but additional modules can
be added using the <code>--modules</code> option.</p>
</li>
<li>
<p><code>--credentials</code> - The path to a text file which contains a list of accepted
credentials.  This may be optional depending on the authenticator being used.</p>
</li>
</ul>
<h3 id="agent-1"><a class="header" href="#agent-1">Agent</a></h3>
<ul>
<li>
<p><code>--authenticatee</code> - Analog to the master's <code>--authenticators</code> option to
specify what module to use.  Defaults to <code>crammd5</code>.</p>
</li>
<li>
<p><code>--credential</code> - Just like the master's <code>--credentials</code> option except that
only one credential is allowed. This credential is used to identify the agent
to the master.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readonly</code> - If <code>true</code>, authentication is required to
make HTTP requests to the read-only HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-only endpoints are those which cannot be used to modify
the state of the agent.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_readwrite</code> - If <code>true</code>, authentication is required
to make HTTP requests to the read-write HTTP endpoints that support
authentication. If <code>false</code> (the default), these endpoints can be used without
authentication. Read-write endpoints are those which can be used to modify the
state of the agent. Note that for backward compatibility reasons, the V1
executor API is not affected by this flag.</p>
</li>
<li>
<p><code>--[no-]authenticate_http_executors</code> - If <code>true</code>, authentication is required
to make HTTP requests to the V1 executor API. If <code>false</code> (the default), that
API can be used without authentication. If this flag is <code>true</code> and custom
HTTP authenticators are not specified, then the default <code>JWT</code> authenticator is
loaded to handle executor authentication.</p>
</li>
<li>
<p><code>--http_authenticators</code> - Specifies which HTTP authenticator module to use.
The default is <code>basic</code>, but additional modules can be added using the
<code>--modules</code> option.</p>
</li>
<li>
<p><code>--http_credentials</code> - The path to a text file which contains a list (in JSON
format) of accepted credentials.  This may be optional depending on the
authenticator being used.</p>
</li>
<li>
<p><code>--authentication_backoff_factor</code> - The agent will time out its authentication
with the master based on exponential backoff. The timeout will be randomly
chosen within the range <code>[min, min + factor*2^n]</code> where <code>n</code> is the number of
failed attempts. To tune these parameters, set the
<code>--authentication_timeout_[min|max|factor]</code> flags. (default: 1secs)</p>
</li>
<li>
<p><code>--authentication_timeout_min</code> - The minimum amount of time the agent waits
before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 5secs)</p>
</li>
<li>
<p><code>--authentication_timeout_max</code> - The maximum amount of time the agent waits
before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 1mins)</p>
</li>
</ul>
<h3 id="scheduler-driver"><a class="header" href="#scheduler-driver">Scheduler Driver</a></h3>
<ul>
<li>
<p><code>--authenticatee</code> - Analog to the master's <code>--authenticators</code> option to
specify what module to use.  Defaults to <code>crammd5</code>.</p>
</li>
<li>
<p><code>--authentication_backoff_factor</code> - The scheduler will time out its
authentication with the master based on exponential backoff. The timeout will
be randomly chosen within the range <code>[min, min + factor*2^n]</code> where <code>n</code> is
the number of failed attempts. To tune these parameters, set the
<code>--authentication_timeout_[min|max|factor]</code> flags. (default: 1secs)</p>
</li>
<li>
<p><code>--authentication_timeout_min</code> - The minimum amount of time the scheduler
waits before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 5secs)</p>
</li>
<li>
<p><code>--authentication_timeout_max</code> - The maximum amount of time the scheduler
waits before retrying authenticating with the master. See
<code>--authentication_backoff_factor</code> for more details. (default: 1mins)</p>
</li>
</ul>
<h3 id="multiple-http-authenticators"><a class="header" href="#multiple-http-authenticators">Multiple HTTP Authenticators</a></h3>
<p>Multiple HTTP authenticators may be loaded into the Mesos master and agent. In
order to load multiple authenticators, specify them as a comma-separated list
using the <code>--http_authenticators</code> flag. The authenticators will be called
serially, and the result of the first successful authentication attempt will be
returned.</p>
<p>If you wish to specify the default basic HTTP authenticator in addition to
custom authenticator modules, add the name <code>basic</code> to your authenticator list.
To specify the default JWT HTTP authenticator in addition to custom
authenticator modules, add the name <code>jwt</code> to your authenticator list.</p>
<h3 id="executor"><a class="header" href="#executor">Executor</a></h3>
<p>If HTTP executor authentication is enabled on the agent, then all requests from
HTTP executors must be authenticated. This includes the default executor, HTTP
command executors, and custom HTTP executors. By default, the agent's JSON web
token (JWT) HTTP authenticator is loaded to handle executor authentication on
both the executor and operator API endpoints. Note that command and custom
executors not using the HTTP API will remain unauthenticated.</p>
<p>When a secret key is loaded via the <code>--jwt_secret_key</code> flag, the agent will
generate a default JWT for each executor before it is launched. This token is
passed into the executor's environment via the
<code>MESOS_EXECUTOR_AUTHENTICATION_TOKEN</code> environment variable. In order to
authenticate with the agent, the executor should place this token into the
<code>Authorization</code> header of all its requests as follows:</p>
<pre><code>    Authorization: Bearer MESOS_EXECUTOR_AUTHENTICATION_TOKEN
</code></pre>
<p>In order to upgrade an existing cluster to require executor authentication, the
following procedure should be followed:</p>
<ol>
<li>
<p>Upgrade all agents, and provide each agent with a cryptographic key via the
<code>--jwt_secret_key</code> flag. This key will be used to sign executor
authentication tokens using the HMAC-SHA256 procedure.</p>
</li>
<li>
<p>Before executor authentication can be enabled successfully, all HTTP
executors must have executor authentication tokens in their environment and
support authentication. To accomplish this, executors which were already
running before the upgrade must be restarted. This could either be done all
at once, or the cluster may be left in this intermediate state while
executors gradually turn over.</p>
</li>
<li>
<p>Once all running default/HTTP command executors have been launched by
upgraded agents, and any custom HTTP executors have been upgraded, the agent
processes can be restarted with the <code>--authenticate_http_executors</code> flag set.
This will enable required HTTP executor authentication, and since all
executors now have authentication tokens and support authentication, their
requests to the agent will authenticate successfully.</p>
</li>
</ol>
<p>Note that HTTP executors make use of the agent operator API in order to make
nested container calls. This means that authentication of the v1 agent operator
API should not be enabled (via <code>--authenticate_http_readwrite</code>) when HTTP
executor authentication is disabled, or HTTP executors will not be able to
function correctly.</p>
<h3 id="framework"><a class="header" href="#framework">Framework</a></h3>
<p>If framework authentication is enabled, each framework must be configured to
supply authentication credentials when registering with the Mesos master. How to
configure this differs between frameworks; consult your framework's
documentation for more information.</p>
<p>As a framework developer, supporting authentication is straightforward: the
scheduler driver handles the details of authentication when a <code>Credential</code>
object is passed to its constructor. To enable <a href="authorization.html">authorization</a>
based on the authenticated principal, the framework developer should also copy
the <code>Credential.principal</code> into <code>FrameworkInfo.principal</code> when registering.</p>
<h2 id="cram-md5-example"><a class="header" href="#cram-md5-example">CRAM-MD5 Example</a></h2>
<ol>
<li>
<p>Create the master's credentials file with the following content:</p>
<pre><code> {
   &quot;credentials&quot; : [
     {
       &quot;principal&quot;: &quot;principal1&quot;,
       &quot;secret&quot;: &quot;secret1&quot;
     },
     {
       &quot;principal&quot;: &quot;principal2&quot;,
       &quot;secret&quot;: &quot;secret2&quot;
     }
   ]
 }
</code></pre>
</li>
<li>
<p>Start the master using the credentials file (assuming the file is <code>/home/user/credentials</code>):</p>
<pre><code> ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos --authenticate --authenticate_agents --credentials=/home/user/credentials
</code></pre>
</li>
<li>
<p>Create another file with a single credential in it (<code>/home/user/agent_credential</code>):</p>
<pre><code> {
   &quot;principal&quot;: &quot;principal1&quot;,
   &quot;secret&quot;: &quot;secret1&quot;
 }
</code></pre>
</li>
<li>
<p>Start the agent:</p>
<pre><code> ./bin/mesos-agent.sh --master=127.0.0.1:5050 --credential=/home/user/agent_credential
</code></pre>
</li>
<li>
<p>Your new agent should have now successfully authenticated with the master.</p>
</li>
<li>
<p>You can test out framework authentication using one of the test frameworks
provided with Mesos as follows:</p>
<pre><code> MESOS_AUTHENTICATE=true DEFAULT_PRINCIPAL=principal2 DEFAULT_SECRET=secret2 ./src/test-framework --master=127.0.0.1:5050
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Authorization
layout: documentation</h2>
<h1 id="authorization-1"><a class="header" href="#authorization-1">Authorization</a></h1>
<p>In Mesos, the authorization subsystem allows the operator to configure the
actions that certain principals are allowed to perform. For example, the
operator can use authorization to ensure that principal <code>foo</code> can only register
frameworks subscribed to role <code>bar</code>, and no other principals can register
frameworks subscribed to any roles.</p>
<p>A reference implementation <em>local authorizer</em> provides basic security for most
use cases. This authorizer is configured using Access Control Lists (ACLs).
Alternative implementations could express their authorization rules in
different ways. The local authorizer is used if the
<a href="configuration/master.html"><code>--authorizers</code></a> flag is not specified (or manually set to
the default value <code>local</code>) and ACLs are specified via the
<a href="configuration.html"><code>--acls</code></a> flag.</p>
<p>This document is divided into two main sections. The first section explores the
concepts necessary to successfully configure the local authorizer. The second
briefly discusses how to implement a custom authorizer; this section is not
directed at operators but at engineers who wish to build their own authorizer
back end.</p>
<h2 id="http-executor-authorization"><a class="header" href="#http-executor-authorization">HTTP Executor Authorization</a></h2>
<p>When the agent's <code>--authenticate_http_executors</code> flag is set, HTTP executors are
required to authenticate with the HTTP executor API. When they do so, a simple
implicit authorization rule is applied. In plain language, the rule states that
executors can only perform actions on themselves. More specifically, an
executor's authenticated principal must contain claims with keys <code>fid</code>, <code>eid</code>,
and <code>cid</code>, with values equal to the currently-running executor's framework ID,
executor ID, and container ID, respectively. By default, an authentication token
containing these claims is injected into the executor's environment (see the
<a href="authentication.html">authentication documentation</a> for more information).</p>
<p>Similarly, when the agent's <code>--authenticate_http_readwrite</code> flag is set, HTTP
executor's are required to authenticate with the HTTP operator API when making
calls such as <code>LAUNCH_NESTED_CONTAINER</code>. In this case, executor authorization is
performed via the loaded authorizer module, if present. The default Mesos local
authorizer applies a simple implicit authorization rule, requiring that the
executor's principal contain a claim with key <code>cid</code> and a value equal to the
currently-running executor's container ID.</p>
<h2 id="local-authorizer"><a class="header" href="#local-authorizer">Local Authorizer</a></h2>
<h3 id="role-vs-principal-1"><a class="header" href="#role-vs-principal-1">Role vs. Principal</a></h3>
<p>A principal identifies an entity (i.e., a framework or an operator) that
interacts with Mesos. A role, on the other hand, is used to associate resources
with frameworks in various ways. A useful analogy can be made with user
management in the Unix world: principals correspond to usernames, while roles
approximately correspond to groups. For more information about roles, see the
<a href="roles.html">roles documentation</a>.</p>
<p>In a real-world organization, principals and roles might be used to represent
various individuals or groups; for example, principals could correspond to
people responsible for particular frameworks, while roles could correspond to
departments within the organization which run frameworks on the cluster. To
illustrate this point, consider a company that wants to allocate datacenter
resources amongst multiple departments, one of which is the accounting
department. Here is a possible scenario in which the accounting department
launches a Mesos framework and then attempts to destroy a persistent volume:</p>
<ul>
<li>An accountant launches their framework, which authenticates with the Mesos
master using its <code>principal</code> and <code>secret</code>. Here, let the framework principal
be <code>payroll-framework</code>; this principal represents the trusted identity of the
framework.</li>
<li>The framework now sends a registration message to the master. This message
includes a <code>FrameworkInfo</code> object containing a <code>principal</code> and <code>roles</code>; in
this case, it will use a single role named <code>accounting</code>. The principal in
this message must be <code>payroll-framework</code>, to match the one used by the
framework for authentication.</li>
<li>The master consults the local authorizer, which in turn looks through its ACLs
to see if it has a <code>RegisterFramework</code> ACL which authorizes the principal
<code>payroll-framework</code> to register with the <code>accounting</code> role. It does find such
an ACL, the framework registers successfully. Now that the framework is
subscribed to the <code>accounting</code> role, any <a href="weights.html">weights</a>,
<a href="reservation.html">reservations</a>, <a href="persistent-volume.html">persistent volumes</a>,
or <a href="quota.html">quota</a> associated with the accounting department's role will
apply when allocating resources to this role within the framework. This
allows operators to control the resource consumption of this department.</li>
<li>Suppose the framework has created a persistent volume on an agent which it
now wishes to destroy. The framework sends an <code>ACCEPT</code> call containing an
offer operation which will <code>DESTROY</code> the persistent volume.</li>
<li>However, datacenter operators have decided that they don't want the accounting
frameworks to delete volumes. Rather, the operators will manually remove the
accounting department's persistent volumes to ensure that no important
financial data is deleted accidentally. To accomplish this, they have set a
<code>DestroyVolume</code> ACL which asserts that the principal <code>payroll-framework</code> can
destroy volumes created by a <code>creator_principal</code> of <code>NONE</code>; in other words,
this framework cannot destroy persistent volumes, so the operation will be
refused.</li>
</ul>
<h3 id="acls"><a class="header" href="#acls">ACLs</a></h3>
<p>When authorizing an action, the local authorizer proceeds through a list of
relevant rules until it finds one that can either grant or deny permission to
the subject making the request. These rules are configured with Access Control
Lists (ACLs) in the case of the local authorizer. The ACLs are defined with a
JSON-based language via the <a href="configuration.html"><code>--acls</code></a> flag.</p>
<p>Each ACL consist of an array of JSON objects. Each of these objects has two
entries. The first, <code>principals</code>, is common to all actions and describes the
subjects which wish to perform the given action. The second entry varies among
actions and describes the object on which the action will be executed. Both
entries are specified with the same type of JSON object, known as <code>Entity</code>. The
local authorizer works by comparing <code>Entity</code> objects, so understanding them is
key to writing good ACLs.</p>
<p>An <code>Entity</code> is essentially a container which can either hold a particular value
or specify the special types <code>ANY</code> or <code>NONE</code>.</p>
<p>A global field which affects all ACLs can be set. This field is called
<code>permissive</code> and it defines the behavior when no ACL applies to the request
made. If set to <code>true</code> (which is the default) it will allow by default all
non-matching requests, if set to <code>false</code> it will reject all non-matching
requests.</p>
<p>Note that when setting <code>permissive</code> to <code>false</code> a number of standard operations
(e.g., <code>run_tasks</code> or <code>register_frameworks</code>) will require ACLs in order to work.
There are two ways to disallow unauthorized uses on specific operations:</p>
<ol>
<li>
<p>Leave <code>permissive</code> set to <code>true</code> and disallow <code>ANY</code> principal to perform
actions to all objects except the ones explicitly allowed.
Consider the <a href="authorization.html#disallowExample">example below</a> for details.</p>
</li>
<li>
<p>Set <code>permissive</code> to <code>false</code> but allow <code>ANY</code> principal to perform the
action on <code>ANY</code> object. This needs to be done for all actions which should
work without being checked against ACLs. A template doing this for all
actions can be found in <a href="../examples/acls_template.json">acls_template.json</a>.</p>
</li>
</ol>
<p>More information about the structure of the ACLs can be found in
<a href="https://github.com/apache/mesos/blob/master/include/mesos/authorizer/acls.proto">their definition</a>
inside the Mesos source code.</p>
<p>ACLs are compared in the order that they are specified. In other words,
if an ACL allows some action and a later ACL forbids it, the action is
allowed; likewise, if the ACL forbidding the action appears earlier than the
one allowing the action, the action is forbidden. If no ACLs match a request,
the request is authorized if the ACLs are permissive (which is the default
behavior). If <code>permissive</code> is explicitly set to false, all non-matching requests
are declined.</p>
<h3 id="authorizable-actions"><a class="header" href="#authorizable-actions">Authorizable Actions</a></h3>
<p>Currently, the local authorizer configuration format supports the following
entries, each representing an authorizable action:</p>
<table class="table table-striped">
<thead>
<tr>
  <th>Action Name</th>
  <th>Subject</th>
  <th>Object</th>
  <th>Description</th>
</tr>
</thead>
<tbody>
<tr>
  <td><code>register_frameworks</code></td>
  <td>Framework principal.</td>
  <td>Resource <a href="roles.html">roles</a> of
      the framework.
  </td>
  <td>(Re-)registering of frameworks.</td>
</tr>
<tr>
  <td><code>run_tasks</code></td>
  <td>Framework principal.</td>
  <td>UNIX user to launch the task as.</td>
  <td>Launching tasks/executors by a framework.</td>
</tr>
<tr>
  <td><code>teardown_frameworks</code></td>
  <td>Operator username.</td>
  <td>Principals whose frameworks can be shutdown by the operator.</td>
  <td>Tearing down frameworks.</td>
</tr>
<tr>
  <td><code>reserve_resources</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Resource role of the reservation.</td>
  <td><a href="reservation.html">Reserving</a> resources.</td>
</tr>
<tr>
  <td><code>unreserve_resources</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Principals whose resources can be unreserved by the operator.</td>
  <td><a href="reservation.html">Unreserving</a> resources.</td>
</tr>
<tr>
  <td><code>create_volumes</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Resource role of the volume.</td>
  <td>Creating
      <a href="persistent-volume.html">volumes</a>.
  </td>
</tr>
<tr>
  <td><code>destroy_volumes</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Principals whose volumes can be destroyed by the operator.</td>
  <td>Destroying
      <a href="persistent-volume.html">volumes</a>.
  </td>
</tr>
<tr>
  <td><code>resize_volume</code></td>
  <td>Framework principal or Operator username.</td>
  <td>Resource role of the volume.</td>
  <td>Growing or shrinking
      <a href="persistent-volume.html">persistent volumes</a>.
  </td>
</tr>
<tr>
  <td><code>create_block_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the block disk.</td>
  <td>Creating a block disk.</td>
</tr>
<tr>
  <td><code>destroy_block_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the block disk.</td>
  <td>Destroying a block disk.</td>
</tr>
<tr>
  <td><code>create_mount_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the mount disk.</td>
  <td>Creating a mount disk.</td>
</tr>
<tr>
  <td><code>destroy_mount_disks</code></td>
  <td>Framework principal.</td>
  <td>Resource role of the mount disk.</td>
  <td>Destroying a mount disk.</td>
</tr>
<tr>
  <td><code>get_quotas</code></td>
  <td>Operator username.</td>
  <td>Resource role whose quota status will be queried.</td>
  <td>Querying <a href="quota.html">quota</a> status.</td>
</tr>
<tr>
  <td><code>update_quotas</code></td>
  <td>Operator username.</td>
  <td>Resource role whose quota will be updated.</td>
  <td>Modifying <a href="quota.html">quotas</a>.</td>
</tr>
<tr>
  <td><code>view_roles</code></td>
  <td>Operator username.</td>
  <td>Resource roles whose information can be viewed by the operator.</td>
  <td>Querying <a href="roles.html">roles</a>
      and <a href="weights.html">weights</a>.
  </td>
</tr>
<tr>
  <td><code>get_endpoints</code></td>
  <td>HTTP username.</td>
  <td>HTTP endpoints the user should be able to access using the HTTP "GET"
      method.</td>
  <td>Performing an HTTP "GET" on an endpoint.</td>
</tr>
<tr>
  <td><code>update_weights</code></td>
  <td>Operator username.</td>
  <td>Resource roles whose weights can be updated by the operator.</td>
  <td>Updating <a href="weights.html">weights</a>.</td>
</tr>
<tr>
  <td><code>view_frameworks</code></td>
  <td>HTTP user.</td>
  <td>UNIX user of whom executors can be viewed.</td>
  <td>Filtering http endpoints.</td>
</tr>
<tr>
  <td><code>view_executors</code></td>
  <td>HTTP user.</td>
  <td>UNIX user of whom executors can be viewed.</td>
  <td>Filtering http endpoints.</td>
</tr>
<tr>
  <td><code>view_tasks</code></td>
  <td>HTTP user.</td>
  <td>UNIX user of whom executors can be viewed.</td>
  <td>Filtering http endpoints.</td>
</tr>
<tr>
  <td><code>access_sandboxes</code></td>
  <td>Operator username.</td>
  <td>Operating system user whose executor/task sandboxes can be accessed.</td>
  <td>Access task sandboxes.</td>
</tr>
<tr>
  <td><code>access_mesos_logs</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>Access Mesos logs.</td>
</tr>
<tr>
  <td><code>register_agents</code></td>
  <td>Agent principal.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      agent (re-)registration.
  </td>
  <td>(Re-)registration of agents.</td>
</tr>
<tr>
  <td><code>get_maintenance_schedules</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>View the maintenance schedule of the machines used by Mesos.</td>
</tr>
<tr>
  <td><code>update_maintenance_schedules</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>Modify the maintenance schedule of the machines used by Mesos.</td>
</tr>
<tr>
  <td><code>start_maintenances</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use types ANY and NONE to allow/deny
      access to the log.
  </td>
  <td>Starts maintenance on a machine. This will make a machine and its agents
      unavailable.
  </td>
</tr>
<tr>
  <td><code>stop_maintenances</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use the types ANY and NONE to
      allow/deny access to the log.
  </td>
  <td>Ends maintenance on a machine.</td>
</tr>
<tr>
  <td><code>get_maintenance_statuses</code></td>
  <td>Operator username.</td>
  <td>Implicitly given. A user should only use the types ANY and NONE to
      allow/deny access to the log.
  </td>
  <td>View if a machine is in maintenance or not.</td>
</tr>
</tbody>
</table>
<h3 id="authorizable-http-endpoints"><a class="header" href="#authorizable-http-endpoints">Authorizable HTTP endpoints</a></h3>
<p>The <code>get_endpoints</code> action covers:</p>
<ul>
<li><code>/files/debug</code></li>
<li><code>/logging/toggle</code></li>
<li><code>/metrics/snapshot</code></li>
<li><code>/slave(id)/containers</code></li>
<li><code>/slave(id)/containerizer/debug</code></li>
<li><code>/slave(id)/monitor/statistics</code></li>
</ul>
<h3 id="examples-4"><a class="header" href="#examples-4">Examples</a></h3>
<p>Consider for example the following ACL: Only principal <code>foo</code> can register
frameworks subscribed to the <code>analytics</code> role. All principals can register
frameworks subscribing to any other roles (including the principal <code>foo</code>
since permissive is the default behavior).</p>
<pre><code class="language-json">{
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;type&quot;: &quot;NONE&quot;
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>Principal <code>foo</code> can register frameworks subscribed to the <code>analytics</code> and
<code>ads</code> roles and no other role. Any other principal (or framework without
a principal) can register frameworks subscribed to any roles.</p>
<pre><code class="language-json">{
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;, &quot;ads&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;type&quot;: &quot;NONE&quot;
                             }
                           }
                         ]
}
</code></pre>
<p>Only principal <code>foo</code> and no one else can register frameworks subscribed to the
<code>analytics</code> role. Any other principal (or framework without a principal) can
register frameworks subscribed to any other roles.</p>
<pre><code class="language-json">{
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;type&quot;: &quot;NONE&quot;
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>Principal <code>foo</code> can register frameworks subscribed to the <code>analytics</code> role
and no other roles. No other principal can register frameworks subscribed to
any roles, including <code>*</code>.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;register_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;roles&quot;: {
                               &quot;values&quot;: [&quot;analytics&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>In the following example <code>permissive</code> is set to <code>false</code>; hence, principals can
only run tasks as operating system users <code>guest</code> or <code>bar</code>, but not as any other
user.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;type&quot;: &quot;ANY&quot; },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;guest&quot;, &quot;bar&quot;] }
                 }
               ]
}
</code></pre>
<p>Principals <code>foo</code> and <code>bar</code> can run tasks as the agent operating system user
<code>alice</code> and no other user. No other principal can run tasks.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;values&quot;: [&quot;foo&quot;, &quot;bar&quot;] },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;alice&quot;] }
                 }
               ]
}
</code></pre>
<p>Principal <code>foo</code> can run tasks only as the agent operating system user <code>guest</code>
and no other user. Any other principal (or framework without a principal) can
run tasks as any user.</p>
<pre><code class="language-json">{
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;values&quot;: [&quot;foo&quot;] },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;guest&quot;] }
                 },
                 {
                   &quot;principals&quot;: { &quot;values&quot;: [&quot;foo&quot;] },
                   &quot;users&quot;: { &quot;type&quot;: &quot;NONE&quot; }
                 }
               ]
}
</code></pre>
<p>No principal can run tasks as the agent operating system user <code>root</code>. Any
principal (or framework without a principal) can run tasks as any other user.</p>
<pre><code class="language-json">{
  &quot;run_tasks&quot;: [
                 {
                   &quot;principals&quot;: { &quot;type&quot;: &quot;NONE&quot; },
                   &quot;users&quot;: { &quot;values&quot;: [&quot;root&quot;] }
                 }
               ]
}
</code></pre>
<p>The order in which the rules are defined is important. In the following
example, the ACLs effectively forbid anyone from tearing down frameworks even
though the intention clearly is to allow only <code>admin</code> to shut them down:</p>
<pre><code class="language-json">{
  &quot;teardown_frameworks&quot;: [
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;NONE&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           },
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;admin&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           }
                         ]
}
</code></pre>
<p><a name="disallowExample"></a>
The previous ACL can be fixed as follows:</p>
<pre><code class="language-json">{
  &quot;teardown_frameworks&quot;: [
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;admin&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           },
                           {
                             &quot;principals&quot;: { &quot;type&quot;: &quot;NONE&quot; },
                             &quot;framework_principals&quot;: { &quot;type&quot;: &quot;ANY&quot; }
                           }
                         ]
}
</code></pre>
<p>The <code>ops</code> principal can teardown any framework using the
<a href="endpoints/master/teardown.html">/teardown</a> HTTP endpoint. No other principal can
teardown any frameworks.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;teardown_frameworks&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;ops&quot;]
                             },
                             &quot;framework_principals&quot;: {
                               &quot;type&quot;: &quot;ANY&quot;
                             }
                           }
                         ]
}
</code></pre>
<p>The principal <code>foo</code> can reserve resources for any role, and no other principal
can reserve resources.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;reserve_resources&quot;: [
                         {
                           &quot;principals&quot;: {
                             &quot;values&quot;: [&quot;foo&quot;]
                           },
                           &quot;roles&quot;: {
                             &quot;type&quot;: &quot;ANY&quot;
                           }
                         }
                       ]
}
</code></pre>
<p>The principal <code>foo</code> cannot reserve resources, and any other principal (or
framework without a principal) can reserve resources for any role.</p>
<pre><code class="language-json">{
  &quot;reserve_resources&quot;: [
                         {
                           &quot;principals&quot;: {
                             &quot;values&quot;: [&quot;foo&quot;]
                           },
                           &quot;roles&quot;: {
                             &quot;type&quot;: &quot;NONE&quot;
                           }
                         }
                       ]
}
</code></pre>
<p>The principal <code>foo</code> can reserve resources only for roles <code>prod</code> and <code>dev</code>, and
no other principal (or framework without a principal) can reserve resources for
any role.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;reserve_resources&quot;: [
                         {
                           &quot;principals&quot;: {
                             &quot;values&quot;: [&quot;foo&quot;]
                           },
                           &quot;roles&quot;: {
                             &quot;values&quot;: [&quot;prod&quot;, &quot;dev&quot;]
                           }
                         }
                       ]
}
</code></pre>
<p>The principal <code>foo</code> can unreserve resources reserved by itself and by the
principal <code>bar</code>. The principal <code>bar</code>, however, can only unreserve its own
resources. No other principal can unreserve resources.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;unreserve_resources&quot;: [
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;]
                             },
                             &quot;reserver_principals&quot;: {
                               &quot;values&quot;: [&quot;foo&quot;, &quot;bar&quot;]
                             }
                           },
                           {
                             &quot;principals&quot;: {
                               &quot;values&quot;: [&quot;bar&quot;]
                             },
                             &quot;reserver_principals&quot;: {
                               &quot;values&quot;: [&quot;bar&quot;]
                             }
                           }
                         ]
}
</code></pre>
<p>The principal <code>foo</code> can create persistent volumes for any role, and no other
principal can create persistent volumes.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;create_volumes&quot;: [
                      {
                        &quot;principals&quot;: {
                          &quot;values&quot;: [&quot;foo&quot;]
                        },
                        &quot;roles&quot;: {
                          &quot;type&quot;: &quot;ANY&quot;
                        }
                      }
                    ]
}
</code></pre>
<p>The principal <code>foo</code> cannot create persistent volumes for any role, and any
other principal can create persistent volumes for any role.</p>
<pre><code class="language-json">{
  &quot;create_volumes&quot;: [
                      {
                        &quot;principals&quot;: {
                          &quot;values&quot;: [&quot;foo&quot;]
                        },
                        &quot;roles&quot;: {
                          &quot;type&quot;: &quot;NONE&quot;
                        }
                      }
                    ]
}
</code></pre>
<p>The principal <code>foo</code> can create persistent volumes only for roles <code>prod</code> and
<code>dev</code>, and no other principal can create persistent volumes for any role.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;create_volumes&quot;: [
                      {
                        &quot;principals&quot;: {
                          &quot;values&quot;: [&quot;foo&quot;]
                        },
                        &quot;roles&quot;: {
                          &quot;values&quot;: [&quot;prod&quot;, &quot;dev&quot;]
                        }
                      }
                    ]
}
</code></pre>
<p>The principal <code>foo</code> can destroy volumes created by itself and by the principal
<code>bar</code>. The principal <code>bar</code>, however, can only destroy its own volumes. No other
principal can destroy volumes.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;destroy_volumes&quot;: [
                       {
                         &quot;principals&quot;: {
                           &quot;values&quot;: [&quot;foo&quot;]
                         },
                         &quot;creator_principals&quot;: {
                           &quot;values&quot;: [&quot;foo&quot;, &quot;bar&quot;]
                         }
                       },
                       {
                         &quot;principals&quot;: {
                           &quot;values&quot;: [&quot;bar&quot;]
                         },
                         &quot;creator_principals&quot;: {
                           &quot;values&quot;: [&quot;bar&quot;]
                         }
                       }
                     ]
}
</code></pre>
<p>The principal <code>ops</code> can query quota status for any role. The principal <code>foo</code>,
however, can only query quota status for <code>foo-role</code>. No other principal can
query quota status.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;get_quotas&quot;: [
                  {
                    &quot;principals&quot;: {
                      &quot;values&quot;: [&quot;ops&quot;]
                    },
                    &quot;roles&quot;: {
                      &quot;type&quot;: &quot;ANY&quot;
                    }
                  },
                  {
                    &quot;principals&quot;: {
                      &quot;values&quot;: [&quot;foo&quot;]
                    },
                    &quot;roles&quot;: {
                      &quot;values&quot;: [&quot;foo-role&quot;]
                    }
                  }
                ]
}
</code></pre>
<p>The principal <code>ops</code> can update quota information (set or remove) for any role.
The principal <code>foo</code>, however, can only update quota for <code>foo-role</code>. No other
principal can update quota.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;update_quotas&quot;: [
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;ops&quot;]
                       },
                       &quot;roles&quot;: {
                         &quot;type&quot;: &quot;ANY&quot;
                       }
                     },
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;foo&quot;]
                       },
                       &quot;roles&quot;: {
                         &quot;values&quot;: [&quot;foo-role&quot;]
                       }
                     }
                   ]
}
</code></pre>
<p>The principal <code>ops</code> can reach all HTTP endpoints using the <em>GET</em>
method. The principal <code>foo</code>, however, can only use the HTTP <em>GET</em> on
the <code>/logging/toggle</code> and <code>/monitor/statistics</code> endpoints.  No other
principals can use <em>GET</em> on any endpoints.</p>
<pre><code class="language-json">{
  &quot;permissive&quot;: false,
  &quot;get_endpoints&quot;: [
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;ops&quot;]
                       },
                       &quot;paths&quot;: {
                         &quot;type&quot;: &quot;ANY&quot;
                       }
                     },
                     {
                       &quot;principals&quot;: {
                         &quot;values&quot;: [&quot;foo&quot;]
                       },
                       &quot;paths&quot;: {
                         &quot;values&quot;: [&quot;/logging/toggle&quot;, &quot;/monitor/statistics&quot;]
                       }
                     }
                   ]
}
</code></pre>
<h2 id="implementing-an-authorizer"><a class="header" href="#implementing-an-authorizer">Implementing an Authorizer</a></h2>
<p>In case you plan to implement your own authorizer <a href="modules.html">module</a>, the
authorization interface consists of three parts:</p>
<p>First, the <code>authorization::Request</code> protobuf message represents a request to be
authorized. It follows the
<em><a href="https://en.wikipedia.org/wiki/Subject%E2%80%93verb%E2%80%93object">Subject-Verb-Object</a></em>
pattern, where a <em>subject</em> ---commonly a principal---attempts to perform an
<em>action</em> on a given <em>object</em>.</p>
<p>Second, the
<code>Future&lt;bool&gt; mesos::Authorizer::authorized(const mesos::authorization::Request&amp; request)</code>
interface defines the entry point for authorizer modules (and the local
authorizer). A call to <code>authorized()</code> returns a future that indicates the result
of the (asynchronous) authorization operation. If the future is set to true, the
request was authorized successfully; if it was set to false, the request was
rejected. A failed future indicates that the request could not be processed at
the moment and it can be retried later.</p>
<p>The <code>authorization::Request</code> message is defined in authorizer.proto:</p>
<pre><code class="language-protoc">message Request {
  optional Subject subject = 1;
  optional Action  action  = 2;
  optional Object  object  = 3;
}

message Subject {
  optional string value = 1;
}

message Object {
  optional string value = 1;
  optional FrameworkInfo framework_info = 2;
  optional Task task = 3;
  optional TaskInfo task_info = 4;
  optional ExecutorInfo executor_info = 5;
  optional MachineID machine_id = 11;
}
</code></pre>
<p><code>Subject</code> or <code>Object</code> are optional fiels; if they are not set they
will only match an ACL with ANY or NONE in the
corresponding location. This allows users to construct the following requests:
<em>Can everybody perform action <strong>A</strong> on object <strong>O</strong>?</em>, or <em>Can principal <strong>Z</strong>
execute action <strong>X</strong> on all objects?</em>.</p>
<p><code>Object</code> has several optional fields of which, depending on the action,
one or more fields must be set
(e.g., the <code>view_executors</code> action expects the <code>executor_info</code> and
<code>framework_info</code> to be set).</p>
<p>The <code>action</code> field of the <code>Request</code> message is an enum. It is kept optional---
even though a valid action is necessary for every request---to allow for
backwards compatibility when adding new fields (see
<a href="https://issues.apache.org/jira/browse/MESOS-4997">MESOS-4997</a> for details).</p>
<p>Third, the <code>ObjectApprover</code> interface. In order to support efficient
authorization of large objects and multiple objects a user can request an
<code>ObjectApprover</code> via
<code>Future&lt;shared_ptr&lt;const ObjectApprover&gt;&gt; getApprover(const authorization::Subject&amp; subject, const authorization::Action&amp; action)</code>.
The resulting <code>ObjectApprover</code> provides
<code>Try&lt;bool&gt; approved(const ObjectApprover::Object&amp; object)</code> to synchronously
check whether objects are authorized. The <code>ObjectApprover::Object</code> follows the
structure of the <code>Request::Object</code> above.</p>
<pre><code class="language-cpp">struct Object
{
  const std::string* value;
  const FrameworkInfo* framework_info;
  const Task* task;
  const TaskInfo* task_info;
  const ExecutorInfo* executor_info;
  const MachineID* machine_id;
};
</code></pre>
<p>As the fields take pointer to each entity the <code>ObjectApprover::Object</code> does not
require the entity to be copied.</p>
<p>Authorizer must ensure that <code>ObjectApprover</code>s returned by <code>getApprover(...)</code> method
are valid throughout their whole lifetime. This is relied upon by parts of Mesos code
(Scheduler API, Operator API events and so on) that have a need to frequently authorize
a limited number of long-lived authorization subjects.
This code on the Mesos side, on its part, must ensure that it does not store
<code>ObjectApprover</code> for authorization subjects that it no longer uses (i.e. that it
does not leak <code>ObjectApprover</code>s).</p>
<p>NOTE: As the <code>ObjectApprover</code> is run synchronously in a different actor process
<code>ObjectApprover.approved()</code> call must not block!</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - SSL in Mesos
layout: documentation</h2>
<h1 id="ssl-in-mesos"><a class="header" href="#ssl-in-mesos">SSL in Mesos</a></h1>
<p>By default, all the messages that flow through the Mesos cluster are
unencrypted, making it possible for anyone with access to the cluster to
intercept and potentially control arbitrary tasks.</p>
<p>SSL/TLS support was added to libprocess in Mesos 0.23.0, which encrypts the
data that Mesos uses for network communication between Mesos components.
Additionally, HTTPS support was added to the Mesos WebUI.</p>
<h1 id="build-configuration"><a class="header" href="#build-configuration">Build Configuration</a></h1>
<p>There are currently two implementations of the
<a href="https://github.com/apache/mesos/blob/master/3rdparty/libprocess/include/process/socket.hpp">libprocess socket interface</a>
that support SSL.</p>
<p>The first implementation, added in Mesos 0.23.0, uses
<a href="https://github.com/libevent/libevent">libevent</a>.
Specifically it relies on the <code>libevent-openssl</code> library that wraps <code>openssl</code>.</p>
<p>The second implementation, added in Mesos 1.10.0, is a generic socket
wrapper which only relies on the OpenSSL (1.1+) library.</p>
<p>Before building Mesos from source, assuming you have installed the
required <a href="ssl.html#Dependencies">Dependencies</a>, you can modify your configure line
to enable SSL as follows:</p>
<pre><code>../configure --enable-ssl
# Or:
../configure --enable-libevent --enable-ssl
</code></pre>
<h1 id="runtime-configuration"><a class="header" href="#runtime-configuration">Runtime Configuration</a></h1>
<p>TLS support in Mesos can be configured with different levels of security. This section aims to help
Mesos operators to better understand the trade-offs involved in them.</p>
<p>On a high level, one can imagine to choose between three available layers of security, each
providing additional security guarantees but also increasing the deployment complexity.</p>
<ol>
<li>
<p><code>LIBPROCESS_SSL_ENABLED=true</code>. This provides external clients (e.g. curl) with the ability to
connect to Mesos HTTP endpoints securely via TLS, verifying that the server certificate is valid
and trusted.</p>
</li>
<li>
<p><code>LIBPROCESS_SSL_VERIFY_SERVER_CERT=true</code>. In addition to the above, this ensures that Mesos components
themselves are verifying the presence of valid and trusted server certificates when making
outgoing connections. This prevents man-in-the-middle attacks on communications between Mesos
components, and on communications between a Mesos component and an external server.</p>
<p><strong>WARNING:</strong> This setting only makes sense if <code>LIBPROCESS_SSL_ENABLE_DOWNGRADE</code> is set
to <code>false</code>, otherwise a malicious actor can simply bypass certificate verification by
downgrading to a non-TLS connection.</p>
</li>
<li>
<p><code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT=true</code>. In addition to the above, this enforces the use of TLS
client certificates on all connections to any Mesos component. This ensures that only trusted
clients can connect to any Mesos component, preventing reception of forged or malformed messages.</p>
<p>This implies that all schedulers or other clients (including the web browsers used by human
operators) that are supposed to connect to any endpoint of a Mesos component must be provided
with valid client certificates.</p>
<p><strong>WARNING:</strong> As above, this setting only makes sense if <code>LIBPROCESS_SSL_ENABLE_DOWNGRADE</code> is
set to <code>false</code>.</p>
</li>
</ol>
<p>For secure usage, it is recommended to set <code>LIBPROCESS_SSL_ENABLED=true</code>,
<code>LIBPROCESS_SSL_VERIFY_SERVER_CERT=true</code>, <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME=openssl</code>
and <code>LIBPROCESS_SSL_ENABLE_DOWNGRADE=false</code>. This provides a good trade-off
between security and usability.</p>
<p>It is not recommended in general to expose Mesos components to the public internet, but in cases
where they are the use of <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code> is strongly suggested.</p>
<h1 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h1>
<p>Once you have successfully built and installed your new binaries, here are the environment variables that are applicable to the <code>Master</code>, <code>Agent</code>, <code>Framework Scheduler/Executor</code>, or any <code>libprocess process</code>:</p>
<p><strong>NOTE:</strong> Prior to 1.0, the SSL related environment variables used to be prefixed by <code>SSL_</code>. However, we found that they may collide with other programs and lead to unexpected results (e.g., openssl, see <a href="https://issues.apache.org/jira/browse/MESOS-5863">MESOS-5863</a> for details). To be backward compatible, we accept environment variables prefixed by both <code>SSL_</code> or <code>LIBPROCESS_SSL_</code>. New users should use the <code>LIBPROCESS_SSL_</code> version.</p>
<h4 id="libprocess_ssl_enabledfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enabledfalse0true1-defaultfalse0">LIBPROCESS_SSL_ENABLED=(false|0,true|1) [default=false|0]</a></h4>
<p>Turn on or off SSL. When it is turned off it is the equivalent of default Mesos with libevent as the backing for events. All sockets default to the non-SSL implementation. When it is turned on, the default configuration for sockets is SSL. This means outgoing connections will use SSL, and incoming connections will be expected to speak SSL as well. None of the below flags are relevant if SSL is not enabled.  If SSL is enabled, <code>LIBPROCESS_SSL_CERT_FILE</code> and <code>LIBPROCESS_SSL_KEY_FILE</code> must be supplied.</p>
<h4 id="libprocess_ssl_support_downgradefalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_support_downgradefalse0true1-defaultfalse0">LIBPROCESS_SSL_SUPPORT_DOWNGRADE=(false|0,true|1) [default=false|0]</a></h4>
<p>Control whether or not non-SSL connections can be established. If this is enabled
<strong>on the accepting side</strong>, then the accepting side will downgrade to a non-SSL socket if the
connecting side is attempting to communicate via non-SSL. (e.g. HTTP).</p>
<p>If this is enabled <strong>on the connecting side</strong>, then the connecting side will retry on a non-SSL
socket if establishing the SSL connection failed.</p>
<p>See <a href="ssl.html#Upgrading">Upgrading Your Cluster</a> for more details.</p>
<h4 id="libprocess_ssl_key_filepath-to-key"><a class="header" href="#libprocess_ssl_key_filepath-to-key">LIBPROCESS_SSL_KEY_FILE=(path to key)</a></h4>
<p>The location of the private key used by OpenSSL.</p>
<pre><code>// For example, to generate a key with OpenSSL:
openssl genrsa -des3 -f4 -passout pass:some_password -out key.pem 4096
</code></pre>
<h4 id="libprocess_ssl_cert_filepath-to-certificate"><a class="header" href="#libprocess_ssl_cert_filepath-to-certificate">LIBPROCESS_SSL_CERT_FILE=(path to certificate)</a></h4>
<p>The location of the certificate that will be presented.</p>
<pre><code>// For example, to generate a root certificate with OpenSSL:
// (assuming the signing key already exists in `key.pem`)
openssl req -new -x509 -passin pass:some_password -days 365 -keyout key.pem -out cert.pem
</code></pre>
<h4 id="libprocess_ssl_verify_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_verify_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_VERIFY_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This is a legacy alias for the <code>LIBPROCESS_SSL_VERIFY_SERVER_CERT</code> setting.</p>
<h4 id="libprocess_ssl_verify_server_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_verify_server_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_VERIFY_SERVER_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This setting only affects the behaviour of libprocess in TLS client mode.</p>
<p>If this is true, a remote server is required to present a server certificate,
and the presented server certificates will be verified. That means
it will be checked that the certificate is cryptographically valid,
was generated by a trusted CA, and contains the correct hostname.</p>
<p>If this is false, a remote server is still required to present a server certificate (unless
an anonymous cipher is used), but the presented server certificates will not be verified.</p>
<p><strong>NOTE:</strong> When <code>LIBPROCESS_SSL_REQUIRE_CERT</code> is true, <code>LIBPROCESS_SSL_VERIFY_CERT</code> is automatically
set to true for backwards compatibility reasons.</p>
<h4 id="libprocess_ssl_require_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_require_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_REQUIRE_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This is a legacy alias for the <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code> setting.</p>
<h4 id="libprocess_ssl_require_client_certfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_require_client_certfalse0true1-defaultfalse0">LIBPROCESS_SSL_REQUIRE_CLIENT_CERT=(false|0,true|1) [default=false|0]</a></h4>
<p>This setting only affects the behaviour of libprocess in TLS server mode.</p>
<p>If this is true, enforce that certificates must be presented by connecting clients. This means all
connections (including external tooling trying to access HTTP endpoints, like web browsers etc.)
must present valid certificates in order to establish a connection.</p>
<p><strong>NOTE:</strong> The specifics of what it means for the certificate to &quot;contain the correct hostname&quot;
depend on the selected value of <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME</code>.</p>
<p><strong>NOTE:</strong> If this is set to false, client certificates are not verified even if they are presented
and <code>LIBPROCESS_SSL_VERIFY_CERT</code> is set to true.</p>
<h4 id="libprocess_ssl_verify_depthn-default4"><a class="header" href="#libprocess_ssl_verify_depthn-default4">LIBPROCESS_SSL_VERIFY_DEPTH=(N) [default=4]</a></h4>
<p>The maximum depth used to verify certificates. The default is 4. See the OpenSSL documentation or contact your system administrator to learn why you may want to change this.</p>
<h4 id="libprocess_ssl_verify_ipaddfalse0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_verify_ipaddfalse0true1-defaultfalse0">LIBPROCESS_SSL_VERIFY_IPADD=(false|0,true|1) [default=false|0]</a></h4>
<p>Enable IP address verification in the certificate subject alternative name extension. When set
to <code>true</code> the peer certificate verification will be able to use the IP address of a peer connection.</p>
<p>The specifics on when a certificate containing an IP address will we accepted depend on the
selected value of the <code>LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME</code>.</p>
<h4 id="libprocess_ssl_ca_dirpath-to-ca-directory"><a class="header" href="#libprocess_ssl_ca_dirpath-to-ca-directory">LIBPROCESS_SSL_CA_DIR=(path to CA directory)</a></h4>
<p>The directory used to find the certificate authority / authorities. You can specify <code>LIBPROCESS_SSL_CA_DIR</code> or <code>LIBPROCESS_SSL_CA_FILE</code> depending on how you want to restrict your certificate authorization.</p>
<h4 id="libprocess_ssl_ca_filepath-to-ca-file"><a class="header" href="#libprocess_ssl_ca_filepath-to-ca-file">LIBPROCESS_SSL_CA_FILE=(path to CA file)</a></h4>
<p>The file used to find the certificate authority. You can specify <code>LIBPROCESS_SSL_CA_DIR</code> or <code>LIBPROCESS_SSL_CA_FILE</code> depending on how you want to restrict your certificate authorization.</p>
<h4 id="libprocess_ssl_ciphersaccepted-ciphers-separated-by--defaultaes128-shaaes256-sharc4-shadhe-rsa-aes128-shadhe-dss-aes128-shadhe-rsa-aes256-shadhe-dss-aes256-sha"><a class="header" href="#libprocess_ssl_ciphersaccepted-ciphers-separated-by--defaultaes128-shaaes256-sharc4-shadhe-rsa-aes128-shadhe-dss-aes128-shadhe-rsa-aes256-shadhe-dss-aes256-sha">LIBPROCESS_SSL_CIPHERS=(accepted ciphers separated by ':') [default=AES128-SHA:AES256-SHA:RC4-SHA:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA:DHE-RSA-AES256-SHA:DHE-DSS-AES256-SHA]</a></h4>
<p>A list of <code>:</code>-separated ciphers. Use these if you want to restrict or open up the accepted ciphers for OpenSSL. Read the OpenSSL documentation or contact your system administrators to see whether you want to override the default values.</p>
<h4 id="libprocess_ssl_enable_ssl_v3false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_ssl_v3false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_SSL_V3=(false|0,true|1) [default=false|0]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_0false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_tls_v1_0false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_TLS_V1_0=(false|0,true|1) [default=false|0]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_1false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_tls_v1_1false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_TLS_V1_1=(false|0,true|1) [default=false|0]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_2false0true1-defaulttrue1"><a class="header" href="#libprocess_ssl_enable_tls_v1_2false0true1-defaulttrue1">LIBPROCESS_SSL_ENABLE_TLS_V1_2=(false|0,true|1) [default=true|1]</a></h4>
<h4 id="libprocess_ssl_enable_tls_v1_3false0true1-defaultfalse0"><a class="header" href="#libprocess_ssl_enable_tls_v1_3false0true1-defaultfalse0">LIBPROCESS_SSL_ENABLE_TLS_V1_3=(false|0,true|1) [default=false|0]</a></h4>
<p>The above switches enable / disable the specified protocols. By default only TLS V1.2 is enabled. SSL V2 is always disabled; there is no switch to enable it. The mentality here is to restrict security by default, and force users to open it up explicitly. Many older version of the protocols have known vulnerabilities, so only enable these if you fully understand the risks.
TLS V1.3 is not supported yet and should not be enabled. <a href="https://issues.apache.org/jira/browse/MESOS-9730">MESOS-9730</a>.
<em>SSLv2 is disabled completely because modern versions of OpenSSL disable it using multiple compile time configuration options.</em>
#<a name="Dependencies"></a>Dependencies</p>
<h4 id="libprocess_ssl_ecdh_curveautolist-of-curves-separated-by--defaultauto"><a class="header" href="#libprocess_ssl_ecdh_curveautolist-of-curves-separated-by--defaultauto">LIBPROCESS_SSL_ECDH_CURVE=(auto|list of curves separated by ':') [default=auto]</a></h4>
<p>List of elliptic curves which should be used for ECDHE-based cipher suites, in preferred order. Available values depend on the OpenSSL version used. Default value <code>auto</code> allows OpenSSL to pick the curve automatically.
OpenSSL versions prior to <code>1.0.2</code> allow for the use of only one curve; in those cases, <code>auto</code> defaults to <code>prime256v1</code>.</p>
<h4 id="libprocess_ssl_hostname_validation_schemelegacyopenssl-defaultlegacy"><a class="header" href="#libprocess_ssl_hostname_validation_schemelegacyopenssl-defaultlegacy">LIBPROCESS_SSL_HOSTNAME_VALIDATION_SCHEME=(legacy|openssl) [default=legacy]</a></h4>
<p>This flag is used to select the scheme by which the hostname validation check works.</p>
<p>Since hostname validation is part of certificate verification, this flag has no
effect unless one of <code>LIBPROCESS_SSL_VERIFY_SERVER_CERT</code> or <code>LIBPROCESS_SSL_REQUIRE_CLIENT_CERT</code>
is set to true.</p>
<p>Currently, it is possible to choose between two schemes:</p>
<ul>
<li>
<p><code>openssl</code>:</p>
<p>In client mode: Perform the hostname validation checks during the TLS handshake.
If the client connects via hostname, accept the certificate if it contains
the hostname as common name (CN) or as a subject alternative name (SAN).
If the client connects via IP address and <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is true,
accept the certificate if it contains the IP as a subject alternative name.</p>
<p><strong>NOTE:</strong> If the client connects via IP address and <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is false,
the connection attempt cannot succeed.</p>
<p>In server mode: Do not perform any hostname validation checks.</p>
<p>This setting requires OpenSSL &gt;= 1.0.2 to be used.</p>
</li>
<li>
<p><code>legacy</code>:</p>
<p>Use a custom hostname validation algorithm that is run after the connection is established,
and immediately close the connection if it fails.</p>
<p>In both client and server mode:
Do a reverse DNS lookup on the peer IP. If <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is set to <code>false</code>,
accept the certificate if it contains the first result of that lookup as either the common name
or as a subject alternative name. If <code>LIBPROCESS_SSL_VERIFY_IPADD</code> is set to <code>true</code>,
additionally accept the certificate if it contains the peer IP as a subject alternative name.</p>
</li>
</ul>
<p>It is suggested that operators choose the 'openssl' setting unless they have
applications relying on the legacy behaviour of the 'libprocess' scheme. It is
using standardized APIs (<code>X509_VERIFY_PARAM_check_{host,ip}</code>) provided by OpenSSL to
make hostname validation more uniform across applications. It is also more secure,
since attackers that are able to forge a DNS or rDNS result can launch a successful
man-in-the-middle attack on the 'legacy' scheme.</p>
<h3 id="libevent"><a class="header" href="#libevent">libevent</a></h3>
<p>If building with <code>--enable-libevent</code>, we require the OpenSSL support from
libevent. The suggested version of libevent is
<a href="https://github.com/libevent/libevent/releases/tag/release-2.0.22-stable"><code>2.0.22-stable</code></a>.
As new releases come out we will try to maintain compatibility.</p>
<pre><code>// For example, on OSX:
brew install libevent
</code></pre>
<h3 id="openssl"><a class="header" href="#openssl">OpenSSL</a></h3>
<p>We require <a href="https://github.com/openssl/openssl">OpenSSL</a>.
There are multiple branches of OpenSSL that are being maintained by the
community. Since security requires being vigilant, we recommend reading
the release notes for the current releases of OpenSSL and deciding on a
version within your organization based on your security needs.</p>
<p>When building with libevent, Mesos is not too deeply dependent on specific
OpenSSL versions, so there is room for you to make security decisions as
an organization. When building without libevent, OpenSSL 1.1+ is required,
because Mesos makes use of APIs introduced in later versions of OpenSSL.</p>
<p>Please ensure the <code>event2</code> (when building with libevent) and
<code>openssl</code> headers are available for building Mesos.</p>
<pre><code>// For example, on OSX:
brew install openssl
</code></pre>
<h1 id="upgrading-your-cluster"><a class="header" href="#upgrading-your-cluster"><a name="Upgrading"></a>Upgrading Your Cluster</a></h1>
<p><em>There is no SSL specific requirement for upgrading different components in a specific order.</em></p>
<p>The recommended strategy is to restart all your components to enable SSL with downgrades support enabled. Once all components have SSL enabled, then do a second restart of all your components to disable downgrades. This strategy will allow each component to be restarted independently at your own convenience with no time restrictions. It will also allow you to try SSL in a subset of your cluster.</p>
<p><strong>NOTE:</strong> While different components in your cluster are serving SSL vs non-SSL traffic, any relative links in the WebUI may be broken. Please see the <a href="ssl.html#WebUI">WebUI</a> section for details. Here are sample commands for upgrading your cluster:</p>
<pre><code>// Restart each component with downgrade support (master, agent, framework):
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true LIBPROCESS_SSL_KEY_FILE=&lt;path-to-your-private-key&gt; LIBPROCESS_SSL_CERT_FILE=&lt;path-to-your-certificate&gt; &lt;Any other LIBPROCESS_SSL_* environment variables you may choose&gt; &lt;your-component (e.g. bin/master.sh)&gt; &lt;your-flags&gt;

// Restart each component WITHOUT downgrade support (master, agent, framework):
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_SUPPORT_DOWNGRADE=false LIBPROCESS_SSL_KEY_FILE=&lt;path-to-your-private-key&gt; LIBPROCESS_SSL_CERT_FILE=&lt;path-to-your-certificate&gt; &lt;Any other LIBPROCESS_SSL_* environment variables you may choose&gt; &lt;your-component (e.g. bin/master.sh)&gt; &lt;your-flags&gt;
</code></pre>
<p>Executors must be able to access the SSL environment variables and the files referred to by those variables. Environment variables can be provided to an executor by specifying <code>CommandInfo.environment</code> or by using the agent's <code>--executor_environment_variables</code> command line flag. If the agent and the executor are running in separate containers, <code>ContainerInfo.volumes</code> can be used to mount SSL files from the host into the executor's container.</p>
<p>The end state is a cluster that is only communicating with SSL.</p>
<p><strong>NOTE:</strong> Any tools you may use that communicate with your components must be able to speak SSL, or they will be denied. You may choose to maintain <code>LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true</code> for some time as you upgrade your internal tooling. The advantage of <code>LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true</code> is that all components that speak SSL will do so, while other components may still communicate over insecure channels.</p>
<h1 id="webui"><a class="header" href="#webui"><a name="WebUI"></a>WebUI</a></h1>
<p>The default Mesos WebUI uses relative links. Some of these links transition between endpoints served by the master and agents. The WebUI currently does not have enough information to change the 'http' vs 'https' links based on whether the target endpoint is currently being served by an SSL-enabled binary. This may cause certain links in the WebUI to be broken when a cluster is in a transition state between SSL and non-SSL. Any tools that hit these endpoints will still be able to access them as long as they hit the endpoint using the right protocol, or the <code>LIBPROCESS_SSL_SUPPORT_DOWNGRADE</code> option is set to true.</p>
<p><strong>NOTE:</strong> Frameworks with their own WebUI will need to add HTTPS support separately.</p>
<h3 id="certificates"><a class="header" href="#certificates">Certificates</a></h3>
<p>Most browsers have built in protection that guard transitions between pages served using different certificates. For this reason you may choose to serve both the master and agent endpoints using a common certificate that covers multiple hostnames. If you do not do this, certain links, such as those to agent sandboxes, may seem broken as the browser treats the transition between differing certificates transition as unsafe.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Secrets Handling
layout: documentation</h2>
<h1 id="secrets"><a class="header" href="#secrets">Secrets</a></h1>
<p>Starting 1.4.0 release, Mesos allows tasks to populate environment variables and
file volumes with secret contents that are retrieved using a secret-resolver
interface. It also allows specifying image-pull secrets for private container
registry. This allows users to avoid exposing critical secrets in task
definitions. Secrets are fetched/resolved using a secret-resolver module (see
below).</p>
<p>NOTE: Secrets are only supported for Mesos containerizer and not for the Docker
containerizer.</p>
<h2 id="secrets-message"><a class="header" href="#secrets-message">Secrets Message</a></h2>
<p>Secrets can be specified using the following protobuf message:</p>
<pre><code>message Secret {
  enum Type {
    UNKNOWN = 0;
    REFERENCE = 1;
    VALUE = 2;
  }

  message Reference {
    required string name = 1;
    optional string key = 2;
  }

  message Value {
    required bytes data = 1;
  }

  optional Type type = 1;

  optional Reference reference = 2;
  optional Value value = 3;
}
</code></pre>
<p>Secrets can be of type <code>reference</code> or <code>value</code> (only one of <code>reference</code> and <code>value</code> must be set).
A secret reference can be used by modules to refer to a secret stored in a secure back-end.
The <code>key</code> field can be used to reference a single value within a secret containing arbitrary key-value pairs.</p>
<p>For example, given a back-end secret store with a secret named &quot;/my/secret&quot; containing the following key-value pairs:</p>
<pre><code>{
  &quot;username&quot;: &quot;my-user&quot;,
  &quot;password&quot;: &quot;my-password
}
</code></pre>
<p>The username could be referred to in a <code>Secret</code> by specifying &quot;my/secret&quot; for the <code>name</code> and &quot;username&quot; for the <code>key</code>.</p>
<p>Secret also supports pass-by-value where the value of a secret can be directly
passed in the message.</p>
<h2 id="environment-based-secrets"><a class="header" href="#environment-based-secrets">Environment-based Secrets</a></h2>
<p>Environment variables can either be traditional value-based or secret-based. For
the latter, one can specify a secret as part of environment definition as shown
in the following example:</p>
<pre><code>{
  &quot;variables&quot; : [
    {
      &quot;name&quot;: &quot;MY_SECRET_ENV&quot;,
      &quot;type&quot;: &quot;SECRET&quot;,
      &quot;secret&quot;: {
        &quot;type&quot;: &quot;REFERENCE&quot;,
        &quot;reference&quot;: {
          &quot;name&quot;: &quot;/my/secret&quot;,
          &quot;key&quot;: &quot;username&quot;
        }
      }
    },
    {
      &quot;name&quot;: &quot;MY_NORMAL_ENV&quot;,
      &quot;value&quot;: &quot;foo&quot;
    }
  ]
}
</code></pre>
<h2 id="file-based-secrets"><a class="header" href="#file-based-secrets">File-based Secrets</a></h2>
<p>A new <code>volume/secret</code> isolator is available to create secret-based files inside
the task container. To use a secret, one can specify a new volume as follows:</p>
<pre><code>{
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;container_path&quot;: &quot;path/to/secret/file&quot;,
  &quot;source&quot;:
  {
    &quot;type&quot;: &quot;SECRET&quot;,
    &quot;secret&quot;: {
      &quot;type&quot;: &quot;REFERENCE&quot;,
      &quot;reference&quot;: {
        &quot;name&quot;: &quot;/my/secret&quot;,
        &quot;key&quot;: &quot;username&quot;
      }
    }
  }
}
</code></pre>
<p>This will create a tmpfs-based file mount in the container at &quot;path/to/secret/file&quot; which will contain the secret text fetched from the back-end secret store.</p>
<p>The <code>volume/secret</code> isolator is not enabled by default. To enable it, it must be specified in <code>--isolator=volume/secret</code> agent flag.</p>
<h2 id="image-pull-secrets"><a class="header" href="#image-pull-secrets">Image-pull Secrets</a></h2>
<p>Currently, image-pull secrets only support Docker images for Mesos
containerizer. Appc images are not supported.
One can store Docker config containing credentials to authenticate with Docker registry in the secret store.
The secret is expected to be a Docker config file in JSON format with UTF-8 character encoding.
The secret can then be referenced in the <code>Image</code> protobuf as follows:</p>
<pre><code>{
  &quot;type&quot;: &quot;DOCKER&quot;,
  &quot;docker&quot;:
  message Docker {
    &quot;name&quot;: &quot;&lt;REGISTRY_HOST&gt;/path/to/image&quot;,
    &quot;secret&quot;: {
      &quot;type&quot;: &quot;REFERENCE&quot;,
      &quot;reference&quot;: {
        &quot;name&quot;: &quot;/my/secret/docker/config&quot;
      }
    }
  }
}
</code></pre>
<h2 id="secretresolver-module"><a class="header" href="#secretresolver-module">SecretResolver Module</a></h2>
<p>The SecretResolver module is called from Mesos agent to fetch/resolve any image-pull, environment-based, or file-based secrets. (See <a href="modules.html">Mesos Modules</a> for more information on using Mesos modules).</p>
<pre><code>class SecretResolver
{
  virtual process::Future&lt;Secret::Value&gt; resolve(const Secret&amp; secret) const;
};
</code></pre>
<p>The default implementation simply resolves value-based Secrets. A custom secret-resolver module can be specified using the <code>--secret_resolver=&lt;module-name&gt;</code> agent flag.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Containerizers
layout: documentation</h2>
<h1 id="containerizers-1"><a class="header" href="#containerizers-1">Containerizers</a></h1>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p>Containerizers are used to run tasks in 'containers', which in turn are
used to:</p>
<ul>
<li>Isolate a task from other running tasks.</li>
<li>'Contain' tasks to run in limited resource runtime environment.</li>
<li>Control a task's resource usage (e.g., CPU, memory) programatically.</li>
<li>Run software in a pre-packaged file system image, allowing it to run in
different environments.</li>
</ul>
<h2 id="types-of-containerizers"><a class="header" href="#types-of-containerizers">Types of containerizers</a></h2>
<p>Mesos plays well with existing container technologies (e.g., docker) and also
provides its own container technology. It also supports composing different
container technologies (e.g., docker and mesos).</p>
<p>Mesos implements the following containerizers:</p>
<ul>
<li><a href="containerizers.html#Composing">Composing</a></li>
<li><a href="containerizers.html#Docker">Docker</a></li>
<li><a href="containerizers.html#Mesos">Mesos (default)</a></li>
</ul>
<p>User can specify the types of containerizers to use via the agent flag
<code>--containerizers</code>.</p>
<p><a name="Composing"></a></p>
<h3 id="composing-containerizer"><a class="header" href="#composing-containerizer">Composing containerizer</a></h3>
<p>This feature allows multiple container technologies to play together. It is
enabled when you configure the <code>--containerizers</code> agent flag with multiple comma
seperated containerizer names (e.g., <code>--containerizers=mesos,docker</code>). The order
of the comma separated list is important as the first containerizer that
supports the task's container configuration will be used to launch the task.</p>
<p>Use cases:</p>
<ul>
<li>For testing tasks with different types of resource isolations. Since 'mesos'
containerizers have more isolation abilities, a framework can use composing
containerizer to test a task using 'mesos' containerizer's controlled
environment and at the same time test it to work with 'docker' containers by
just changing the container parameters for the task.</li>
</ul>
<p><a name="Docker"></a></p>
<h3 id="docker-containerizer"><a class="header" href="#docker-containerizer">Docker containerizer</a></h3>
<p>Docker containerizer allows tasks to be run inside docker container. This
containerizer is enabled when you configure the agent flag as
<code>--containerizers=docker</code>.</p>
<p>Use cases:</p>
<ul>
<li>If a task needs to be run with the tooling that comes with the docker package.</li>
<li>If Mesos agent is running inside a docker container.</li>
</ul>
<p>For more details, see
<a href="docker-containerizer.html">Docker Containerizer</a>.</p>
<p><a name="Mesos"></a></p>
<h3 id="mesos-containerizer"><a class="header" href="#mesos-containerizer">Mesos containerizer</a></h3>
<p>This containerizer allows tasks to be run with an array of pluggable isolators
provided by Mesos. This is the native Mesos containerizer solution and is
enabled when you configure the agent flag as <code>--containerizers=mesos</code>.</p>
<p>Use cases:</p>
<ul>
<li>Allow Mesos to control the task's runtime environment without depending on
other container technologies (e.g., docker).</li>
<li>Want fine grained operating system controls (e.g., cgroups/namespaces provided
by Linux).</li>
<li>Want Mesos's latest container technology features.</li>
<li>Need additional resource controls like disk usage limits, which
might not be provided by other container technologies.</li>
<li>Want to add custom isolation for tasks.</li>
</ul>
<p>For more details, see
<a href="mesos-containerizer.html">Mesos Containerizer</a>.</p>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><a href="containerizer-internals.html">Containerizer Internals</a> for
implementation details of containerizers.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Containerizer Internals
layout: documentation</h2>
<h1 id="containerizer"><a class="header" href="#containerizer">Containerizer</a></h1>
<p>Containerizers are Mesos components responsible for launching
containers. They own the containers launched for the tasks/executors,
and are responsible for their isolation, resource management, and
events (e.g., statistics).</p>
<h1 id="containerizer-internals"><a class="header" href="#containerizer-internals">Containerizer internals</a></h1>
<h3 id="containerizer-creation-and-launch"><a class="header" href="#containerizer-creation-and-launch">Containerizer creation and launch</a></h3>
<ul>
<li>Agent creates a containerizer based on the flags (using agent flag
<code>--containerizers</code>). If multiple containerizers (e.g., docker,
mesos) are specified using the <code>--containerizers</code> flag, then the
composing containerizer will be used to create a containerizer.</li>
<li>If an executor is not specified in <code>TaskInfo</code>, Mesos agent will use
the default executor for the task (depending on the Containerizer
the agent is using, it could be <code>mesos-executor</code> or
<code>mesos-docker-executor</code>). TODO: Update this after MESOS-1718 is
completed. After this change, master will be responsible for
generating executor information.</li>
</ul>
<h3 id="types-of-containerizers-1"><a class="header" href="#types-of-containerizers-1">Types of containerizers</a></h3>
<p>Mesos currently supports the following containerizers:</p>
<ul>
<li>Composing</li>
<li><a href="docker-containerizer.html">Docker</a></li>
<li><a href="mesos-containerizer.html">Mesos</a></li>
</ul>
<h4 id="composing-containerizer-1"><a class="header" href="#composing-containerizer-1">Composing Containerizer</a></h4>
<p>Composing containerizer will compose the specified containerizers
(using agent flag <code>--containerizers</code>) and act like a single
containerizer. This is an implementation of the <code>composite</code> design
pattern.</p>
<h4 id="docker-containerizer-1"><a class="header" href="#docker-containerizer-1">Docker Containerizer</a></h4>
<p>Docker containerizer manages containers using the docker engine provided
in the docker package.</p>
<h5 id="container-launch"><a class="header" href="#container-launch">Container launch</a></h5>
<ul>
<li>Docker containerizer will attempt to launch the task in docker only
if <code>ContainerInfo::type</code> is set to DOCKER.</li>
<li>Docker containerizer will first pull the image.</li>
<li>Calls pre-launch hook.</li>
<li>The executor will be launched in one of the two ways:</li>
</ul>
<p>A) Mesos agent runs in a docker container</p>
<ul>
<li>This is indicated by the presence of agent flag
<code>--docker_mesos_image</code>. In this case, the value of flag
<code>--docker_mesos_image</code> is assumed to be the docker image used to
launch the Mesos agent.</li>
<li>If the task includes an executor (custom executor), then that executor is
launched in a docker container.</li>
<li>If the task does not include an executor i.e. it defines a command, the
default executor <code>mesos-docker-executor</code> is launched in a docker container to
execute the command via Docker CLI.</li>
</ul>
<p>B) Mesos agent does not run in a docker container</p>
<ul>
<li>If the task includes an executor (custom executor), then that executor is
launched in a docker container.</li>
<li>If task does not include an executor i.e. it defines a command, a subprocess
is forked to execute the default executor <code>mesos-docker-executor</code>.
<code>mesos-docker-executor</code> then spawns a shell to execute the command via Docker
CLI.</li>
</ul>
<h4 id="mesos-containerizer-1"><a class="header" href="#mesos-containerizer-1">Mesos Containerizer</a></h4>
<p>Mesos containerizer is the native Mesos containerizer. Mesos
Containerizer will handle any executor/task that does not specify
<code>ContainerInfo::DockerInfo</code>.</p>
<h5 id="container-launch-1"><a class="header" href="#container-launch-1">Container launch</a></h5>
<ul>
<li>Calls prepare on each isolator.</li>
<li>Forks the executor using Launcher (see <a href="containerizer-internals.html#Launcher">Launcher</a>). The
forked child is blocked from executing until it is been isolated.</li>
<li>Isolate the executor. Call isolate with the pid for each isolator
(see <a href="containerizer-internals.html#Isolators">Isolators</a>).</li>
<li>Fetch the executor.</li>
<li>Exec the executor. The forked child is signalled to continue. It
will first execute any preparation commands from isolators and then
exec the executor.</li>
</ul>
<p><a name="Launcher"></a></p>
<h5 id="launcher"><a class="header" href="#launcher">Launcher</a></h5>
<p>Launcher is responsible for forking/destroying containers.</p>
<ul>
<li>Forks a new process in the containerized context. The child will
exec the binary at the given path with the given argv, flags, and
environment.</li>
<li>The I/O of the child will be redirected according to the specified
I/O descriptors.</li>
</ul>
<h6 id="linux-launcher"><a class="header" href="#linux-launcher">Linux launcher</a></h6>
<ul>
<li>Creates a &quot;freezer&quot; cgroup for the container.</li>
<li>Creates posix &quot;pipe&quot; to enable communication between host (parent
process) and container process.</li>
<li>Spawn child process (container process) using <code>clone</code> system call.</li>
<li>Moves the new container process to the freezer hierarchy.</li>
<li>Signals the child process to continue (exec'ing) by writing a
character to the write end of the pipe in the parent process.</li>
</ul>
<p>Starting from Mesos 1.1.0, <a href="nested-container-and-task-group.html">nested container</a>
is supported. The Linux Launcher is responsible to fork the subprocess
for the nested container with appropriate Linux namespaces being
cloned. The following is the table for Linux namespaces that
are supported for top level and nested containers.</p>
<h6 id="linux-namespaces"><a class="header" href="#linux-namespaces">Linux Namespaces</a></h6>
<table class="table table-striped">
  <tr>
    <th>Linux Namespaces</th>
    <th>Top Level Container</th>
    <th>Nested Container</th>
  </tr>
  <tr>
    <td>Mount</td>
    <td>Not shared</td>
    <td>Not shared</td>
  </tr>
  <tr>
    <td>PID</td>
    <td>Configurable</td>
    <td>Configurable</td>
  </tr>
  <tr>
    <td>Network & UTS</td>
    <td>Configurable</td>
    <td>Shared w/ parent</td>
  </tr>
  <tr>
    <td>IPC</td>
    <td>Not shared -> configurable (TBD)</td>
    <td>Not shared -> configurable (TBD)</td>
  </tr>
  <tr>
    <td>Cgroup</td>
    <td>Shared w/ agent -> Not shared (TBD)</td>
    <td>Shared w/ parent -> Not shared (TBD)</td>
  </tr>
  <tr>
    <td>User (not supported)</td>
    <td>Shared w/ agent</td>
    <td>Shared w/ parent</td>
  </tr>
</table>
<p>*Note: For the top level container, <code>shared</code> means that the container
shares the namespace from the agent. For the nested container, <code>shared</code>
means that the nested container shares the namespace from its parent
container.</p>
<h6 id="posix-launcher-tbd"><a class="header" href="#posix-launcher-tbd">Posix launcher (TBD)</a></h6>
<p><a name="Isolators"></a></p>
<h5 id="isolators"><a class="header" href="#isolators"><a href="mesos-containerizer.html#isolators">Isolators</a></a></h5>
<p><a href="mesos-containerizer.html#isolators">Isolators</a> are responsible for creating
an environment for the containers where resources like cpu, network,
storage and memory can be isolated from other containers.</p>
<h3 id="containerizer-states"><a class="header" href="#containerizer-states">Containerizer states</a></h3>
<h4 id="docker"><a class="header" href="#docker">Docker</a></h4>
<ul>
<li>FETCHING</li>
<li>PULLING</li>
<li>RUNNING</li>
<li>DESTROYING</li>
</ul>
<h4 id="mesos"><a class="header" href="#mesos">Mesos</a></h4>
<ul>
<li>PREPARING</li>
<li>ISOLATING</li>
<li>FETCHING</li>
<li>RUNNING</li>
<li>DESTROYING</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Docker Containerizer
layout: documentation</h2>
<h1 id="docker-containerizer-2"><a class="header" href="#docker-containerizer-2">Docker Containerizer</a></h1>
<p>Mesos 0.20.0 adds the support for launching tasks that contains Docker
images, with also a subset of Docker options supported while we plan
on adding more in the future.</p>
<p>Users can either launch a Docker image as a Task, or as an Executor.</p>
<p>The following sections will describe the API changes along with Docker
support, and also how to setup Docker.</p>
<h2 id="setup"><a class="header" href="#setup">Setup</a></h2>
<p>To run the agent to enable the Docker Containerizer, you must launch
the agent with &quot;docker&quot; as one of the containerizers option.</p>
<p>Example: <code>mesos-agent --containerizers=docker,mesos</code></p>
<p>Each agent that has the Docker containerizer should have Docker CLI
client installed (version &gt;= 1.8.0).</p>
<p>If you enable iptables on agent, make sure the iptables allow all
traffic from docker bridge interface through add below rule:</p>
<pre><code>iptables -A INPUT -s 172.17.0.0/16 -i docker0 -p tcp -j ACCEPT
</code></pre>
<h2 id="how-do-i-use-the-docker-containerizer"><a class="header" href="#how-do-i-use-the-docker-containerizer">How do I use the Docker Containerizer?</a></h2>
<p>TaskInfo before 0.20.0 used to only support either setting a
CommandInfo that launches a task running the bash command, or an
ExecutorInfo that launches a custom Executor that will launch the
task.</p>
<p>With 0.20.0 we added a ContainerInfo field to TaskInfo and
ExecutorInfo that allows a Containerizer such as Docker to be
configured to run the task or executor.</p>
<p>To run a Docker image as a task, in TaskInfo one must set both the
command and the container field as the Docker Containerizer will use
the accompanied command to launch the docker image.  The ContainerInfo
should have type Docker and a DockerInfo that has the desired docker
image.</p>
<p>To run a Docker image as an executor, in TaskInfo one must set the
ExecutorInfo that contains a ContainerInfo with type docker and the
CommandInfo that will be used to launch the executor.  Note that the
Docker image is expected to launch up as a Mesos executor that will
register with the agent once it launches.</p>
<h2 id="what-does-the-docker-containerizer-do"><a class="header" href="#what-does-the-docker-containerizer-do">What does the Docker Containerizer do?</a></h2>
<p>The Docker Containerizer is translating Task/Executor <code>Launch</code> and
<code>Destroy</code> calls to Docker CLI commands.</p>
<p>Currently the Docker Containerizer when launching as task will do the
following:</p>
<ol>
<li>
<p>Fetch all the files specified in the CommandInfo into the sandbox.</p>
</li>
<li>
<p>Pull the docker image from the remote repository.</p>
</li>
<li>
<p>Run the docker image with the Docker executor, and map the sandbox
directory into the Docker container and set the directory mapping to
the MESOS_SANDBOX environment variable. The executor will also stream
the container logs into stdout/stderr files in the sandbox.</p>
</li>
<li>
<p>On container exit or containerizer destroy, stop and remove the
docker container.</p>
</li>
</ol>
<p>The Docker Containerizer launches all containers with the <code>mesos-</code>
prefix plus the agent id (ie: <code>mesos-agent1-abcdefghji</code>), and also
assumes all containers with the <code>mesos-</code> prefix is managed by the
agent and is free to stop or kill the containers.</p>
<p>When launching the docker image as an Executor, the only difference is
that it skips launching a command executor but just reaps on the
docker container executor pid.</p>
<p>Note that we currently default to host networking when running a
docker image, to easier support running a docker image as an Executor.</p>
<p>The containerizer also supports optional force pulling of the image.
It is set disabled as default, so the docker image will only be
updated again if it's not available on the host. To enable force
pulling an image, <code>force_pull_image</code> has to be set as true.</p>
<h2 id="private-docker-repository"><a class="header" href="#private-docker-repository">Private Docker repository</a></h2>
<p>To run an image from a private repository, one can include the uri
pointing to a <code>.dockercfg</code> that contains login information.  The
<code>.dockercfg</code> file will be pulled into the sandbox the Docker
Containerizer set the HOME environment variable pointing to the
sandbox so docker cli will automatically pick up the config file.</p>
<p>Starting from 1.0, we provide an alternative way to specify docker
config file for pulling images from private registries. We allow
operators to specify a shared docker config file using an agent flag.
This docker config file will be used to pull images from private
registries for all containers. See <a href="configuration/agent.html">configuration
documentation</a> for detail. Operators can either
specify the flag as an absolute path pointing to the docker config
file (need to manually configure <code>.docker/config.json</code> or <code>.dockercfg</code>
on each agent), or specify the flag as a JSON-formatted string.  For
example:</p>
<pre><code>--docker_config=file:///home/vagrant/.docker/config.json
</code></pre>
<p>or as a JSON object,</p>
<pre><code>--docker_config=&quot;{ \
  \&quot;auths\&quot;: { \
    \&quot;https://index.docker.io/v1/\&quot;: { \
      \&quot;auth\&quot;: \&quot;xXxXxXxXxXx=\&quot;, \
      \&quot;email\&quot;: \&quot;username@example.com\&quot; \
    } \
  } \
}&quot;
</code></pre>
<h2 id="commandinfo-to-run-docker-images"><a class="header" href="#commandinfo-to-run-docker-images">CommandInfo to run Docker images</a></h2>
<p>A docker image currently supports having an entrypoint and/or a
default command.</p>
<p>To run a docker image with the default command (ie: <code>docker run image</code>), the CommandInfo's value must not be set. If the value is set
then it will override the default command.</p>
<p>To run a docker image with an entrypoint defined, the CommandInfo's
shell option must be set to false.  If shell option is set to true the
Docker Containerizer will run the user's command wrapped with <code>/bin/sh -c</code> which will also become parameters to the image entrypoint.</p>
<h2 id="recover-docker-containers-on-agent-recovery"><a class="header" href="#recover-docker-containers-on-agent-recovery">Recover Docker containers on agent recovery</a></h2>
<p>The Docker containerizer supports recovering Docker containers when
the agent restarts, which supports both when the agent is running in a
Docker container or not.</p>
<p>With the <code>--docker_mesos_image</code> flag enabled, the Docker containerizer
assumes the containerizer is running in a container itself and
modifies the mechanism it recovers and launches docker containers
accordingly.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Mesos Containerizer
layout: documentation</h2>
<h1 id="mesos-containerizer-2"><a class="header" href="#mesos-containerizer-2">Mesos Containerizer</a></h1>
<p>The Mesos Containerizer provides lightweight containerization and
resource isolation of executors using Linux-specific functionality
such as control cgroups and namespaces. It is composable so operators
can selectively enable different <a href="mesos-containerizer.html#isolators">isolators</a>.</p>
<p>It also provides basic support for POSIX systems (e.g., OSX) but
without any actual isolation, only resource usage reporting.</p>
<h2 id="isolators-1"><a class="header" href="#isolators-1">Isolators</a></h2>
<p>Isolators are components that each define an aspect of how a tasks
execution environment (or container) is constructed. Isolators can
control how containers are isolated from each other, how task resource
limits are enforced, how networking is configured, how security
policies are applied.</p>
<p>Since the isolator interface is <a href="modules.html">modularized</a>, operators
can write modules that implement custom isolators.</p>
<p>Mesos supports the following built-in isolators.</p>
<ul>
<li>appc/runtime</li>
<li><a href="isolators/cgroups-blkio.html">cgroups/blkio</a></li>
<li><a href="isolators/cgroups-cpu.html">cgroups/cpu</a></li>
<li>cgroups/cpuset</li>
<li><a href="isolators/cgroups-devices.html">cgroups/devices</a></li>
<li>cgroups/hugetlb</li>
<li>cgroups/mem</li>
<li><a href="isolators/cgroups-net-cls.html">cgroups/net_cls</a></li>
<li>cgroups/net_prio</li>
<li>cgroups/perf_event</li>
<li>cgroups/pids</li>
<li><a href="isolators/disk-du.html">disk/du</a></li>
<li><a href="isolators/disk-xfs.html">disk/xfs</a></li>
<li><a href="isolators/docker-runtime.html">docker/runtime</a></li>
<li><a href="isolators/docker-volume.html">docker/volume</a></li>
<li><a href="secrets.html#environment-based-secrets">environment_secret</a></li>
<li><a href="isolators/filesystems.html">filesystem/linux</a></li>
<li><a href="isolators/filesystems.html">filesystem/posix</a></li>
<li><a href="isolators/filesystem-shared.html">filesystem/shared</a></li>
<li>filesystem/windows</li>
<li><a href="gpu-support.html">gpu/nvidia</a></li>
<li><a href="isolators/linux-capabilities.html">linux/capabilities</a></li>
<li><a href="isolators/linux-devices.html">linux/devices</a></li>
<li><a href="isolators/linux-nnp.html">linux/nnp</a></li>
<li><a href="isolators/linux-seccomp.html">linux/seccomp</a></li>
<li><a href="isolators/namespaces-ipc.html">namespaces/ipc</a></li>
<li><a href="isolators/namespaces-pid.html">namespaces/pid</a></li>
<li><a href="cni.html">network/cni</a></li>
<li><a href="isolators/network-port-mapping.html">network/port_mapping</a></li>
<li><a href="isolators/network-ports.html">network/ports</a></li>
<li>posix/cpu</li>
<li>posix/mem</li>
<li><a href="isolators/posix-rlimits.html">posix/rlimits</a></li>
<li><a href="isolators/csi-volume.html">volume/csi</a></li>
<li><a href="container-volume.html#host_path-volume-source">volume/host_path</a></li>
<li>volume/image</li>
<li><a href="container-volume.html#sandbox_path-volume-source">volume/sandbox_path</a></li>
<li><a href="secrets.html#file-based-secrets">volume/secret</a></li>
<li><a href="isolators/windows.html#cpu-limits">windows/cpu</a></li>
<li><a href="isolators/windows.html#memory-limits">windows/mem</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Supporting Container Images in Mesos Containerizer
layout: documentation</h2>
<h1 id="supporting-container-images-in-mesos-containerizer"><a class="header" href="#supporting-container-images-in-mesos-containerizer">Supporting Container Images in <a href="mesos-containerizer.html">Mesos Containerizer</a></a></h1>
<h2 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h2>
<p>Mesos currently supports several <a href="containerizers.html">containerizers</a>,
notably the Mesos containerizer and the Docker containerizer. Mesos
containerizer uses native OS features directly to provide isolation
between containers, while Docker containerizer delegates container
management to the Docker engine.</p>
<p>Maintaining two containerizers is hard. For instance, when we add new
features to Mesos (e.g., persistent volumes, disk isolation), it
becomes a burden to update both containerizers. Even worse, sometimes
the isolation on some resources (e.g., network handles on an agent)
requires coordination between two containerizers, which is very hard
to implement in practice. In addition, we found that extending and
customizing isolation for containers launched by Docker engine is
difficult, mainly because we do not have a way to inject logics during
the life cycle of a container.</p>
<p>Therefore, we made an effort to unify containerizers in Mesos
(<a href="https://issues.apache.org/jira/browse/MESOS-2840">MESOS-2840</a>,
a.k.a. the Universal Containerizer). We improved Mesos containerizer
so that it now supports launching containers that specify container
images (e.g., Docker/Appc images).</p>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>To support container images, we introduced a new component in Mesos
containerizer, called image provisioner. Image provisioner is
responsible for pulling, caching and preparing container root
filesystems. It also extracts runtime configurations from container
images which will then be passed to the corresponding isolators for
proper isolation.</p>
<p>There are a few container image specifications, notably
<a href="https://github.com/docker/docker/blob/master/image/spec/v1.md">Docker</a>,
<a href="https://github.com/appc/spec/blob/master/SPEC.md">Appc</a>, and
<a href="https://github.com/opencontainers/specs">OCI</a> (future). Currently, we
support Docker and Appc images. More details about what features are
supported or not can be found in the following sections.</p>
<p><strong>NOTE</strong>: container image is only supported on Linux currently.</p>
<h3 id="configure-the-agent"><a class="header" href="#configure-the-agent">Configure the agent</a></h3>
<p>To enable container image support in Mesos containerizer, the operator
will need to specify the <code>--image_providers</code> agent flag which tells
Mesos containerizer what types of container images are allowed. For
example, setting <code>--image_providers=docker</code> allow containers to use
Docker images. The operators can also specify multiple container image
types. For instance, <code>--image_providers=docker,appc</code> allows both
Docker and Appc container images.</p>
<p>A few isolators need to be turned on in order to provide proper
isolation according to the runtime configurations specified in the
container image. The operator needs to add the following isolators to
the <code>--isolation</code> flag.</p>
<ul>
<li>
<p><code>filesystem/linux</code>: This is needed because supporting container
images involves changing filesystem root, and only <code>filesystem/linux</code>
support that currently. Note that this isolator requires root
permission.</p>
</li>
<li>
<p><code>docker/runtime</code>: This is used to provide support for runtime
configurations specified in Docker images (e.g., Entrypoint/Cmd,
environment variables, etc.). See more details about this isolator in
<a href="mesos-containerizer.html">Mesos containerizer doc</a>. Note that if this
isolator is not specified and <code>--image_providers</code> contains <code>docker</code>,
the agent will refuse to start.</p>
</li>
</ul>
<p>In summary, to enable container image support in Mesos containerizer,
please specify the following agent flags:</p>
<pre><code>$ sudo mesos-agent \
  --containerizers=mesos \
  --image_providers=appc,docker \
  --isolation=filesystem/linux,docker/runtime
</code></pre>
<h3 id="framework-api"><a class="header" href="#framework-api">Framework API</a></h3>
<p>We introduced a new protobuf message <code>Image</code> which allow frameworks to
specify container images for their containers. It has two types right
now: <code>APPC</code> and <code>DOCKER</code>, representing Appc and Docker images
respectively.</p>
<p>For Appc images, the <code>name</code> and <code>labels</code> are what described in the
<a href="https://github.com/appc/spec/blob/master/spec/aci.md#image-manifest-schema">spec</a>.</p>
<p>For Docker images, the <code>name</code> is the Docker image reference in the
following form (the same format expected by <code>docker pull</code>):
<code>[REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG|@DIGEST]</code></p>
<pre><code>message Image {
  enum Type {
    APPC = 1;
    DOCKER = 2;
  }

  message Appc {
    required string name = 1;
    optional Labels labels = 3;
  }

  message Docker {
    required string name = 1;
  }

  required Type type = 1;

  // Only one of the following image messages should be set to match
  // the type.
  optional Appc appc = 2;
  optional Docker docker = 3;
}
</code></pre>
<p>The framework needs to specify <code>MesosInfo</code> in <code>ContainerInfo</code> in order
to launch containers with container images. In other words, the
framework needs to set the type to <code>ContainerInfo.MESOS</code>, indicating
that it wants to use the Mesos containerizer. If <code>MesosInfo.image</code> is
not specified, the container will use the host filesystem. If
<code>MesosInfo.image</code> is specified, it will be used as the container
image when launching the container.</p>
<pre><code>message ContainerInfo {
  enum Type {
    DOCKER = 1;
    MESOS = 2;
  }

  message MesosInfo {
    optional Image image = 1;
  }

  required Type type = 1;
  optional MesosInfo mesos = 5;
}
</code></pre>
<h3 id="test-it-out"><a class="header" href="#test-it-out">Test it out!</a></h3>
<p>First, start the Mesos master:</p>
<pre><code>$ sudo sbin/mesos-master --work_dir=/tmp/mesos/master
</code></pre>
<p>Then, start the Mesos agent:</p>
<pre><code>$ sudo GLOG_v=1 sbin/mesos-agent \
  --master=&lt;MASTER_IP&gt;:5050 \
  --isolation=docker/runtime,filesystem/linux \
  --work_dir=/tmp/mesos/agent \
  --image_providers=docker \
  --executor_environment_variables=&quot;{}&quot;
</code></pre>
<p>Now, use Mesos CLI (i.e., mesos-execute) to launch a Docker container
(e.g., redis). Note that <code>--shell=false</code> tells Mesos to use the
default entrypoint and cmd specified in the Docker image.</p>
<pre><code>$ sudo bin/mesos-execute \
  --master=&lt;MASTER_IP&gt;:5050 \
  --name=test \
  --docker_image=library/redis \
  --shell=false
</code></pre>
<p>Verify if your container is running by launching a redis client:</p>
<pre><code>$ sudo docker run -ti --net=host redis redis-cli
127.0.0.1:6379&gt; ping
PONG
127.0.0.1:6379&gt;
</code></pre>
<h2 id="docker-support-and-current-limitations"><a class="header" href="#docker-support-and-current-limitations">Docker Support and Current Limitations</a></h2>
<p>Image provisioner uses <a href="https://docs.docker.com/registry/spec/api/">Docker v2 registry
API</a> to fetch Docker
images/layers. Both docker manifest
<a href="https://docs.docker.com/registry/spec/manifest-v2-1/">v2 schema1</a>
and <a href="https://docs.docker.com/registry/spec/manifest-v2-2/">v2 schema2</a>
are supported (v2 schema2 is supported starting from 1.8.0). The
fetching is based on <code>curl</code>, therefore SSL is automatically handled.
For private registries, the operator needs to configure <code>curl</code>
with the location of required CA certificates.</p>
<p>Fetching requiring authentication is supported through the
<code>--docker_config</code> agent flag. Starting from 1.0, operators can use
this agent flag to specify a shared docker config file, which is
used for pulling private repositories with authentication. Per
container credential is not supported yet (coming soon).</p>
<p>Operators can either specify the flag as an absolute path pointing to
the docker config file (need to manually configure
<code>.docker/config.json</code> or <code>.dockercfg</code> on each agent), or specify the
flag as a JSON-formatted string. See <a href="configuration/agent.html">configuration
documentation</a> for detail. For example:</p>
<pre><code>--docker_config=file:///home/vagrant/.docker/config.json
</code></pre>
<p>or as a JSON object,</p>
<pre><code>--docker_config=&quot;{ \
  \&quot;auths\&quot;: { \
    \&quot;https://index.docker.io/v1/\&quot;: { \
      \&quot;auth\&quot;: \&quot;xXxXxXxXxXx=\&quot;, \
      \&quot;email\&quot;: \&quot;username@example.com\&quot; \
    } \
  } \
}&quot;
</code></pre>
<p>Private registry is supported either through the <code>--docker_registry</code>
agent flag, or specifying private registry for each container using
image name <code>&lt;REGISTRY&gt;/&lt;REPOSITORY&gt;</code> (e.g.,
<code>localhost:80/gilbert/inky:latest</code>). If <code>&lt;REGISTRY&gt;</code> is included as
a prefix in the image name, the registry specified through the agent
flag <code>--docker_registry</code> will be ignored.</p>
<p>If the <code>--docker_registry</code> agent flag points to a local directory
(e.g., <code>/tmp/mesos/images/docker</code>), the provisioner will pull Docker
images from local filesystem, assuming Docker archives (result of
<code>docker save</code>) are stored there based on the image name and tag.  For
example, the operator can put a <code>busybox:latest.tar</code> (the result of
<code>docker save -o busybox:latest.tar busybox</code>) under
<code>/tmp/mesos/images/docker</code> and launch the agent by specifying
<code>--docker_registry=/tmp/mesos/images/docker</code>. Then the framework can
launch a Docker container by specifying <code>busybox:latest</code> as the name
of the Docker image. This flag can also point to an HDFS URI
(<em>experimental</em> in Mesos 1.7) (e.g., <code>hdfs://localhost:8020/archives/</code>)
to fetch images from HDFS if the <code>hadoop</code> command is available on the
agent.</p>
<p>If the <code>--switch_user</code> flag is set on the agent and the framework
specifies a user (either <code>CommandInfo.user</code> or <code>FrameworkInfo.user</code>),
we expect that user exists in the container image and its uid and gids
matches that on the host. User namespace is not supported yet. If the
user is not specified, <code>root</code> will be used by default. The operator or
the framework can limit the
<a href="http://man7.org/linux/man-pages/man7/capabilities.7.html">capabilities</a>
of the container by using the
<a href="isolators/linux-capabilities.html">linux/capabilities</a> isolator.</p>
<p>Currently, we support <code>host</code>, <code>bridge</code> and user defined networks
(<a href="https://docs.docker.com/engine/userguide/networking/">reference</a>).
<code>none</code> is not supported yet. We support the above networking modes in
<a href="mesos-containerizer.html">Mesos Containerizer</a> using the
<a href="https://github.com/containernetworking/cni">CNI</a> (Container Network
Interface) standard. Please refer to the <a href="cni.html">network/cni</a>
isolator document for more details about how to configure the network
for the container.</p>
<h3 id="more-agent-flags"><a class="header" href="#more-agent-flags">More agent flags</a></h3>
<p><code>--docker_registry</code>: The default URL for pulling Docker images. It
could either be a Docker registry server URL (i.e:
<code>https://registry.docker.io</code>), or a local path (i.e:
<code>/tmp/docker/images</code>) in which Docker image archives (result of
<code>docker save</code>) are stored. The default value is
<code>https://registry-1.docker.io</code>.</p>
<p><code>--docker_store_dir</code>: Directory the Docker provisioner will store
images in. All the Docker images are cached under this directory. The
default value is <code>/tmp/mesos/store/docker</code>.</p>
<p><code>--docker_config</code>: The default docker config file for agent. Can
be provided either as an absolute path pointing to the agent local
docker config file, or as a JSON-formatted string. The format of
the docker config file should be identical to docker's default one
(e.g., either <code>$HOME/.docker/config.json</code> or <code>$HOME/.dockercfg</code>).</p>
<h2 id="appc-support-and-current-limitations"><a class="header" href="#appc-support-and-current-limitations">Appc Support and Current Limitations</a></h2>
<p>Currently, only the root filesystem specified in the Appc image is
supported. Other runtime configurations like environment variables,
exec, working directory are not supported yet (coming soon).</p>
<p>For image discovery, we current support a simple discovery mechanism.
We allow operators to specify a URI prefix which will be prepend to
the URI template <code>{name}-{version}-{os}-{arch}.{ext}</code>. For example, if
the URI prefix is <code>file:///tmp/appc/</code> and the Appc image name is
<code>example.com/reduce-worker</code> with <code>version:1.0.0</code>, we will fetch the
image at <code>file:///tmp/appc/example.com/reduce-worker-1.0.0.aci</code>.</p>
<h3 id="more-agent-flags-1"><a class="header" href="#more-agent-flags-1">More agent flags</a></h3>
<p><code>appc_simple_discovery_uri_prefix</code>: URI prefix to be used for simple
discovery of appc images, e.g., <code>http://</code>, <code>https://</code>,
<code>hdfs://&lt;hostname&gt;:9000/user/abc/cde</code>. The default value is <code>http://</code>.</p>
<p><code>appc_store_dir</code>: Directory the appc provisioner will store images in.
All the Appc images are cached under this directory. The default value
is <code>/tmp/mesos/store/appc</code>.</p>
<h2 id="provisioner-backends"><a class="header" href="#provisioner-backends">Provisioner Backends</a></h2>
<p>A provisioner backend takes a set of filesystem layers and stacks them
into a root filesystem. Currently, we support the following backends:
<code>copy</code>, <code>bind</code>, <code>overlay</code> and <code>aufs</code>. Mesos will validate if the
selected backend works with the underlying filesystem (the filesystem
used by the image store <code>--docker_store_dir</code> or <code>--appc_store_dir</code>)
using the following logic table:</p>
<pre><code>+---------+--------------+------------------------------------------+
| Backend | Suggested on | Disabled on                              |
+---------+--------------+------------------------------------------+
| aufs    | ext4 xfs     | btrfs aufs eCryptfs                      |
| overlay | ext4 xfs*    | btrfs aufs overlay overlay2 zfs eCryptfs |
| bind    |              | N/A(`--sandbox_directory' must exist)    |
| copy    |              | N/A                                      |
+---------+--------------+------------------------------------------+
</code></pre>
<p>NOTE: <code>xfs</code> support on <code>overlay</code> is enabled only when <code>d_type=true</code>. Use
<code>xfs_info</code> to verify that the <code>xfs</code> ftype option is set to 1. To format
an xfs filesystem for <code>overlay</code>, use the flag <code>-n ftype=1</code> with <code>mkfs.xfs</code>.</p>
<p>The provisioner backend can be specified through the agent flag
<code>--image_provisioner_backend</code>. If not set, Mesos will select the best
backend automatically for the users/operators. The selection logic is
as following:</p>
<pre><code>1. Use `overlay` backend if the overlayfs is available.
2. Use `aufs` backend if the aufs is available and overlayfs is not supported.
3. Use `copy` backend if none of above is selected.
</code></pre>
<h3 id="copy"><a class="header" href="#copy">Copy</a></h3>
<p>The Copy backend simply copies all the layers into a target root
directory to create a root filesystem.</p>
<h3 id="bind"><a class="header" href="#bind">Bind</a></h3>
<p>This is a specialized backend that may be useful for deployments using
large (multi-GB) single-layer images <em>and</em> where more recent kernel
features such as overlayfs are not available. For small images (10's
to 100's of MB) the copy backend may be sufficient. Bind backend is
faster than Copy as it requires nearly zero IO.</p>
<p>The bind backend currently has these two limitations:</p>
<ol>
<li>
<p>The bind backend supports only a single layer. Multi-layer images will
fail to provision and the container will fail to launch!</p>
</li>
<li>
<p>The filesystem is read-only because all containers using this image
share the source. Select writable areas can be achieved by mounting
read-write volumes to places like <code>/tmp</code>, <code>/var/tmp</code>, <code>/home</code>, etc.
using the <code>ContainerInfo</code>. These can be relative to the executor work
directory. Since the filesystem is read-only, <code>--sandbox_directory</code>
and <code>/tmp</code> must already exist within the filesystem because the
filesystem isolator is unable to create it (e.g., either the image
writer needs to create the mount point in the image, or the operator
needs to set agent flag <code>--sandbox_directory</code> properly).</p>
</li>
</ol>
<h3 id="overlay"><a class="header" href="#overlay">Overlay</a></h3>
<p>The reason overlay backend was introduced is because the copy backend
will waste IO and space while the bind backend can only deal with one
layer. The overlay backend allows containizer to utilize the
filesystem to merge multiple filesystems into one efficiently.</p>
<p>The overlay backend depends on support for multiple lower layers,
which requires Linux kernel version 4.0 or later. For more information
of overlayfs, please refer to
<a href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt">here</a>.</p>
<h3 id="aufs"><a class="header" href="#aufs">AUFS</a></h3>
<p>The reason AUFS is introduced is because overlayfs support hasn't been
merged until kernel 3.18 and Docker's default storage backend for
ubuntu 14.04 is AUFS.</p>
<p>Like overlayfs, AUFS is also a unioned file system, which is very
stable, has a lot of real-world deployments, and has strong community
support.</p>
<p>Some Linux distributions do not support AUFS. This is usually because
AUFS is not included in the mainline (upstream) Linux kernel.</p>
<p>For more information of AUFS, please refer to
<a href="http://aufs.sourceforge.net/aufs2/man.html">here</a>.</p>
<h2 id="executor-dependencies-in-a-container-image"><a class="header" href="#executor-dependencies-in-a-container-image">Executor Dependencies in a Container Image</a></h2>
<p>Mesos has this concept of executors. All tasks are launched by an
executor. For a general purpose executor (e.g., thermos) of a
framework (e.g., Aurora), requiring it and all its dependencies to be
present in all possible container images that a user might use is
not trivial.</p>
<p>In order to solve this issue, we propose a solution where we allow the
executor to run on the host filesystem (without a container image).
Instead, it can specify a <code>volume</code> whose source is an <code>Image</code>. Mesos
containerizer will provision the <code>image</code> specified in the <code>volume</code>,
and mount it under the sandbox directory. The executor can perform
<code>pivot_root</code> or <code>chroot</code> itself to enter the container root
filesystem.</p>
<h2 id="garbage-collect-unused-container-images"><a class="header" href="#garbage-collect-unused-container-images">Garbage Collect Unused Container Images</a></h2>
<p>Experimental support of garbage-collecting unused container images was added at
Mesos 1.5. This can be either configured automatically via a new agent flag
<code>--image_gc_config</code>, or manually invoked through agent's
<a href="operator-http-api.html#prune_images">v1 Operator HTTP API</a>. This can be used
to avoid unbounded disk space usage of image stores.</p>
<p>This is implemented with a simple mark-and-sweep logic. When image GC happens,
we check all layers and images referenced by active running containers and avoid
removing them from the image store. As a pre-requisite, if there are active
containers launched before Mesos 1.5.0, we cannot determine what images can be
safely garbage collected, so agent will refuse to invoke image GC. To garbage
collect container images, users are expected to drain all containers launched
before Mesos 1.5.0.</p>
<p><strong>NOTE</strong>: currently, the image GC is only supported for docker store in Mesos
Containerizer.</p>
<h3 id="automatic-image-gc-through-agent-flag"><a class="header" href="#automatic-image-gc-through-agent-flag">Automatic Image GC through Agent Flag</a></h3>
<p>To enable automatic image GC, use the new agent flag <code>--image_gc_config</code>:</p>
<pre><code>--image_gc_config=file:///home/vagrant/image-gc-config.json
</code></pre>
<p>or as a JSON object,</p>
<pre><code>--image_gc_config=&quot;{ \
  \&quot;image_disk_headroom\&quot;: 0.1, \
  \&quot;image_disk_watch_interval\&quot;: { \
    \&quot;nanoseconds\&quot;: 3600000000000 \
    }, \
  \&quot;excluded_images\&quot;: \[ \] \
}&quot;
</code></pre>
<h3 id="manual-image-gc-through-http-api"><a class="header" href="#manual-image-gc-through-http-api">Manual Image GC through HTTP API</a></h3>
<p>See <code>PRUNE_IMAGES</code> section in
<a href="operator-http-api.html#prune_images">v1 Operator HTTP API</a> for manual image GC
through the agent HTTP API.</p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<p>For more information on the Mesos containerizer filesystem, namespace,
and isolator features, visit <a href="mesos-containerizer.html">Mesos
Containerizer</a>.  For more information on
launching Docker containers through the Docker containerizer, visit
<a href="docker-containerizer.html">Docker Containerizer</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Docker Volume Support in Mesos Containerizer
layout: documentation</h2>
<h1 id="docker-volume-support-in-mesos-containerizer"><a class="header" href="#docker-volume-support-in-mesos-containerizer">Docker Volume Support in Mesos Containerizer</a></h1>
<p>Mesos 1.0 adds Docker volume support to the
<a href="isolators/../mesos-containerizer.html">MesosContainerizer</a> (a.k.a., the universal
containerizer) by introducing the new <code>docker/volume</code> isolator.</p>
<p>This document describes the motivation, overall architecture, configuration
steps for enabling Docker volume isolator, and required framework changes.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="isolators/docker-volume.html#motivation">Motivation</a></li>
<li><a href="isolators/docker-volume.html#how-does-it-work">How does it work?</a></li>
<li><a href="isolators/docker-volume.html#configuration">Configuration</a>
<ul>
<li><a href="isolators/docker-volume.html#pre-conditions">Pre-conditions</a></li>
<li><a href="isolators/docker-volume.html#configure-Docker-volume-isolator">Configuring Docker Volume Isolator</a></li>
<li><a href="isolators/docker-volume.html#enable-frameworks">Enabling frameworks to use Docker volumes</a>
<ul>
<li><a href="isolators/docker-volume.html#volume-protobuf">Volume Protobuf</a></li>
<li><a href="isolators/docker-volume.html#examples">Examples</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="isolators/docker-volume.html#limitations">Limitations</a></li>
<li><a href="isolators/docker-volume.html#test-it-out">Test it out!</a></li>
</ul>
<h2 id="motivation-2"><a class="header" href="#motivation-2"><a name="motivation"></a>Motivation</a></h2>
<p>The integration of external storage in Mesos is an attractive feature.  The
Mesos <a href="isolators/../persistent-volume.html">persistent volume</a> primitives allow stateful
services to persist data on an agent's local storage. However, the amount of
storage capacity that can be directly attached to a single agent is
limited---certain applications (e.g., databases) would like to access more data
than can easily be attached to a single node. Using external storage can
also simplify data migration between agents/containers, and can make backups and
disaster recovery easier.</p>
<p>The <a href="https://github.com/Docker/Docker/blob/master/docs/extend/plugins_volume.md">Docker Volume Driver
API</a>
defines an interface between the container runtime and external storage systems.
It has been widely adopted. There are Docker volume plugins for a variety of
storage drivers, such as <a href="https://github.com/rancher/convoy">Convoy</a>,
<a href="https://docs.clusterhq.com/en/latest/Docker-integration/">Flocker</a>,
<a href="https://github.com/calavera/Docker-volume-glusterfs">GlusterFS</a>, and
<a href="https://github.com/emccode/rexray">REX-Ray</a>. Each plugin typically supports a
variety of external storage systems, such as Amazon EBS, OpenStack Cinder, etc.</p>
<p>Therefore, introducing support for external storage in Mesos through the
<code>docker/volume</code> isolator provides Mesos with tremendous flexibility to
orchestrate containers on a wide variety of external storage technologies.</p>
<h2 id="how-does-it-work-2"><a class="header" href="#how-does-it-work-2"><a name="how-does-it-work"></a>How does it work?</a></h2>
<p><img src="isolators/images/docker-volume-isolator.png" alt="Docker Volume Isolator Architecture" /></p>
<p>The <code>docker/volume</code> isolator interacts with Docker volume plugins using
<a href="https://github.com/emccode/dvdcli">dvdcli</a>, an open-source command line tool
from EMC.</p>
<p>When a new task with Docker volumes is launched, the <code>docker/volume</code> isolator
will invoke <a href="https://github.com/emccode/dvdcli">dvdcli</a> to mount the
corresponding Docker volume onto the host and then onto the container.</p>
<p>When the task finishes or is killed, the <code>docker/volume</code> isolator will invoke
<a href="https://github.com/emccode/dvdcli">dvdcli</a> to unmount the corresponding Docker
volume.</p>
<p>The detailed workflow for the <code>docker/volume</code> isolator is as follows:</p>
<ol>
<li>
<p>A framework specifies external volumes in <code>ContainerInfo</code> when launching a
task.</p>
</li>
<li>
<p>The master sends the launch task message to the agent.</p>
</li>
<li>
<p>The agent receives the message and asks all isolators (including the
<code>docker/volume</code> isolator) to prepare for the container with the
<code>ContainerInfo</code>.</p>
</li>
<li>
<p>The isolator invokes <a href="https://github.com/emccode/dvdcli">dvdcli</a> to mount the
corresponding external volume to a mount point on the host.</p>
</li>
<li>
<p>The agent launches the container and bind-mounts the volume into the
container.</p>
</li>
<li>
<p>The bind-mounted volume inside the container will be unmounted from the
container automatically when the container finishes, as the container is in
its own mount namespace.</p>
</li>
<li>
<p>The agent invokes isolator cleanup which invokes
<a href="https://github.com/emccode/dvdcli">dvdcli</a> to unmount all mount points for
the container.</p>
</li>
</ol>
<h2 id="configuration-1"><a class="header" href="#configuration-1"><a name="configuration"></a>Configuration</a></h2>
<p>To use the <code>docker/volume</code> isolator, there are certain actions required by
operators and framework developers. In this section we list the steps required
by the operator to configure <code>docker/volume</code> isolator and the steps required by
framework developers to specify the Docker volumes.</p>
<h3 id="pre-conditions"><a class="header" href="#pre-conditions"><a name="pre-conditions"></a>Pre-conditions</a></h3>
<ul>
<li>
<p>Install <code>dvdcli</code> version
<a href="https://github.com/emccode/dvdcli/releases/tag/v0.1.0">0.1.0</a> on each agent.</p>
</li>
<li>
<p>Install the <a href="https://github.com/Docker/Docker/blob/master/docs/extend/plugins.md#volume-plugins">Docker volume
plugin</a>
on each agent.</p>
</li>
<li>
<p>Explicitly create the Docker volumes that are going to be accessed by Mesos
tasks. If this is not done, volumes will be implicitly created by
<a href="https://github.com/emccode/dvdcli">dvdcli</a> but the volumes may not fit into
framework resource requirement well.</p>
</li>
</ul>
<h3 id="configuring-docker-volume-isolator"><a class="header" href="#configuring-docker-volume-isolator"><a name="configure-Docker-volume-isolator"></a>Configuring Docker Volume Isolator</a></h3>
<p>In order to configure the <code>docker/volume</code> isolator, the operator needs to
configure two flags at agent startup as follows:</p>
<pre><code class="language-{.console}">  sudo mesos-agent \
    --master=&lt;master IP&gt; \
    --ip=&lt;agent IP&gt; \
    --work_dir=/var/lib/mesos \
    --isolation=filesystem/linux,docker/volume \
    --docker_volume_checkpoint_dir=&lt;mount info checkpoint path&gt;
</code></pre>
<p>The <code>docker/volume</code> isolator must be specified in the <code>--isolation</code> flag at
agent startup; the <code>docker/volume</code> isolator has a dependency on the
<code>filesystem/linux</code> isolator.</p>
<p>The <code>--docker_volume_checkpoint_dir</code> is an optional flag with a default value of
<code>/var/run/mesos/isolators/docker/volume</code>. The <code>docker/volume</code> isolator will
checkpoint all Docker volume mount point information under
<code>--docker_volume_checkpoint_dir</code> for recovery. The checkpoint information under
the default <code>--docker_volume_checkpoint_dir</code> will be cleaned up after agent
restart. Therefore, it is recommended to set <code>--docker_volume_checkpoint_dir</code> to
a directory which will survive agent restart.</p>
<h3 id="enabling-frameworks-to-use-docker-volumes"><a class="header" href="#enabling-frameworks-to-use-docker-volumes"><a name="enable-frameworks"></a>Enabling frameworks to use Docker volumes</a></h3>
<h4 id="volume-protobuf"><a class="header" href="#volume-protobuf"><a name="volume-protobuf"></a>Volume Protobuf</a></h4>
<p>The <code>Volume</code> protobuf message has been updated to support Docker volumes.</p>
<pre><code class="language-{.proto}">message Volume {
  ...

  required string container_path = 1;

  message Source {
    enum Type {
      UNKNOWN = 0;
      DOCKER_VOLUME = 1;
    }

    message DockerVolume {
      optional string driver = 1;
      required string name = 2;
      optional Parameters driver_options = 3;
    }

    optional Type type = 1;
    optional DockerVolume docker_volume = 2;
  }

  optional Source source = 5;
}
</code></pre>
<p>When requesting a Docker volume for a container, the framework developer needs to
set <code>Volume</code> for the container, which includes <code>mode</code>, <code>container_path</code> and
<code>source</code>.</p>
<p>The <code>source</code> field specifies where the volume comes from. Framework developers need to
specify the <code>type</code>, Docker volume <code>driver</code>, <code>name</code> and <code>options</code>. At present,
only the <code>DOCKER_VOLUME</code> type is supported; we plan to add support for more
types of volumes in the future.</p>
<p>How to specify <code>container_path</code>:</p>
<ol>
<li>
<p>If you are launching a Mesos container <code>without rootfs</code>. If <code>container_path</code>
is an absolute path, you need to make sure the absolute path exists on your
host root file system as the container shares the host root file system;
otherwise, the task will fail.</p>
</li>
<li>
<p>For other cases like launching a Mesos container <code>without rootfs</code> and
<code>container_path</code> is a relative path, or launching a task <code>with rootfs</code> and
<code>container_path</code> is an absolute path, or launching a task <code>with rootfs</code> and
<code>container_path</code> as a relative path, the isolator will help create the
<code>container_path</code> as the mount point.</p>
</li>
</ol>
<p>The following table summarizes the above rules for <code>container_path</code>:</p>
<table class="table table-striped">
  <tr>
    <th></th>
    <th>Container with rootfs</th>
    <th>Container without rootfs</th>
  </tr>
  <tr>
    <td>Absolute container_path</td>
    <td>No need to exist</td>
    <td>Must exist</td>
  </tr>
  <tr>
    <td>Relative container_path</td>
    <td>No need to exist</td>
    <td>No need to exist</td>
  </tr>
</table>
<h4 id="examples-5"><a class="header" href="#examples-5"><a name="examples"></a>Examples</a></h4>
<ol>
<li>
<p>Launch a task with one Docker volume using the default command executor.</p>
<pre><code class="language-{.json}">TaskInfo {
  ...
  &quot;command&quot; : ...,
  &quot;container&quot; : {
    &quot;volumes&quot; : [
      {
        &quot;container_path&quot; : &quot;/mnt/volume&quot;,
        &quot;mode&quot; : &quot;RW&quot;,
        &quot;source&quot; : {
          &quot;type&quot; : &quot;DOCKER_VOLUME&quot;,
          &quot;docker_volume&quot; : {
            &quot;driver&quot; : &quot;rexray&quot;,
            &quot;name&quot; : &quot;myvolume&quot;
          }
        }
      }
    ]
  }
}
</code></pre>
</li>
<li>
<p>Launch a task with two Docker volumes using the default command executor.</p>
<pre><code class="language-{.json}">TaskInfo {
  ...
  &quot;command&quot; : ...,
  &quot;container&quot; : {
    &quot;volumes&quot; : [
      {
        &quot;container_path&quot; : &quot;volume1&quot;,
        &quot;mode&quot; : &quot;RW&quot;,
        &quot;source&quot; : {
          &quot;type&quot; : &quot;DOCKER_VOLUME&quot;,
          &quot;docker_volume&quot; : {
            &quot;driver&quot; : &quot;rexray&quot;,
            &quot;name&quot; : &quot;volume1&quot;
          }
        }
      },
      {
        &quot;container_path&quot; : &quot;volume2&quot;,
        &quot;mode&quot; : &quot;RW&quot;,
        &quot;source&quot; : {
          &quot;type&quot; : &quot;DOCKER_VOLUME&quot;,
          &quot;docker_volume&quot; : {
            &quot;driver&quot; : &quot;rexray&quot;,
            &quot;name&quot; : &quot;volume2&quot;,
            &quot;driver_options&quot; : {
              &quot;parameter&quot; : [{
                &quot;key&quot; : &lt;key&gt;,
                &quot;value&quot; : &lt;value&gt;
              }, {
                &quot;key&quot; : &lt;key&gt;,
                &quot;value&quot; : &lt;value&gt;
              }]
            }
          }
        }
      }
    ]
  }
}
</code></pre>
</li>
</ol>
<p><strong>NOTE</strong>: The task launch will be failed if one container uses multiple Docker
volumes with the same <code>driver</code> and <code>name</code>.</p>
<h2 id="limitations"><a class="header" href="#limitations"><a name="limitations"></a>Limitations</a></h2>
<p>Using the same Docker volume in both the <a href="isolators/../docker-containerizer.html">Docker
Containerizer</a> and the <a href="isolators/../mesos-containerizer.html">Mesos
Containerizer</a> simultaneously is <strong>strongly
discouraged</strong>, because the MesosContainerizer has its own reference
counting to decide when to unmount a Docker volume. Otherwise, it
would be problematic if a Docker volume is unmounted by
MesosContainerizer but the DockerContainerizer is still using it.</p>
<h2 id="test-it-out-1"><a class="header" href="#test-it-out-1"><a name="test-it-out"></a>Test it out!</a></h2>
<p>This section presents examples for launching containers with Docker volumes.
The following example is using <a href="https://github.com/rancher/convoy/">convoy</a>
as the Docker volume driver.</p>
<p>Start the Mesos master.</p>
<pre><code class="language-{.console}">  $ sudo mesos-master --work_dir=/tmp/mesos/master
</code></pre>
<p>Start the Mesos agent.</p>
<pre><code class="language-{.console}">  $ sudo mesos-agent \
    --master=&lt;MASTER_IP&gt;:5050 \
    --isolation=docker/volume,docker/runtime,filesystem/linux \
    --work_dir=/tmp/mesos/agent \
    --image_providers=docker \
    --executor_environment_variables=&quot;{}&quot;
</code></pre>
<p>Create a volume named as <code>myvolume</code> with
<a href="https://github.com/rancher/convoy/">convoy</a>.</p>
<pre><code class="language-{.console}">  $ convoy create myvolume
</code></pre>
<p>Prepare a volume json file named as <code>myvolume.json</code> with following content.</p>
<pre><code>  [{
    &quot;container_path&quot;:&quot;\/tmp\/myvolume&quot;,
    &quot;mode&quot;:&quot;RW&quot;,
    &quot;source&quot;:
    {
      &quot;docker_volume&quot;:
        {
          &quot;driver&quot;:&quot;convoy&quot;,
          &quot;name&quot;:&quot;myvolume&quot;
        },
        &quot;type&quot;:&quot;DOCKER_VOLUME&quot;
    }
  }]
</code></pre>
<p>Now, use Mesos CLI (i.e., mesos-execute) to launch a Docker container with
<code>--volumes=&lt;path&gt;/myvolume.json</code> option.</p>
<pre><code class="language-{.console}">  $ sudo mesos-execute \
    --master=&lt;MASTER_IP&gt;:5050 \
    --name=test \
    --docker_image=ubuntu:14.04 \
    --command=&quot;touch /tmp/myvolume/myfile&quot; \
    --volumes=&lt;path&gt;/myvolume.json
</code></pre>
<p>Create another task to verify the file <code>myfile</code> was created successfully.</p>
<pre><code class="language-{.console}">  $ sudo mesos-execute \
    --master=&lt;MASTER_IP&gt;:5050 \
    --name=test \
    --docker_image=ubuntu:14.04 \
    --command=&quot;ls /tmp/myvolume&quot; \
    --volumes=&lt;path&gt;/myvolume.json
</code></pre>
<p>Check the <a href="isolators/../sandbox.html#where-is-it">sandbox</a>
for the second task to check the file <code>myfile</code> was created successfully.</p>
<pre><code class="language-{.console}">  $ cat stdout
    Received SUBSCRIBED event
    Subscribed executor on mesos002
    Received LAUNCH event
    Starting task test
    Forked command at 27288
    sh -c 'ls /tmp/myvolume/'
    lost+found
    myfile
    Command exited with status 0 (pid: 27288)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Nvidia GPU Support
layout: documentation</h2>
<h1 id="nvidia-gpu-support"><a class="header" href="#nvidia-gpu-support">Nvidia GPU Support</a></h1>
<p>Mesos 1.0.0 added first-class support for Nvidia GPUs.
The minimum required Nvidia driver version is <code>340.29</code>.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Getting up and running with GPU support in Mesos is fairly
straightforward once you know the steps necessary to make it work as
expected. On one side, this includes setting the necessary agent flags
to enumerate GPUs and advertise them to the Mesos master. On the other
side, this includes setting the proper framework capabilities so that
the Mesos master will actually include GPUs in the resource offers it
sends to a framework. So long as all of these constraints are met,
accepting offers that contain GPUs and launching tasks that consume
them should be just as straightforward as launching a traditional task
that only consumes CPUs, memory, and disk.</p>
<p>Mesos exposes GPUs as a simple <code>SCALAR</code> resource in the same
way it always has for CPUs, memory, and disk. That is, a resource
offer such as the following is now possible:</p>
<pre><code>cpus:8; mem:1024; disk:65536; gpus:4;
</code></pre>
<p>However, unlike CPUs, memory, and disk, <em>only</em> whole numbers of GPUs
can be selected. If a fractional amount is selected, launching the
task will result in a <code>TASK_ERROR</code>.</p>
<p>At the time of this writing, Nvidia GPU support is only available for
tasks launched through the Mesos containerizer (i.e., no support exists
for launching GPU capable tasks through the Docker containerizer).
That said, the Mesos containerizer now supports running docker
images natively, so this limitation should not affect most users.</p>
<p>Moreover, we mimic the support provided by <a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-driver">nvidia-docker</a> to
automatically mount the proper Nvidia drivers and tools directly into
your docker container. This means you can easily test your GPU-enabled
docker containers locally and deploy them to Mesos with the assurance
that they will work without modification.</p>
<p>In the following sections we walk through all of the flags and
framework capabilities necessary to enable Nvidia GPU support in
Mesos. We then show an example of setting up and running an example
test cluster that launches tasks both with and without docker
containers. Finally, we conclude with a step-by-step guide of how to
install any necessary Nvidia GPU drivers on your machine.</p>
<h2 id="agent-flags-1"><a class="header" href="#agent-flags-1">Agent Flags</a></h2>
<p>The following isolation flags are required to enable Nvidia GPU
support on an agent.</p>
<pre><code>--isolation=&quot;filesystem/linux,cgroups/devices,gpu/nvidia&quot;
</code></pre>
<p>The <code>filesystem/linux</code> flag tells the agent to use Linux-specific
commands to prepare the root filesystem and volumes (e.g., persistent
volumes) for containers that require them. Specifically, it relies on
Linux mount namespaces to prevent the mounts of a container from being
propagated to the host mount table. In the case of GPUs, we require
this flag to properly mount certain Nvidia binaries (e.g.,
<code>nvidia-smi</code>) and libraries (e.g., <code>libnvidia-ml.so</code>) into a container
when necessary.</p>
<p>The <code>cgroups/devices</code> flag tells the agent to restrict access to a
specific set of devices for each task that it launches (i.e., a subset
of all devices listed in <code>/dev</code>). When used in conjunction with the
<code>gpu/nvidia</code> flag, the <code>cgroups/devices</code> flag allows us to grant /
revoke access to specific GPUs on a per-task basis.</p>
<p>By default, all GPUs on an agent are automatically discovered and sent
to the Mesos master as part of its resource offer. However, it may
sometimes be necessary to restrict access to only a subset of the GPUs
available on an agent. This is useful, for example, if you want to
exclude a specific GPU device because an unwanted Nvidia graphics card
is listed alongside a more powerful set of GPUs. When this is
required, the following additional agent flags can be used to
accomplish this:</p>
<pre><code>--nvidia_gpu_devices=&quot;&lt;list_of_gpu_ids&gt;&quot;

--resources=&quot;gpus:&lt;num_gpus&gt;&quot;
</code></pre>
<p>For the <code>--nvidia_gpu_devices</code> flag, you need to provide a comma
separated list of GPUs, as determined by running <code>nvidia-smi</code> on the
host where the agent is to be launched (<a href="gpu-support.html#external-dependencies">see
below</a> for instructions on what external
dependencies must be installed on these hosts to run this command).
Example output from running <code>nvidia-smi</code> on a machine with four GPUs
can be seen below:</p>
<pre><code>+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla M60           Off  | 0000:05:00.0     Off |                    0 |
| N/A   35C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla M60           Off  | 0000:83:00.0     Off |                    0 |
| N/A   38C    P0    40W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla M60           Off  | 0000:84:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |     97%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<p>The GPU <code>id</code> to choose can be seen in the far left of each row. Any
subset of these <code>ids</code> can be listed in the <code>--nvidia_gpu_devices</code>
flag (i.e., all of the following values of this flag are valid):</p>
<pre><code>--nvidia_gpu_devices=&quot;0&quot;
--nvidia_gpu_devices=&quot;0,1&quot;
--nvidia_gpu_devices=&quot;0,1,2&quot;
--nvidia_gpu_devices=&quot;0,1,2,3&quot;
--nvidia_gpu_devices=&quot;0,2,3&quot;
--nvidia_gpu_devices=&quot;3,1&quot;
etc...
</code></pre>
<p>For the <code>--resources=gpus:&lt;num_gpus&gt;</code> flag, the value passed to
<code>&lt;num_gpus&gt;</code> must equal the number of GPUs listed in
<code>--nvidia_gpu_devices</code>. If these numbers do not match, launching the
agent will fail. This can sometimes be a source of confusion, so it
is important to emphasize it here for clarity.</p>
<h2 id="framework-capabilities"><a class="header" href="#framework-capabilities">Framework Capabilities</a></h2>
<p>Once you launch an agent with the flags above, GPU resources will be
advertised to the Mesos master along side all of the traditional
resources such as CPUs, memory, and disk. However, the master will
only forward offers that contain GPUs to frameworks that have
explicitly enabled the <code>GPU_RESOURCES</code> framework capability.</p>
<p>The choice to make frameworks explicitly opt-in to this <code>GPU_RESOURCES</code>
capability was to keep legacy frameworks from accidentally consuming
non-GPU resources on GPU-capable machines (and thus preventing your GPU
jobs from running). It's not that big a deal if all of your nodes have
GPUs, but in a mixed-node environment, it can be a big problem.</p>
<p>An example of setting this capability in a C++-based framework can be
seen below:</p>
<pre><code>FrameworkInfo framework;
framework.add_capabilities()-&gt;set_type(
      FrameworkInfo::Capability::GPU_RESOURCES);

GpuScheduler scheduler;

driver = new MesosSchedulerDriver(
    &amp;scheduler,
    framework,
    127.0.0.1:5050);

driver-&gt;run();
</code></pre>
<h2 id="minimal-gpu-capable-cluster"><a class="header" href="#minimal-gpu-capable-cluster">Minimal GPU Capable Cluster</a></h2>
<p>In this section we walk through two examples of configuring GPU-capable
clusters and running tasks on them. The first example demonstrates the
minimal setup required to run a command that consumes GPUs on a GPU-capable
agent. The second example demonstrates the setup necessary to
launch a docker container that does the same.</p>
<p><strong>Note</strong>: Both of these examples assume you have installed the
external dependencies required for Nvidia GPU support on Mesos. Please
see <a href="gpu-support.html#external-dependencies">below</a> for more information.</p>
<h3 id="minimal-setup-without-support-for-docker-containers"><a class="header" href="#minimal-setup-without-support-for-docker-containers">Minimal Setup Without Support for Docker Containers</a></h3>
<p>The commands below show a minimal example of bringing up a GPU-capable
Mesos cluster on <code>localhost</code> and executing a task on it. The required
agent flags are set as described above, and the <code>mesos-execute</code>
command has been told to enable the <code>GPU_RESOURCES</code> framework
capability so it can receive offers containing GPU resources.</p>
<pre><code>$ mesos-master \
      --ip=127.0.0.1 \
      --work_dir=/var/lib/mesos

$ mesos-agent \
      --master=127.0.0.1:5050 \
      --work_dir=/var/lib/mesos \
      --isolation=&quot;cgroups/devices,gpu/nvidia&quot;

$ mesos-execute \
      --master=127.0.0.1:5050 \
      --name=gpu-test \
      --command=&quot;nvidia-smi&quot; \
      --framework_capabilities=&quot;GPU_RESOURCES&quot; \
      --resources=&quot;gpus:1&quot;
</code></pre>
<p>If all goes well, you should see something like the following in the
<code>stdout</code> out of your task:</p>
<pre><code>+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<h3 id="minimal-setup-with-support-for-docker-containers"><a class="header" href="#minimal-setup-with-support-for-docker-containers">Minimal Setup With Support for Docker Containers</a></h3>
<p>The commands below show a minimal example of bringing up a GPU-capable
Mesos cluster on <code>localhost</code> and running a docker container on it. The
required agent flags are set as described above, and the
<code>mesos-execute</code> command has been told to enable the <code>GPU_RESOURCES</code>
framework capability so it can receive offers containing GPU
resources.  Additionally, the required flags to enable support for
docker containers (as described <a href="container-image.html">here</a>) have been
set up as well.</p>
<pre><code>$ mesos-master \
      --ip=127.0.0.1 \
      --work_dir=/var/lib/mesos

$ mesos-agent \
      --master=127.0.0.1:5050 \
      --work_dir=/var/lib/mesos \
      --image_providers=docker \
      --executor_environment_variables=&quot;{}&quot; \
      --isolation=&quot;docker/runtime,filesystem/linux,cgroups/devices,gpu/nvidia&quot;

$ mesos-execute \
      --master=127.0.0.1:5050 \
      --name=gpu-test \
      --docker_image=nvidia/cuda \
      --command=&quot;nvidia-smi&quot; \
      --framework_capabilities=&quot;GPU_RESOURCES&quot; \
      --resources=&quot;gpus:1&quot;
</code></pre>
<p>If all goes well, you should see something like the following in the
<code>stdout</code> out of your task.</p>
<pre><code>+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
</code></pre>
<p><a name="external-dependencies"></a></p>
<h2 id="external-dependencies"><a class="header" href="#external-dependencies">External Dependencies</a></h2>
<p>Any host running a Mesos agent with Nvidia GPU support <strong>MUST</strong> have a
valid Nvidia kernel driver installed. It is also <em>highly</em> recommended to
install the corresponding user-level libraries and tools available as
part of the Nvidia CUDA toolkit. Many jobs that use Nvidia GPUs rely
on CUDA and not including it will severely limit the type of
GPU-aware jobs you can run on Mesos.</p>
<p><strong>Note:</strong> The minimum supported version of CUDA is <code>6.5</code>.</p>
<h3 id="installing-the-required-tools"><a class="header" href="#installing-the-required-tools">Installing the Required Tools</a></h3>
<p>The Nvidia kernel driver can be downloaded at the link below. Make
sure to choose the proper model of GPU, operating system, and CUDA
toolkit you plan to install on your host:</p>
<pre><code>http://www.nvidia.com/Download/index.aspx
</code></pre>
<p>Unfortunately, most Linux distributions come preinstalled with an open
source video driver called <code>Nouveau</code>. This driver conflicts with the
Nvidia driver we are trying to install. The following guides may prove
useful to help guide you through the process of uninstalling <code>Nouveau</code>
before installing the Nvidia driver on CentOS or Ubuntu.</p>
<pre><code>http://www.dedoimedo.com/computers/centos-7-nvidia.html
http://www.allaboutlinux.eu/remove-nouveau-and-install-nvidia-driver-in-ubuntu-15-04/
</code></pre>
<p>After installing the Nvidia kernel driver, you can follow the
instructions in the link below to install the Nvidia CUDA toolkit:</p>
<pre><code>http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/
</code></pre>
<p>In addition to the steps listed in the link above, it is <em>highly</em>
recommended to add CUDA's <code>lib</code> directory into your <code>ldcache</code> so that
tasks launched by Mesos will know where these libraries exist and link
with them properly.</p>
<pre><code>sudo bash -c &quot;cat &gt; /etc/ld.so.conf.d/cuda-lib64.conf &lt;&lt; EOF
/usr/local/cuda/lib64
EOF&quot;

sudo ldconfig
</code></pre>
<p>If you choose <strong>not</strong> to add CUDAs <code>lib</code> directory to your <code>ldcache</code>,
you <strong>MUST</strong> add it to every task's <code>LD_LIBRARY_PATH</code> that requires
it.</p>
<p><strong>Note:</strong> This is <em>not</em> the recommended method. You have been warned.</p>
<h3 id="verifying-the-installation"><a class="header" href="#verifying-the-installation">Verifying the Installation</a></h3>
<p>Once the kernel driver has been installed, you can make sure
everything is working by trying to run the bundled <code>nvidia-smi</code> tool.</p>
<pre><code>nvidia-smi
</code></pre>
<p>You should see output similar to the following:</p>
<pre><code>Thu Apr 14 11:58:17 2016
+------------------------------------------------------+
| NVIDIA-SMI 352.79     Driver Version: 352.79         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M60           Off  | 0000:04:00.0     Off |                    0 |
| N/A   34C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla M60           Off  | 0000:05:00.0     Off |                    0 |
| N/A   35C    P0    39W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla M60           Off  | 0000:83:00.0     Off |                    0 |
| N/A   38C    P0    38W / 150W |     34MiB /  7679MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla M60           Off  | 0000:84:00.0     Off |                    0 |
| N/A   34C    P0    38W / 150W |     34MiB /  7679MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>To verify your CUDA installation, it is recommended to go through the instructions at the link below:</p>
<pre><code>http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/#install-samples
</code></pre>
<p>Finally, you should get a developer to run Mesos's Nvidia GPU-related
unit tests on your machine to ensure that everything passes (as
described below).</p>
<h3 id="running-mesos-unit-tests"><a class="header" href="#running-mesos-unit-tests">Running Mesos Unit Tests</a></h3>
<p>At the time of this writing, the following Nvidia GPU specific unit
tests exist on Mesos:</p>
<pre><code>DockerTest.ROOT_DOCKER_NVIDIA_GPU_DeviceAllow
DockerTest.ROOT_DOCKER_NVIDIA_GPU_InspectDevices
NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_VerifyDeviceAccess
NvidiaGpuTest.ROOT_INTERNET_CURL_CGROUPS_NVIDIA_GPU_NvidiaDockerImage
NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_FractionalResources
NvidiaGpuTest.NVIDIA_GPU_Discovery
NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_FlagValidation
NvidiaGpuTest.NVIDIA_GPU_Allocator
NvidiaGpuTest.ROOT_NVIDIA_GPU_VolumeCreation
NvidiaGpuTest.ROOT_NVIDIA_GPU_VolumeShouldInject)
</code></pre>
<p>The capitalized words following the <code>'.'</code> specify test filters to
apply when running the unit tests. In our case the filters that apply
are <code>ROOT</code>, <code>CGROUPS</code>, and <code>NVIDIA_GPU</code>. This means that these tests
must be run as <code>root</code> on Linux machines with <code>cgroups</code> support that
have Nvidia GPUs installed on them. The check to verify that Nvidia
GPUs exist is to look for the existence of the Nvidia System
Management Interface (<code>nvidia-smi</code>) on the machine where the tests are
being run. This binary should already be installed if the instructions
above have been followed correctly.</p>
<p>So long as these filters are satisfied, you can run the following to
execute these unit tests:</p>
<pre><code>[mesos]$ GTEST_FILTER=&quot;&quot; make -j check
[mesos]$ sudo bin/mesos-tests.sh --gtest_filter=&quot;*NVIDIA_GPU*&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Sandbox
layout: documentation</h2>
<h1 id="mesos-sandbox"><a class="header" href="#mesos-sandbox">Mesos &quot;Sandbox&quot;</a></h1>
<p>Mesos refers to the &quot;sandbox&quot; as a temporary directory that holds files specific
to a single executor.  Each time an executor is run, the executor is given its
own sandbox and the executor's working directory is set to the sandbox.</p>
<h2 id="sandbox-files"><a class="header" href="#sandbox-files">Sandbox files</a></h2>
<p>The sandbox holds:</p>
<ul>
<li>Files <a href="fetcher.html">fetched by Mesos</a>, prior to starting
the executor's tasks.</li>
<li>The output of the executor and tasks (as files &quot;stdout&quot; and &quot;stderr&quot;).</li>
<li>Files created by the executor and tasks, with some exceptions.</li>
</ul>
<p><strong>NOTE:</strong> With the introduction of <a href="persistent-volume.html">persistent volumes</a>,
executors and tasks should never create files outside of the sandbox.  However,
some containerizers do not enforce this sandboxing.</p>
<h2 id="where-is-the-sandbox"><a class="header" href="#where-is-the-sandbox"><a name="where-is-it"></a>Where is the sandbox?</a></h2>
<p>The sandbox is located within the agent's working directory (which is specified
via the <code>--work_dir</code> flag).  To find a particular executor's sandbox, you must
know the agent's ID, the executor's framework's ID, and the executor's ID.
Each run of the executor will have a corresponding sandbox, denoted by a
container ID.</p>
<p>The sandbox is located on the agent, inside a directory tree like the following:</p>
<pre><code>root ('--work_dir')
|-- slaves
|   |-- latest (symlink)
|   |-- &lt;agent ID&gt;
|       |-- frameworks
|           |-- &lt;framework ID&gt;
|               |-- executors
|                   |-- &lt;executor ID&gt;
|                       |-- runs
|                           |-- latest (symlink)
|                           |-- &lt;container ID&gt; (Sandbox!)
</code></pre>
<h2 id="using-the-sandbox"><a class="header" href="#using-the-sandbox">Using the sandbox</a></h2>
<p><strong>NOTE:</strong> For anything other than Mesos, the executor, or the task(s), the
sandbox should be considered a read-only directory.  This is not enforced via
permissions, but the executor/tasks may malfunction if the sandbox is mutated
unexpectedly.</p>
<h3 id="via-a-file-browser"><a class="header" href="#via-a-file-browser">Via a file browser</a></h3>
<p>If you have access to the machine running the agent, you can <a href="sandbox.html#where-is-it">navigate to the
sandbox directory directly</a>.</p>
<h3 id="via-the-mesos-web-ui"><a class="header" href="#via-the-mesos-web-ui">Via the Mesos web UI</a></h3>
<p>Sandboxes can be browsed and downloaded via the Mesos web UI.  Tasks and
executors will be shown with a &quot;Sandbox&quot; link.  Any files that live in the
sandbox will appear in the web UI.</p>
<h3 id="via-the-files-endpoint"><a class="header" href="#via-the-files-endpoint">Via the <code>/files</code> endpoint</a></h3>
<p>Underneath the web UI, the files are fetched from the agent via the <code>/files</code>
endpoint running on the agent.</p>
<table class="table table-striped">
  <thead>
    <tr>
      <th width="30%">
        Endpoint
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
<tr>
    <td>
       <code>/files/browse?path=...</code>
    </td>
    <td>
      Returns a JSON list of files and directories contained in the path.
      Each list is a JSON object containing all the fields normally found in
      <code>ls -l</code>.
    </td>
  </tr>
  <tr>
    <td>
       <code>/files/debug</code>
    </td>
    <td>
      Returns a JSON object holding the internal mapping of files managed by
      this endpoint.  This endpoint can be used to quickly fetch the paths
      of all files exposed on the agent.
    </td>
  </tr>
  <tr>
    <td>
       <code>/files/download?path=...</code>
    </td>
    <td>
      Returns the raw contents of the file located at the given path.
      Where the file extension is understood, the <code>Content-Type</code>
      header will be set appropriately.
    </td>
  </tr>
  <tr>
    <td>
       <code>/files/read?path=...</code>
    </td>
    <td>
      Reads a chunk of the file located at the given path and returns a JSON
      object containing the read <code>"data"</code> and the
      <code>"offset"</code> in bytes.
      <blockquote>
        <p>
          <strong>NOTE:</strong> This endpoint is not designed to read
          arbitrary binary files. Binary files may be returned as
          invalid/un-parseable JSON.
          Use <code>/files/download</code> instead.
        </p>
      </blockquote>
      Optional query parameters:
      <ul>
        <li><code>offset</code> - can be used to page through the file.</li>
        <li><code>length</code> - maximum size of the chunk to read.</li>
      </ul>
    </td>
  </tr>
</table>
<h2 id="sandbox-size"><a class="header" href="#sandbox-size">Sandbox size</a></h2>
<p>The maximum size of the sandbox is dependent on the containerization of the
executor and isolators:</p>
<ul>
<li>Mesos containerizer - For backwards compatibility, the Mesos containerizer
does not enforce a container's disk quota by default.  However, if the
<code>--enforce_container_disk_quota</code> flag is enabled on the agent, and
<code>disk/du</code> is specified in the <code>--isolation</code> flag, the executor
will be killed if the sandbox size exceeds the executor's <code>disk</code> resource.</li>
<li>Docker containerizer - As of Docker <code>1.9.1</code>, the Docker containerizer
does not enforce nor support a disk quota.  See the
<a href="https://github.com/docker/docker/issues/3804">Docker issue</a>.</li>
</ul>
<h2 id="sandbox-lifecycle"><a class="header" href="#sandbox-lifecycle">Sandbox lifecycle</a></h2>
<p>Sandbox files are scheduled for garbage collection when:</p>
<ul>
<li>An executor is removed or terminated.</li>
<li>A framework is removed.</li>
<li>An executor is recovered unsuccessfully during agent recovery.</li>
<li>If the <code>--gc_non_executor_container_sandboxes</code> agent flag is enabled,
nested container sandboxes will also be garbage collected when the
container exits.</li>
</ul>
<p><strong>NOTE:</strong> During agent recovery, all of the executor's runs, except for the
latest run, are scheduled for garbage collection as well.</p>
<p>Garbage collection is scheduled based on the <code>--gc_delay</code> agent flag.  By
default, this is one week since the sandbox was last modified.
After the delay, the files are deleted.</p>
<p>Additionally, according to the <code>--disk_watch_interval</code> agent flag, files
scheduled for garbage collection are pruned based on the available disk and
the <code>--gc_disk_headroom</code> agent flag.
See <a href="configuration/agent.html#gc_disk_headroom">the formula here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Container Volumes
layout: documentation</h2>
<h1 id="container-volumes"><a class="header" href="#container-volumes">Container Volumes</a></h1>
<p>For each volume a container specifies (i.e., <code>ContainerInfo.volumes</code>),
the following fields must be specified:</p>
<ul>
<li>
<p><code>container_path</code>: Path in the container filesystem at which the
volume will be mounted. If the path is a relative path, it is
relative to the container's sandbox.</p>
</li>
<li>
<p><code>mode</code>: If the volume is read-only or read-write.</p>
</li>
<li>
<p><code>source</code>: Describe where the volume originates from. See more
details in the following section.</p>
</li>
</ul>
<h2 id="volume-source-types"><a class="header" href="#volume-source-types">Volume Source Types</a></h2>
<ul>
<li><a href="container-volume.html#host_path-volume-source">HOST_PATH</a></li>
<li><a href="container-volume.html#sandbox_path-volume-source">SANDBOX_PATH</a></li>
<li><a href="container-volume.html#docker_volume-volume-source">DOCKER_VOLUME</a></li>
<li><a href="container-volume.html#secret-volume-source">SECRET</a></li>
</ul>
<h3 id="host_path-volume-source"><a class="header" href="#host_path-volume-source">HOST_PATH Volume Source</a></h3>
<p>This volume source represents a path on the host filesystem. The path
can either point to a directory or a file (either a regular file or a
device file).</p>
<p>The following example shows a <code>HOST_PATH</code> volume that mounts
<code>/var/lib/mysql</code> on the host filesystem to the same location in the
container.</p>
<pre><code class="language-json">{
  &quot;container_path&quot;: &quot;/var/lib/mysql&quot;,
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;source&quot;: {
    &quot;type&quot;: &quot;HOST_PATH&quot;,
    &quot;host_path&quot;: {
      &quot;path&quot;: &quot;/var/lib/mysql&quot;
    }
  }
}
</code></pre>
<p>The mode and ownership of the volume will be the same as that on the
host filesystem.</p>
<p>If you are using the <a href="mesos-containerizer.html">Mesos Containerizer</a>,
<code>HOST_PATH</code> volumes are handled by the <code>volume/host_path</code> isolator. To
enable this isolator, append <code>volume/host_path</code> to the <code>--isolation</code>
flag when starting the agent. This isolator depends on the
<a href="isolators/filesystems.html#filesystemlinux-isolator"><code>filesystem/linux</code></a>
isolator.</p>
<p><a href="docker-containerizer.html">Docker Containerizer</a> supports <code>HOST_PATH</code>
volume as well.</p>
<h3 id="sandbox_path-volume-source"><a class="header" href="#sandbox_path-volume-source">SANDBOX_PATH Volume Source</a></h3>
<p>There are currently two types of <code>SANDBOX_PATH</code> volume sources:
<a href="container-volume.html#self-type"><code>SELF</code></a> and <a href="container-volume.html#parent-type"><code>PARENT</code></a>.</p>
<p>If you are using <a href="mesos-containerizer.html">Mesos Containerizer</a>,
<code>SANDBOX_PATH</code> volumes are handled by the <code>volume/sandbox_path</code>
isolator.  To enable this isolator, append <code>volume/sandbox_path</code> to
the <code>--isolation</code> flag when starting the agent.</p>
<p>The <a href="docker-containerizer.html">Docker Containerizer</a> only supports
<code>SELF</code> type <code>SANDBOX_PATH</code> volumes currently.</p>
<h4 id="self-type"><a class="header" href="#self-type"><code>SELF</code> Type</a></h4>
<p>This represents a path in the container's own sandbox. The path can
point to either a directory or a file in the sandbox of the container.</p>
<p>The following example shows a <code>SANDBOX_PATH</code> volume from the
container's own sandbox that mount the subdirectory <code>tmp</code> in the
sandbox to <code>/tmp</code> in the container root filesystem. This will be
useful to cap the <code>/tmp</code> usage in the container (if disk isolator is
used and <code>--enforce_container_disk_quota</code> is turned on).</p>
<pre><code class="language-json">{
  &quot;container_path&quot;: &quot;/tmp&quot;,
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;source&quot;: {
    &quot;type&quot;: &quot;SANDBOX_PATH&quot;,
    &quot;sandbox_path&quot;: {
      &quot;type&quot;: &quot;SELF&quot;,
      &quot;path&quot;: &quot;tmp&quot;
    }
  }
}
</code></pre>
<p>The ownership of the volume will be the same as that of the sandbox of
the container.</p>
<p>Note that <code>container_path</code> has to be an absolute path in this case. If
<code>container_path</code> is relative, that means it's a volume from a
subdirectory in the container sandbox to another subdirectory in the
container sandbox. In that case, the user can just create a symlink,
instead of using a volume.</p>
<h4 id="parent-type"><a class="header" href="#parent-type">PARENT Type</a></h4>
<p>This represents a path in the sandbox of the parent container. The
path can point to either a directory or a file in the sandbox of the
parent container. See the <a href="nested-container-and-task-group.html">nested container
doc</a> for more details about what a
parent container is.</p>
<p>The following example shows a <code>SANDBOX_PATH</code> volume from the sandbox
of the parent container that mounts the subdirectory <code>shared_volume</code> in
the sandbox of the parent container to subdirectory <code>volume</code> in the
sandbox of the container.</p>
<pre><code class="language-json">{
  &quot;container_path&quot;: &quot;volume&quot;,
  &quot;mode&quot;: &quot;RW&quot;,
  &quot;source&quot;: {
    &quot;type&quot;: &quot;SANDBOX_PATH&quot;,
    &quot;sandbox_path&quot;: {
      &quot;type&quot;: &quot;PARENT&quot;,
      &quot;path&quot;: &quot;shared_volume&quot;
    }
  }
}
</code></pre>
<p>The ownership of the volume will be the same as that of the sandbox of
the parent container.</p>
<h3 id="docker_volume-volume-source"><a class="header" href="#docker_volume-volume-source">DOCKER_VOLUME Volume Source</a></h3>
<p>See more details in this <a href="isolators/docker-volume.html">doc</a>.</p>
<h3 id="secret-volume-source"><a class="header" href="#secret-volume-source">SECRET Volume Source</a></h3>
<p>See more details in this <a href="secrets.html#file-based-secrets">doc</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Mesos Nested Container and Task Group
layout: documentation</h2>
<h1 id="overview-5"><a class="header" href="#overview-5">Overview</a></h1>
<h2 id="motivation-3"><a class="header" href="#motivation-3">Motivation</a></h2>
<p>A <a href="http://kubernetes.io/docs/user-guide/pods">pod</a> can be defined as
a set of containers co-located and co-managed on an agent that share
some resources (e.g., network namespace, volumes) but not others
(e.g., container image, resource limits). Here are the use cases for
pod:</p>
<ul>
<li>Run a side-car container (e.g., logger, backup) next to the main
application controller.</li>
<li>Run an adapter container (e.g., metrics endpoint, queue consumer)
next to the main container.</li>
<li>Run transient tasks inside a pod for operations which are
short-lived and whose exit does not imply that a pod should
exit (e.g., a task which backs up data in a persistent volume).</li>
<li>Provide performance isolation between latency-critical application
and supporting processes.</li>
<li>Run a group of containers sharing volumes and network namespace
while some of them can have their own mount namespace.</li>
<li>Run a group of containers with the same life cycle, e.g, one
container's failure would cause all other containers being
cleaned up.</li>
</ul>
<p>In order to have first class support for running &quot;pods&quot;, two new
primitives are introduced in Mesos: <code>Task Group</code> and <code>Nested Container</code>.</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>Mesos has the concept of Executors and Tasks. An executor can launch
multiple tasks while the executor runs in a container. An agent can
run multiple executors. The pod can be implemented by leveraging the
executor and task abstractions. More specifically, the executor runs
in the top level container (called executor container) and its tasks
run in separate nested containers inside this top level container,
while the container image can be specified for each container.</p>
<h2 id="task-groups"><a class="header" href="#task-groups">Task Groups</a></h2>
<p>The concept of a &quot;task group&quot; addresses a previous limitation of the scheduler
and executor APIs, which could not send a group of tasks to an executor
atomically. Even though a scheduler can launch multiple tasks for the same
executor in a LAUNCH operation, these tasks are delivered to the executor one at
a time via separate LAUNCH events. It cannot guarantee atomicity since any
individual task might be dropped due to different reasons (e.g., network
partition). Therefore, the task group provides all-or-nothing semantics to
ensure a group of tasks are delivered atomically to an executor.</p>
<h2 id="nested-containers"><a class="header" href="#nested-containers">Nested Containers</a></h2>
<p>The concept of a &quot;nested container&quot; describes containers nested under an
executor container. In the typical case of a Linux agent, they share a network
namespace and volumes so that they can communicate using the network and access
the same data, though they may have their own container images and resource
limits. On Linux, they may share cgroups or have their own - see the section
below on resource limits for more information.</p>
<p>With the agent nested container API, executors can utilize the
containerizer in the agent to launch nested containers. Both authorized
operators or executors will be allowed to create nested containers. The Mesos
default executor makes use of this API when launching tasks, and custom
executors may consume it as well.</p>
<h2 id="resource-requests-and-limits"><a class="header" href="#resource-requests-and-limits">Resource Requests and Limits</a></h2>
<p>In each task, the resources required by that task can be specified. Common
resource types are <code>cpus</code>, <code>mem</code>, and <code>disk</code>. The resources listed in the
<code>resources</code> field are known as resource &quot;requests&quot; and represent the minimum
resource guarantee required by the task; these resources are used to set the
cgroups of the nested container associated with the task and will always be
available to the task process if they are needed. The quantities specified in
the <code>limits</code> field are the resource &quot;limits&quot;, which represent the maximum amount
of <code>cpus</code> and/or <code>mem</code> that the task may use. Setting a CPU or memory limit
higher than the corresponding request allows the task to consume more than its
allocated amount of CPU or memory when there are unused resources available on
the agent.</p>
<p>When multiple nested containers run under a single executor, the enforcement
of resource constraints depends on the value of the
<code>container.linux_info.share_cgroups</code> field. When this boolean field is <code>true</code>
(this is the default), each container is constrained by the cgroups of its
parent container. This means that if multiple tasks run underneath one executor,
their resource constraints will be enforced as a sum of all the task resource
constraints, applied collectively to those task processes. In this case, nested
container resource consumption is collectively managed via one set of cgroup
subsystem control files associated with the parent executor container.</p>
<p>When the <code>share_cgroups</code> field is set to <code>false</code>, the resource consumption of
each task is managed via a unique set of cgroups associated with that task's
nested container, which means that each task process is subject to its own
resource requests and limits. Note that if you want to specify <code>limits</code> on a
task, the task's container MUST set <code>share_cgroups</code> to <code>false</code>. Also note that
all nested containers under a single executor container must share the same
value of <code>share_cgroups</code>.</p>
<p>Note that when a task sets a memory limit higher than its memory request, the
Mesos agent will change the OOM score adjustment of the task process using a
heuristic based on the task's memory request and the agent's memory capacity.
This means that if the agent's memory becomes exhausted and processes must be
OOM-killed to reclaim memory at a time when the task is consuming more than its
memory request, the task process will be killed preferentially.</p>
<h1 id="task-group-api"><a class="header" href="#task-group-api">Task Group API</a></h1>
<h2 id="framework-api-1"><a class="header" href="#framework-api-1">Framework API</a></h2>
<pre><code>message TaskGroupInfo {
  repeated TaskInfo tasks = 1;
}

message Offer {
  ...

  message Operation {
    enum Type {
      ...
      LAUNCH_GROUP = 6;
      ...
    }
    ...

    message LaunchGroup {
      required ExecutorInfo executor = 1;
      required TaskGroupInfo task_group = 2;
    }
    ...

    optional LaunchGroup launch_group = 7;
  }
}
</code></pre>
<p>By using the TaskGroup Framework API, frameworks can launch a task
group with the <a href="app-framework-development-guide.html">default executor</a>
or a custom executor. The group of tasks can be specified through an
offer operation <code>LaunchGroup</code> when accepting an offer. The
<code>ExecutorInfo</code> indicates the executor to launch the task group, while
the <code>TaskGroupInfo</code> includes the group of tasks to be launched
atomically.</p>
<p>To use the default executor for launching the task group, the framework should:</p>
<ul>
<li>Set <code>ExecutorInfo.type</code> as <code>DEFAULT</code>.</li>
<li>Set <code>ExecutorInfo.resources</code> for the resources needed for the executor.</li>
</ul>
<p>Please note that the following fields in the <code>ExecutorInfo</code> are not allowed to set when using the default executor:</p>
<ul>
<li><code>ExecutorInfo.command</code>.</li>
<li><code>ExecutorInfo.container.type</code>, <code>ExecutorInfo.container.docker</code> and <code>ExecutorInfo.container.mesos</code>.</li>
</ul>
<p>To allow containers to share a network namespace:</p>
<ul>
<li>Set <code>ExecutorInfo.container.network</code>.</li>
</ul>
<p>To allow containers to share an ephemeral volume:</p>
<ul>
<li>Specify the <code>volume/sandbox_path</code> isolator.</li>
<li>Set <code>TaskGroupInfo.tasks.container.volumes.source.type</code> as <code>SANDBOX_PATH</code>.</li>
<li>Set <code>TaskGroupInfo.tasks.container.volumes.source.sandbox_path.type</code> as <code>PARENT</code> and the path relative to the parent container's sandbox.</li>
</ul>
<h2 id="executor-api"><a class="header" href="#executor-api">Executor API</a></h2>
<pre><code>message Event {
  enum Type {
    ...
    LAUNCH_GROUP = 8;
    ...
  }
  ...

  message LaunchGroup {
    required TaskGroupInfo task_group = 1;
  }
  ...

  optional LaunchGroup launch_group = 8;
}
</code></pre>
<p>A new event <code>LAUNCH_GROUP</code> is added to Executor API. Similar to the
Framework API, the <code>LAUNCH_GROUP</code> event guarantees a group of tasks
are delivered to the executor atomically.</p>
<h1 id="nested-container-api"><a class="header" href="#nested-container-api">Nested Container API</a></h1>
<h2 id="new-agent-api"><a class="header" href="#new-agent-api">New Agent API</a></h2>
<pre><code>package mesos.agent;

message Call {
  enum Type {
    ...
    // Calls for managing nested containers underneath an executor's container.
    NESTED_CONTAINER_LAUNCH = 14;  // See 'NestedContainerLaunch' below.
    NESTED_CONTAINER_WAIT = 15;    // See 'NestedContainerWait' below.
    NESTED_CONTAINER_KILL = 16;    // See 'NestedContainerKill' below.
  }

  // Launches a nested container within an executor's tree of containers.
  message LaunchNestedContainer {
    required ContainerID container_id = 1;
    optional CommandInfo command = 2;
    optional ContainerInfo container = 3;
  }

  // Waits for the nested container to terminate and receives the exit status.
  message WaitNestedContainer {
    required ContainerID container_id = 1;
  }

  // Kills the nested container. Currently only supports SIGKILL.
  message KillNestedContainer {
    required ContainerID container_id = 1;
  }

  optional Type type = 1;
  ...
  optional NestedContainerLaunch nested_container_launch = 6;
  optional NestedContainerWait nested_container_wait = 7;
  optional NestedContainerKill nested_container_kill = 8;
}

message Response {
  enum Type {
    ...
    NESTED_CONTAINER_WAIT = 13;    // See 'NestedContainerWait' below.
  }

  // Returns termination information about the nested container.
  message NestedContainerWait {
    optional int32 exit_status = 1;
  }

  optional Type type = 1;
  ...
  optional NestedContainerWait nested_container_wait = 14;
}
</code></pre>
<p>By adding the new Agent API, any authorized entity, including the
executor itself, its tasks, or the operator can use this API to
launch/wait/kill nested containers. Multi-level nesting is supported
by using this API. Technically, the nested level is up to 32 since
it is limited by the maximum depth of <a href="https://github.com/torvalds/linux/commit/f2302505775fd13ba93f034206f1e2a587017929">pid namespace</a> and
<a href="http://man7.org/linux/man-pages/man7/user_namespaces.7.html">user namespace</a> from the Linux Kernel.</p>
<p>The following is the workflow of how the new Agent API works:</p>
<ol>
<li>
<p>The executor sends a <code>NESTED_CONTAINER_LAUNCH</code> call to the agent.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+                | +-----------------+ |
 |             |     LAUNCH     | |                 | |
 |             | &lt;------------+ | |    Executor     | |
 |             |                | |                 | |
 | Mesos Agent |                | +-----------------+ |
 |             |                |                     |
 |             |                |                     |
 |             |                |                     |
 +-------------+                |                     |
                                +---------------------+
</code></pre>
</li>
<li>
<p>Depending on the <code>LaunchNestedContainer</code> from the executor, the
agent launches a nested container inside of the executor container
by calling <code>containerizer::launch()</code>.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+                | +-----------------+ |
 |             |     LAUNCH     | |                 | |
 |             | &lt;------------+ | |    Executor     | |
 |             |                | |                 | |
 | Mesos Agent |                | +-----------------+ |
 |             |                |                     |
 |             |                | +---------+         |
 |             | +------------&gt; | |Nested   |         |
 +-------------+                | |Container|         |
                                | +---------+         |
                                +---------------------+
</code></pre>
</li>
<li>
<p>The executor sends a <code>NESTED_CONTAINER_WAIT</code> call to the agent.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+                | +-----------------+ |
 |             |      WAIT      | |                 | |
 |             | &lt;------------+ | |    Executor     | |
 |             |                | |                 | |
 | Mesos Agent |                | +-----------------+ |
 |             |                |                     |
 |             |                | +---------+         |
 |             |                | |Nested   |         |
 +-------------+                | |Container|         |
                                | +---------+         |
                                +---------------------+
</code></pre>
</li>
<li>
<p>Depending on the <code>ContainerID</code>, the agent calls <code>containerizer::wait()</code>
to wait for the nested container to terminate or exit. Once the container
terminates or exits, the agent returns the container exit status to the
executor.</p>
<pre><code>                                +---------------------+
                                |                     |
                                |     Container       |
                                |                     |
 +-------------+      WAIT      | +-----------------+ |
 |             | &lt;------------+ | |                 | |
 |             |                | |    Executor     | |
 |             | +------------&gt; | |                 | |
 | Mesos Agent |  Exited with   | +-----------------+ |
 |             |  status 0      |                     |
 |             |                | +--XX-XX--+         |
 |             |                | |   XXX   |         |
 +-------------+                | |   XXX   |         |
                                | +--XX-XX--+         |
                                +---------------------+
</code></pre>
</li>
</ol>
<h1 id="future-work-1"><a class="header" href="#future-work-1">Future Work</a></h1>
<ul>
<li>Authentication and authorization on the new Agent API.</li>
<li>Command health checks inside of the container's mount namespace.</li>
<li>Resource isolation for nested containers.</li>
<li>Resource statistics reporting for nested containers.</li>
<li>Multiple task groups.</li>
</ul>
<h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<ul>
<li><a href="http://kubernetes.io/docs/user-guide/pods/">Kubernetes Pods</a></li>
<li><a href="https://github.com/docker/docker/issues/8781">Docker Pods</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Standalone Containers
layout: documentation</h2>
<h1 id="standalone-containers"><a class="header" href="#standalone-containers">Standalone Containers</a></h1>
<p>Traditionally, launching a container in a Mesos cluster involves
communication between multiple components:</p>
<pre><code>                                                 Container(s)
  +-----------+     +--------+     +-------+     +----------+
  | Framework | &lt;-&gt; | Master | &lt;-&gt; | Agent | &lt;-&gt; | Executor |
  +-----------+     +--------+     +-------+     |  `-&gt;Task |
                         ^                       +----------+
                         |         +-------+     +----------+
                         +------&gt;  | Agent | &lt;-&gt; | Executor |
                         |         +-------+     |  `-&gt;Task |
                        ...                      +----------+
</code></pre>
<p>Mesos 1.5 introduced &quot;Standalone Containers&quot;, which provide an alternate
path for launching containers with a reduced scope and feature set:</p>
<pre><code>                   +-------+    +----------------------+
  Operator API &lt;-&gt; | Agent | -&gt; | Standalone Container |
                   +-------+    +----------------------+
</code></pre>
<p><strong>NOTE:</strong> Agents currently require a connection to a Mesos master in
order to accept any Operator API calls.  This limitation is not necessary
and may be fixed in future.</p>
<p><strong>NOTE:</strong> Standalone containers only apply to the Mesos containerizer.
For standalone docker containers, use docker directly.</p>
<p>As hinted by the diagrams, standalone containers are launched on single
Agents, rather than cluster-wide.  This document describes the major
differences between normal containers and standalone containers; and
provides some examples of how to use the new Operator APIs.</p>
<h2 id="launching-a-standalone-container"><a class="header" href="#launching-a-standalone-container">Launching a Standalone Container</a></h2>
<p>Because standalone containers are launched directly on Mesos Agents,
these containers do not participate in the Mesos Master's offer cycle.
This means standalone containers can be launched regardless of resource
allocation and can potentially overcommit the Mesos Agent, but cannot
use reserved resources.</p>
<p>An Operator API might look like this:</p>
<pre><code>LAUNCH_CONTAINER HTTP Request (JSON):

POST /api/v1  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json

{
  &quot;type&quot;: &quot;LAUNCH_CONTAINER&quot;,
  &quot;launch_container&quot;: {
    &quot;container_id&quot;: {
      &quot;value&quot;: &quot;my-standalone-container-id&quot;
    },
    &quot;command&quot;: {
      &quot;value&quot;: &quot;sleep 100&quot;
    },
    &quot;resources&quot;: [
      {
        &quot;name&quot;: &quot;cpus&quot;,
        &quot;scalar&quot;: { &quot;value&quot;: 2.0 },
        &quot;type&quot;: &quot;SCALAR&quot;
      },
      {
        &quot;name&quot;: &quot;mem&quot;,
        &quot;scalar&quot;: { &quot;value&quot;: 1024.0 },
        &quot;type&quot;: &quot;SCALAR&quot;
      },
      {
        &quot;name&quot;: &quot;disk&quot;,
        &quot;scalar&quot;: { &quot;value&quot;: 1024.0 },
        &quot;type&quot;: &quot;SCALAR&quot;
      }
    ],
    &quot;container&quot;: {
      &quot;type&quot;: &quot;MESOS&quot;,
      &quot;mesos&quot;: {
        &quot;image&quot;: {
          &quot;type&quot;: &quot;DOCKER&quot;,
          &quot;docker&quot;: {
            &quot;name&quot;: &quot;alpine&quot;
          }
        }
      }
    }
  }
}
</code></pre>
<p>The Agent will return:</p>
<ul>
<li>200 OK if the launch succeeds, including fetching any container images
or URIs specified in the launch command.</li>
<li>202 Accepted if the specified ContainerID is already in use by a running
container.</li>
<li>400 Bad Request if the launch fails for any reason.</li>
</ul>
<p><strong>NOTE:</strong> Nested containers share the same Operator API.  To launch a nested
container, the ContainerID needs to have a parent; and no resources may be
specified in the request.</p>
<h2 id="monitoring-a-standalone-container"><a class="header" href="#monitoring-a-standalone-container">Monitoring a Standalone Container</a></h2>
<p>Standalone containers are not managed by a framework, do not use executors,
and therefore do not have status updates.  They are not automatically
relaunched upon completion/failure.</p>
<p>After launching a standalone container, the operator should monitor the
container via the <code>WAIT_CONTAINER</code> call:</p>
<pre><code>WAIT_CONTAINER HTTP Request (JSON):

POST /api/v1  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json
Accept: application/json

{
  &quot;type&quot;: &quot;WAIT_CONTAINER&quot;,
  &quot;wait_container&quot;: {
    &quot;container_id&quot;: {
      &quot;value&quot;: &quot;my-standalone-container-id&quot;
    }
  }
}

WAIT_CONTAINER HTTP Response (JSON):

HTTP/1.1 200 OK

Content-Type: application/json

{
  &quot;type&quot;: &quot;WAIT_CONTAINER&quot;,
  &quot;wait_container&quot;: {
    &quot;exit_status&quot;: 0
  }
}
</code></pre>
<p>This is a blocking HTTP call that only returns after the container has
exited.</p>
<p>If the specified ContainerID does not exist, the call returns a 404.</p>
<h2 id="killing-a-standalone-container"><a class="header" href="#killing-a-standalone-container">Killing a Standalone Container</a></h2>
<p>A standalone container can be signalled (usually to kill it) via this API:</p>
<pre><code>KILL_CONTAINER HTTP Request (JSON):

POST /api/v1  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json

{
  &quot;type&quot;: &quot;KILL_CONTAINER&quot;,
  &quot;kill_container&quot;: {
    &quot;container_id&quot;: {
      &quot;value&quot;: &quot;my-standalone-container-id&quot;
    }
  }
}

KILL_CONTAINER HTTP Response (JSON):

HTTP/1.1 200 OK
</code></pre>
<p>If the specified ContainerID does not exist, the call returns a 404.</p>
<h2 id="cleaning-up-a-standalone-container"><a class="header" href="#cleaning-up-a-standalone-container">Cleaning up a Standalone Container</a></h2>
<p>Unlike other containers, a standalone container's sandbox is not garbage
collected by the Agent after some time (like other sandbox directories).
The Agent is unable to garbage collect these containers because there is
no status update mechanism to report the exit status of the container.</p>
<p>Standalone container sandboxes must be manually cleaned up by the operator and
are located in the agent's work directory under
<code>/containers/&lt;my-standalone-container-id&gt;</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="networking-support-in-mesos"><a class="header" href="#networking-support-in-mesos">Networking support in Mesos</a></h2>
<h3 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of contents</a></h3>
<ul>
<li><a href="networking.html#introduction">Introduction</a></li>
<li><a href="networking.html#attaching-containers">Attaching containers to IP networks</a>
<ul>
<li><a href="networking.html#attaching-containers-mesos">Mesos containerizer</a></li>
<li><a href="networking.html#attaching-containers-docker">Docker containerizer</a></li>
<li><a href="networking.html#limitations-docker">Limitations of Docker containerizer</a></li>
</ul>
</li>
<li><a href="networking.html#retrieve-network-info">Retrieving network information for a container</a></li>
</ul>
<h3 id="introduction"><a class="header" href="#introduction"><a name="introduction"></a>Introduction</a></h3>
<p>Mesos supports two container runtime engines, the <code>MesosContainerizer</code>
and the <code>DockerContainerizer</code>. Both the container run time engines
provide IP-per-container support allowing containers to be attached to
different types of IP networks.  However, the two container run time
engines differ in the way IP-per-container support is implemented. The
<code>MesosContainerizer</code> uses the <code>network/cni</code> isolator to implement the
<a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">Container Network Interface (CNI)</a>
to provide networking support for Mesos containers, while the
<code>DockerContainerizer</code> relies on the Docker daemon to provide
networking support using Docker's <a href="https://github.com/docker/libnetwork">Container Network
Model</a>.</p>
<p>Note that while IP-per-container is one way to achieve network
isolation between containers, there are other alternatives to
implement network isolation within <code>MesosContainerizer</code>, e.g.,  using
the <a href="isolators/network-port-mapping.html">port-mapping network isolator</a>.</p>
<p>While the two container run-time engines use different mechanisms to
provide networking support for containers, the interface to specify
the network that a container needs to join, and the interface to
retrieve networking information for a container remain the same.</p>
<p>The <code>NetworkInfo</code> protobuf, described below, is the interface provided
by Mesos to specify network related information for a container and to
learn network information associated with a container.</p>
<pre><code class="language-{.proto}">message NetworkInfo {
  enum Protocol {
    IPv4 = 1;
    IPv6 = 2;
  }

  message IPAddress {
    optional Protocol protocol = 1;
    optional string ip_address = 2;
  }

  repeated IPAddress ip_addresses = 5;
  optional string name = 6;
  repeated string groups = 3;
  optional Labels labels = 4;
};
</code></pre>
<p>This document describes the usage of the <code>NetworkInfo</code> protobuf, by
frameworks, to attach containers to IP networks. It also describes the
interfaces provided to retrieve IP address and other network related
information for a container, once the container has been attached to
an IP network.</p>
<h3 id="attaching-containers-to-ip-networks"><a class="header" href="#attaching-containers-to-ip-networks"><a name="attaching-containers"></a>Attaching containers to IP networks</a></h3>
<h4 id="mesos-containerizer-3"><a class="header" href="#mesos-containerizer-3"><a name="attaching-containers-mesos"></a>Mesos containerizer</a></h4>
<p><code>MesosContainerizer</code> has the <a href="cni.html"><code>network/cni</code></a> isolator enabled
by default, which implements CNI (Container Network Interface). The
<code>network/cni</code> isolator identifies CNI networks by using canonical
names. When frameworks want to associate containers to a specific CNI
network they specify a network name in the <code>name</code> field of the <code> NetworkInfo</code> protobuf. Details about the configuration and interaction
of Mesos containers with CNI networks can be found in the
documentation describing <a href="cni.html">&quot;CNI support for Mesos containers&quot;</a>.</p>
<h4 id="docker-containerizer-3"><a class="header" href="#docker-containerizer-3"><a name="attaching-containers-docker"></a>Docker containerizer</a></h4>
<p>Starting docker 1.9, there are four networking modes available in
Docker: NONE, HOST, BRIDGE and USER. <a href="https://docs.docker.com/engine/userguide/networking/dockernetworks/">&quot;Docker container
networks&quot;</a>
provides more details about the various networking modes available in
docker. Mesos supports all the four networking modes provided by
Docker. To connect a docker container using a specific mode the
framework needs to specify the network mode in the <code>DockerInfo</code>
protobuf.</p>
<pre><code class="language-{.proto}">message DockerInfo {
  // The docker image that is going to be passed to the registry.
  required string image = 1;

  // Network options.
  enum Network {
    HOST = 1;
    BRIDGE = 2;
    NONE = 3;
    USER = 4;
  }

  optional Network network = 2 [default = HOST];
 };

</code></pre>
<p>For <code>NONE</code>, <code>HOST</code>, and <code>BRIDGE</code> network mode the framework only needs
to specify the network mode in the <code>DockerInfo</code> protobuf. To use other
networks, such as <code>MACVLAN</code> on Linux, <code>TRANSPARENT</code> and <code>L2BRIDGE</code> on
Windows, or any other user-defined network, the network needs to be
created beforehand and the <code>USER</code> network mode needs to be chosen. For
the <code>USER</code> mode, since a user-defined docker network is identified by a
canonical network name (similar to CNI networks) apart from setting the
network mode in <code>DockerInfo</code> the framework also needs to specify the
<code>name</code> field in the <code>NetworkInfo</code> protobuf corresponding to the name of
the user-defined docker network.</p>
<p>Note that on Windows, the <code>HOST</code> network mode is not supported. Although the
<code>BRIDGE</code> network mode does not exist on Windows, it has an equivalent mode
called <code>NAT</code>, so on Windows agents, the <code>BRIDGE</code> mode will be interpretted as
<code>NAT</code>. If the network mode is not specified, then the default mode will be
chosen, which is <code>HOST</code> on Linux and <code>NAT</code> on Windows.</p>
<h4 id="limitations-of-docker-containerizer"><a class="header" href="#limitations-of-docker-containerizer"><a name="limitations-docker"></a>Limitations of Docker containerizer</a></h4>
<p>One limitation that the <code>DockerContainerizer</code> imposes on the
containers using the USER network mode is that these containers cannot
be attached to multiple docker networks. The reason this limitation
exists is that to connect a container to multiple Docker networks,
Docker requires the container to be created first and then attached to
the different networks. This model of orchestration does not fit the
current implementation of the <code>DockerContainerizer</code> and hence the
restriction of limiting docker container to a single network.</p>
<h3 id="retrieving-network-information-for-a-container"><a class="header" href="#retrieving-network-information-for-a-container"><a name="retrieve-network-info"></a>Retrieving network information for a container</a></h3>
<p>Whenever a task runs on a Mesos agent, the executor associated with
the task returns a <code>TaskStatus</code> protobuf associated with the task.
Containerizers (Mesos or Docker) responsible for the container will
populate the <code>ContainerStatus</code> protobuf associated with the
<code>TaskStatus</code>. The <code>ContainerStatus</code> will contain multiple
<code>NetworkInfo</code> protobuf instances, one each for the interfaces
associated with the container. Any IP address associated with the
container will be reflected in the <code>NetworkInfo</code> protobuf instances.</p>
<p>The <code>TaskStatus</code> associated with each task can be accessed at the
Agent's <code>state</code> endpoint on which the task is running or it can be
accessed in the Master's <code>state</code> endpoint.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Networking for Mesos-Managed Containers
layout: documentation</h2>
<h1 id="networking-for-mesos-managed-containers"><a class="header" href="#networking-for-mesos-managed-containers">Networking for Mesos-Managed Containers</a></h1>
<p>While networking plays a key role in data center infrastructure, it is -- for
now -- beyond the scope of Mesos to try to address the concerns of networking
setup, topology and performance. However, Mesos can ease integrations with
existing networking solutions and enable features, like IP per container,
task-granular task isolation and service discovery. More often than not, it
will be challenging to provide a one-size-fits-all networking solution. The
requirements and available solutions will vary across all cloud-only,
on-premise, and hybrid deployments.</p>
<p>One of the primary goals for the networking support in Mesos was to have a
pluggable mechanism to allow users to enable custom networking solution as
needed. As a result, several extensions were added to Mesos components in
version 0.25.0 to enable networking support. Further, all the extensions are
opt-in to allow older frameworks and applications without networking support to
coexist with the newer ones.</p>
<p>The rest of this document describes the overall architecture of all the involved
components, configuration steps for enabling IP-per-container, and required
framework changes.</p>
<h2 id="how-does-it-work-3"><a class="header" href="#how-does-it-work-3">How does it work?</a></h2>
<p><img src="images/networking-architecture.png" alt="Mesos Networking Architecture" /></p>
<p>A key observation is that the networking support is enabled via a Mesos module
and thus the Mesos master and agents are completely oblivious of it. It is
completely up to the networking module to provide the desired support. Next,
the IP requests are provided on a best effort manner. Thus, the framework should
be willing to handle ignored (in cases where the module(s) are not present) or
declined (the IPs can't be assigned due to various reasons) requests.</p>
<p>To maximize backwards-compatibility with existing frameworks, schedulers must
opt-in to network isolation per-container. Schedulers opt in to network
isolation using new data structures in the TaskInfo message.</p>
<h3 id="terminology-1"><a class="header" href="#terminology-1">Terminology</a></h3>
<ul>
<li>
<p>IP Address Management (IPAM) Server</p>
<ul>
<li>assigns IPs on demand</li>
<li>recycles IPs once they have been released</li>
<li>(optionally) can tag IPs with a given string/id.</li>
</ul>
</li>
<li>
<p>IPAM client</p>
<ul>
<li>tightly coupled with a particular IPAM server</li>
<li>acts as a bridge between the &quot;Network Isolator Module&quot; and the IPAM server</li>
<li>communicates with the server to request/release IPs</li>
</ul>
</li>
<li>
<p>Network Isolator Module (NIM):</p>
<ul>
<li>a Mesos module for the Agent implementing the <code>Isolator</code> interface</li>
<li>looks at TaskInfos to detect the IP requirements for the tasks</li>
<li>communicates with the IPAM client to request/release IPs</li>
<li>communicates with an external network virtualizer/isolator to enable network
isolation</li>
</ul>
</li>
<li>
<p>Cleanup Module:</p>
<ul>
<li>responsible for doing a cleanup (releasing IPs, etc.) during an Agent lost
event, dormant otherwise</li>
</ul>
</li>
</ul>
<h3 id="framework-requests-ip-address-for-containers"><a class="header" href="#framework-requests-ip-address-for-containers">Framework requests IP address for containers</a></h3>
<ol>
<li>
<p>A Mesos framework uses the TaskInfo message to requests IPs for each
container being launched. (The request is ignored if the Mesos cluster
doesn't have support for IP-per-container.)</p>
</li>
<li>
<p>Mesos Master processes TaskInfos and forwards them to the Agent for launching
tasks.</p>
</li>
</ol>
<h3 id="network-isolator-module-gets-ip-from-ipam-server"><a class="header" href="#network-isolator-module-gets-ip-from-ipam-server">Network isolator module gets IP from IPAM server</a></h3>
<ol start="3">
<li>
<p>Mesos Agent inspects the TaskInfo to detect the container requirements
(MesosContainerizer in this case) and prepares various Isolators for the
to-be-launched container.</p>
<ul>
<li>The NIM inspects the TaskInfo to decide whether to enable network isolator
or not.</li>
</ul>
</li>
<li>
<p>If network isolator is to be enabled, NIM requests IP address(es) via IPAM
client and informs the Agent.</p>
</li>
</ol>
<h3 id="agent-launches-container-with-a-network-namespace"><a class="header" href="#agent-launches-container-with-a-network-namespace">Agent launches container with a network namespace</a></h3>
<ol start="5">
<li>The Agent launches a container within a new network namespace.
<ul>
<li>The Agent calls into NIM to perform &quot;isolation&quot;</li>
<li>The NIM then calls into network virtualizer to isolate the container.</li>
</ul>
</li>
</ol>
<h3 id="network-virtualizer-assigns-ip-address-to-the-container-and-isolates-it"><a class="header" href="#network-virtualizer-assigns-ip-address-to-the-container-and-isolates-it">Network virtualizer assigns IP address to the container and isolates it.</a></h3>
<ol start="6">
<li>NIM then &quot;decorates&quot; the TaskStatus with the IP information.
<ul>
<li>The IP address(es) from TaskStatus are made available at Master's
<a href="endpoints/master/state.html">/state</a> endpoint.</li>
<li>The TaskStatus is also forwarded to the framework to inform it of the IP
addresses.</li>
<li>When a task is killed or lost, NIM communicates with IPAM client to release
corresponding IP address(es).</li>
</ul>
</li>
</ol>
<h3 id="cleanup-module-detects-lost-agents-and-performs-cleanup"><a class="header" href="#cleanup-module-detects-lost-agents-and-performs-cleanup">Cleanup module detects lost Agents and performs cleanup</a></h3>
<ol start="7">
<li>
<p>The cleanup module gets notified if there is an Agent-lost event.</p>
</li>
<li>
<p>The cleanup module communicates with the IPAM client to release all IP
address(es) associated with the lost Agent. The IPAM may have a grace period
before the address(es) are recycled.</p>
</li>
</ol>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<p>The network isolator module is not part of standard Mesos distribution. However,
there is an example implementation at https://github.com/mesosphere/net-modules.</p>
<p>Once the network isolation module has been built into a shared dynamic library,
we can load it into Mesos Agent (see <a href="modules.html">modules documentation</a> on
instructions for building and loading a module).</p>
<h2 id="enabling-frameworks-for-ip-per-container-capability"><a class="header" href="#enabling-frameworks-for-ip-per-container-capability">Enabling frameworks for IP-per-container capability</a></h2>
<h3 id="networkinfo"><a class="header" href="#networkinfo">NetworkInfo</a></h3>
<p>A new NetworkInfo message has been introduced:</p>
<pre><code class="language-{.proto}">message NetworkInfo {
  enum Protocol {
    IPv4 = 1;
    IPv6 = 2;
  }

  message IPAddress {
    optional Protocol protocol = 1;

    optional string ip_address = 2;
  }

  repeated IPAddress ip_addresses = 5;

  optional string name = 6;

  optional Protocol protocol = 1 [deprecated = true]; // Since 0.26.0

  optional string ip_address = 2 [deprecated = true]; // Since 0.26.0

  repeated string groups = 3;

  optional Labels labels = 4;
};
</code></pre>
<p>When requesting an IP address from the IPAM, one needs to set the <code>protocol</code>
field to <code>IPv4</code> or <code>IPv6</code>. Setting <code>ip_address</code> to a valid IP address allows the
framework to specify a static IP address for the container (if supported by the
NIM). This is helpful in situations where a task must be bound to a particular
IP address even as it is killed and restarted on a different node.</p>
<p>Setting <code>name</code> to a valid network name allows the framework to specify a network
for the container to join, it is up to the network isolator to decide how to
interpret this field, e.g., <code>network/cni</code> isolator will interpret it as the name
of a CNI network.</p>
<h3 id="examples-of-specifying-network-requirements"><a class="header" href="#examples-of-specifying-network-requirements">Examples of specifying network requirements</a></h3>
<p>Frameworks wanting to enable IP per container, need to provide <code>NetworkInfo</code>
message in TaskInfo. Here are a few examples:</p>
<ol>
<li>
<p>A request for one address of unspecified protocol version using the default
command executor</p>
<pre><code>TaskInfo {
  ...
  command: ...,
  container: ContainerInfo {
    network_infos: [
      NetworkInfo {
        ip_addresses: [
          IPAddress {
            protocol: None;
            ip_address: None;
          }
        ]
        groups: [];
        labels: None;
      }
    ]
  }
}
</code></pre>
</li>
<li>
<p>A request for one IPv4 and one IPv6 address, in two groups using the
default command executor</p>
<pre><code>TaskInfo {
  ...
  command: ...,
  container: ContainerInfo {
    network_infos: [
      NetworkInfo {
        ip_addresses: [
          IPAddress {
            protocol: IPv4;
            ip_address: None;
          },
          IPAddress {
            protocol: IPv6;
            ip_address: None;
          }
        ]
        groups: [&quot;dev&quot;, &quot;test&quot;];
        labels: None;
      }
    ]
  }
}
</code></pre>
</li>
<li>
<p>A request for two network interfaces, each with one IP address, each in
a different network group using the default command executor</p>
<pre><code>TaskInfo {
  ...
  command: ...,
  container: ContainerInfo {
    network_infos: [
      NetworkInfo {
        ip_addresses: [
          IPAddress {
            protocol: None;
            ip_address: None;
          }
        ]
        groups: [&quot;foo&quot;];
        labels: None;
      },
      NetworkInfo {
        ip_addresses: [
          IPAddress {
            protocol: None;
            ip_address: None;
          }
        ]
        groups: [&quot;bar&quot;];
        labels: None;
      },
    ]
  }
}
</code></pre>
</li>
<li>
<p>A request for a specific IP address using a custom executor</p>
<pre><code>TaskInfo {
  ...
  executor: ExecutorInfo {
    ...,
    container: ContainerInfo {
      network_infos: [
        NetworkInfo {
          ip_addresses: [
            IPAddress {
              protocol: None;
              ip_address: &quot;10.1.2.3&quot;;
            }
          ]
          groups: [];
          labels: None;
        }
      ]
    }
  }
}
</code></pre>
</li>
<li>
<p>A request for joining a specific network using the default command executor</p>
<pre><code>TaskInfo {
  ...
  command: ...,
  container: ContainerInfo {
    network_infos: [
      NetworkInfo {
        name: &quot;network1&quot;;
      }
    ]
  }
}
</code></pre>
</li>
</ol>
<p>NOTE: The Mesos Containerizer will reject any CommandInfo that has a
ContainerInfo. For this reason, when opting in to network isolation when
using the Mesos Containerizer, set TaskInfo.ContainerInfo.NetworkInfo.</p>
<h2 id="address-discovery"><a class="header" href="#address-discovery">Address Discovery</a></h2>
<p>The NetworkInfo message allows frameworks to request IP address(es) to be
assigned at task launch time on the Mesos agent. After opting in to network
isolation for a given executor's container in this way, frameworks will need to
know what address(es) were ultimately assigned in order to perform health
checks, or any other out-of-band communication.</p>
<p>This is accomplished by adding a new field to the TaskStatus message.</p>
<pre><code class="language-{.proto}">message ContainerStatus {
   repeated NetworkInfo network_infos;
}

message TaskStatus {
  ...
  optional ContainerStatus container;
  ...
};
</code></pre>
<p>Further, the container IP addresses are also exposed via Master's state
endpoint. The JSON output from Master's state endpoint contains a list of task
statuses. If a task's container was started with it's own IP address, the
assigned IP address will be exposed as part of the <code>TASK_RUNNING</code> status.</p>
<p>NOTE: Since per-container address(es) are strictly opt-in from the framework,
the framework may ignore the IP address(es) provided in StatusUpdate if it
didn't set NetworkInfo in the first place.</p>
<h2 id="writing-a-custom-network-isolator-module"><a class="header" href="#writing-a-custom-network-isolator-module">Writing a Custom Network Isolator Module</a></h2>
<p>A network isolator module implements the Isolator interface provided by Mesos.
The module is loaded as a dynamic shared library in to the Mesos Agent and gets
hooked up in the container launch sequence. A network isolator may communicate
with external IPAM and network virtualizer tools to fulfill framework
requirements.</p>
<p>In terms of the Isolator API, there are three key callbacks that a network
isolator module should implement:</p>
<ol>
<li>
<p><code>Isolator::prepare()</code> provides the module with a chance to decide whether or
not the enable network isolation for the given task container. If the network
isolation is to be enabled, the Isolator::prepare call would inform the Agent
to create a private network namespace for the coordinator. It is this
interface, that will also generate an IP address (statically or with the help
of an external IPAM agent) for the container.</p>
</li>
<li>
<p><code>Isolator::isolate()</code> provide the module with the opportunity to <em>isolate</em>
the container <em>after</em> it has been created but before the executor is launched
inside the container. This would involve creating virtual ethernet adapter
for the container and assigning it an IP address. The module can also use
help of an external network virtualizer/isolator for setting up network for
the container.</p>
</li>
<li>
<p><code>Isolator::cleanup()</code> is called when the container terminates. This allows the
module to perform any cleanups such as recovering resources and releasing IP
addresses as needed.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="container-network-interface-cni-for-mesos-containers"><a class="header" href="#container-network-interface-cni-for-mesos-containers">Container Network Interface (CNI) for Mesos Containers</a></h2>
<p>This document describes the <code>network/cni</code> isolator, a network isolator
for the <a href="mesos-containerizer.html">MesosContainerizer</a> that implements
the <a href="https://github.com/containernetworking/cni">Container Network Interface
(CNI)</a> specification.  The
<code>network/cni</code> isolator allows containers launched using the
<code>MesosContainerizer</code> to be attached to several different types of IP
networks.  The network technologies on which containers can possibly
be launched range from traditional layer 3/layer 2 networks such as
VLAN, ipvlan, macvlan, to the new class of networks designed for
container orchestration such as
<a href="https://www.projectcalico.org/">Calico</a>,
<a href="https://weave.works/">Weave</a> and
<a href="https://coreos.com/flannel/docs/latest/">Flannel</a>.  The
<code>MesosContainerizer</code> has the <code>network/cni</code> isolator enabled by
default.</p>
<h3 id="table-of-contents-2"><a class="header" href="#table-of-contents-2">Table of Contents</a></h3>
<ul>
<li><a href="cni.html#motivation">Motivation</a></li>
<li><a href="cni.html#usage">Usage</a>
<ul>
<li><a href="cni.html#configuring-cni-networks">Configuring CNI networks</a></li>
<li><a href="cni.html#adding-modifying-deleting">Adding/Deleting/Modifying CNI networks</a></li>
<li><a href="cni.html#attaching-containers-to-cni-networks">Attaching containers to CNI networks</a></li>
<li><a href="cni.html#accessing-container-network-namespace">Accessing container network namespace</a></li>
<li><a href="cni.html#passing-network-labels-and-port-mapping-information-to-cni-plugins">Passing network labels and port-mapping information to CNI plugins</a></li>
</ul>
</li>
<li><a href="cni.html#networking-recipes">Networking Recipes</a>
<ul>
<li><a href="cni.html#a-bridge-network">A bridge network</a></li>
<li><a href="cni.html#a-port-mapper-plugin">A port-mapper plugin for CNI networks</a></li>
<li><a href="cni.html#a-calico-network">A Calico network</a></li>
<li><a href="cni.html#a-cilium-network">A Cilium network</a></li>
<li><a href="cni.html#a-weave-network">A Weave network</a></li>
</ul>
</li>
</ul>
<h3 id="motivation-4"><a class="header" href="#motivation-4"><a name="motivation"></a>Motivation</a></h3>
<p>Having a separate network namespace for each container is attractive
for orchestration engines such as Mesos, since it provides containers
with network isolation and allows users to operate on containers as if
they were operating on an end-host.  Without network isolation users
have to deal with managing network resources such as TCP/UDP ports on
an end host, complicating the design of their application.</p>
<p>The challenge is in implementing the ability in the orchestration
engine to communicate with the underlying network in order to
configure IP connectivity to the container.  This problem arises due
to the diversity in terms of the choices of IPAM (IP address
management system) and networking technologies available for enabling
IP connectivity. To solve this problem we would need to adopt a driver
based network orchestration model, where the <code>MesosContainerizer</code> can
offload the business intelligence of configuring IP connectivity to a
container, to network specific drivers.</p>
<p>The <strong><a href="https://github.com/containernetworking/cni">Container Network Interface
(CNI)</a></strong> is a
specification proposed by CoreOS that provides such a driver based
model. The specification defines a JSON schema that defines the inputs
and outputs expected of a CNI plugin (network driver). The
specification also provides a clear separation of concerns for the
container run time and the CNI plugin. As per the specification the
container run time is expected to configure the namespace for the
container, a unique identifier for the container (container ID), and a
JSON formatted input to the plugin that defines the configuration
parameters for a given network. The responsibility of the plugin is to
create a <em>veth</em> pair and attach one of the <em>veth</em> pairs to the network
namespace of the container, and the other end to a network understood
by the plugin.  The CNI specification also allows for multiple
networks to exist simultaneously, with each network represented by a
canonical name, and associated with a unique CNI configuration. There
are already CNI plugins for a variety of networks such as bridge,
ipvlan, macvlan, Calico, Weave and Flannel.</p>
<p>Thus, introducing support for CNI in Mesos through the <code>network/cni</code>
isolator provides Mesos with tremendous flexibility to orchestrate
containers on a wide variety of network technologies.</p>
<h3 id="usage-2"><a class="header" href="#usage-2"><a name="usage"></a>Usage</a></h3>
<p>The <code>network/cni</code> isolator is enabled by default.  However, to use the
isolator there are certain actions required by the operator and the
frameworks. In this section we specify the steps required by the
operator to configure CNI networks on Mesos and the steps required by
frameworks to attach containers to a CNI network.</p>
<h4 id="configuring-cni-networks"><a class="header" href="#configuring-cni-networks"><a name="configuring-cni-networks"></a>Configuring CNI networks</a></h4>
<p>In order to configure the <code>network/cni</code> isolator the operator
specifies two flags at Agent startup as follows:</p>
<pre><code class="language-{.console}">sudo mesos-slave --master=&lt;master IP&gt; --ip=&lt;Agent IP&gt;
  --work_dir=/var/lib/mesos
  --network_cni_config_dir=&lt;location of CNI configs&gt;
  --network_cni_plugins_dir=&lt;search path for CNI plugins&gt;
</code></pre>
<p>Note that the <code>network/cni</code> isolator learns all the available networks
by looking at the CNI configuration in the <code>--network_cni_config_dir</code>
at startup. This implies that if a new CNI network needs to be added
after Agent startup, the Agent needs to be restarted. The
<code>network/cni</code> isolator has been designed with <code>recover</code> capabilities
and hence restarting the Agent (and therefore the <code>network/cni</code>
isolator) will not affect container orchestration.</p>
<p>Optionally, the operator could specify the
<code>--network_cni_root_dir_persist</code> flag. This flag would allow
<code>network/cni</code> isolator to persist the network related information
across reboot and allow <code>network/cni</code> isolator to carry out network
cleanup post reboot. This is useful for the CNI networks that depend
on the isolator to clean their network state.</p>
<h4 id="addingdeletingmodifying-cni-networks"><a class="header" href="#addingdeletingmodifying-cni-networks"><a name="adding-modifying-deleting"></a>Adding/Deleting/Modifying CNI networks</a></h4>
<p>The <code>network/cni</code> isolator learns about all the CNI networks by
reading the CNI configuration specified in <code>--network_cni_config_dir</code>.
Hence, if the operator wants to add a CNI network, the corresponding
configuration needs to be added to <code>--network_cni_config_dir</code>.</p>
<p>While the <code>network/cni</code> isolator learns the CNI networks by reading
the CNI configuration files in <code>--network_cni_config_dir</code>, it does not
keep an in-memory copy of the CNI configurations. The <code>network/cni</code>
isolator only stores a mapping of the CNI network names to the
corresponding CNI configuration files. Whenever the <code>network/cni</code>
isolator needs to attach a container to a CNI network it reads the
corresponding configuration from the disk and invokes the appropriate
plugin with the specified JSON configuration. Though the <code>network/cni</code>
isolator does not keep an in-memory copy of the JSON configuration, it
checkpoints the CNI configuration used to launch a container.
Checkpointing the CNI configuration protects the resources, associated
with the container, by freeing them correctly when the container is
destroyed, even if the CNI configuration is deleted.</p>
<p>The fact that the <code>network/cni</code> isolator always reads the CNI
configurations from the disk allows the operator to dynamically add,
modify and delete CNI configurations without the need to restart the
agent. Whenever the operator modifies an existing CNI configuration,
the agent will pick up this new CNI configuration when the next
container is launched on that specific CNI network. Similarly when the
operator deletes a CNI network the <code>network/cni</code> isolator will
&quot;unlearn&quot; the CNI network (since it will have a reference to this CNI
network when it started) in case a framework tries to launch a
container on the deleted CNI network.</p>
<h4 id="attaching-containers-to-cni-networks"><a class="header" href="#attaching-containers-to-cni-networks"><a name="attaching-containers-to-cni-networks"></a>Attaching containers to CNI networks</a></h4>
<p>Frameworks can specify the CNI network to which they want their
containers to be attached by setting the <code>name</code> field in the
<code>NetworkInfo</code> protobuf. The <code>name</code> field was introduced in the
<code>NetworkInfo</code> protobuf as part of
<a href="https://issues.apache.org/jira/browse/MESOS-4758">MESOS-4758</a>.  Also,
by specifying multiple instances of the <code>NetworkInfo</code> protobuf with
different <code>name</code> in each of the protobuf, the <code>MesosContainerizer</code>
will attach the container to all the different CNI networks specified.</p>
<p>The <strong>default behavior</strong> for containers is to join the <code>host network</code>, i.e., if the framework <em>does not</em> specify a <code>name</code> in the
<code>NetworkInfo</code> protobuf, the <code>network/cni</code> isolator will be a no-op for
that container and will not associate a new network namespace with the
container. This would effectively make the container use the host
network namespace, <code>attaching</code> it to the host network.</p>
<pre><code>**NOTE**: While specifying multiple `NetworkInfo` protobuf allows a
container to be attached to different CNI networks, if one of the
`NetworkInfo` protobuf is without the `name` field the `network/cni`
isolator simply &quot;skips&quot; the protobuf, attaching the container to all
the specified CNI networks except the `host network`.  To attach a
container to the host network as well as other CNI networks you
will need to attach the container to a CNI network (such as
bridge/macvlan) that, in turn, is attached to the host network.
</code></pre>
<h4 id="passing-network-labels-and-port-mapping-information-to-cni-plugins"><a class="header" href="#passing-network-labels-and-port-mapping-information-to-cni-plugins"><a name="network-labels-and-port-mapping">Passing network labels and port-mapping information to CNI plugins</a></a></h4>
<p>When invoking CNI plugins (e.g., with command ADD), the isolator will
pass on some Mesos meta-data to the plugins by specifying the <code>args</code>
field in the <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration">network configuration
JSON</a>
according to the CNI spec. Currently, the isolator only passes on
<code>NetworkInfo</code> of the corresponding network to the plugin. This is
simply the JSON representation of the <code>NetworkInfo</code> protobuf. For
instance:</p>
<pre><code class="language-{.json}">{
  &quot;name&quot; : &quot;mynet&quot;,
  &quot;type&quot; : &quot;bridge&quot;,
  &quot;args&quot; : {
    &quot;org.apache.mesos&quot; : {
      &quot;network_info&quot; : {
        &quot;name&quot; : &quot;mynet&quot;,
        &quot;labels&quot; : {
          &quot;labels&quot; : [
            { &quot;key&quot; : &quot;app&quot;, &quot;value&quot; : &quot;myapp&quot; },
            { &quot;key&quot; : &quot;env&quot;, &quot;value&quot; : &quot;prod&quot; }
          ]
        },
        &quot;port_mappings&quot; : [
          { &quot;host_port&quot; : 8080, &quot;container_port&quot; : 80 },
          { &quot;host_port&quot; : 8081, &quot;container_port&quot; : 443 }
        ]
      }
    }
  }
}
</code></pre>
<p>It is important to note that <code>labels</code> or <code>port_mappings</code> within the
<code>NetworkInfo</code> is set by frameworks launching the container, and the
isolator passses on this information to the CNI plugins. As per the
spec, it is the prerogative of the CNI plugins to use this meta-data
information as they see fit while attaching/detaching containers to a CNI
network. E.g., CNI plugins could use <code>labels</code> to enforce domain
specific policies, or <code>port_mappings</code> to implement NAT rules.</p>
<h4 id="accessing-container-network-namespace"><a class="header" href="#accessing-container-network-namespace"><a name="accessing-container-network-namespace"></a>Accessing container network namespace</a></h4>
<p>The <code>network/cni</code> isolator allocates a network namespace to a
container when it needs to attach the container to a CNI network. The
network namespace is checkpointed on the host file system and can be
useful to debug network connectivity to the network namespace. For a
given container the <code>network/cni</code> isolator checkpoints its network
namespace at:</p>
<pre><code class="language-{.console}">/var/run/mesos/isolators/network/cni/&lt;container ID&gt;/ns
</code></pre>
<p>The network namespace can be used with the <code>ip</code> command from the
<a href="http://man7.org/linux/man-pages/man8/ip-netns.8.html">iproute2</a>
package by creating a symbolic link to the network namespace. Assuming
the container ID is <code>5baff64c-d028-47ba-864e-a5ee679fc069</code> you can
create the symlink as follows:</p>
<pre><code class="language-{.console}">ln -s /var/run/mesos/isolators/network/cni/5baff64c-d028-47ba-8ff64c64e-a5ee679fc069/ns /var/run/netns/5baff64c
</code></pre>
<p>Now we can use the network namespace identifier <code>5baff64c</code> to run
commands in the new network name space using the
<a href="http://man7.org/linux/man-pages/man8/ip-netns.8.html">iproute2</a> package.
E.g. you can view all the links in the container network namespace by
running the command:</p>
<pre><code class="language-{.console}">ip netns exec 5baff64c ip link
</code></pre>
<p>Similarly you can view the container's route table by running:</p>
<pre><code class="language-{.console}">ip netns exec 5baff64c ip route show
</code></pre>
<p><em>NOTE</em>: Once
<a href="https://issues.apache.org/jira/browse/MESOS-5278">MESOS-5278</a> is
completed, executing commands within the container network namespace
would be simplified and we will no longer have a dependency on the
<code>iproute2</code> package to debug Mesos container networking.</p>
<h3 id="networking-recipes"><a class="header" href="#networking-recipes"><a name="networking-recipes"></a>Networking Recipes</a></h3>
<p>This section presents examples for launching containers on different
CNI networks. For each of the examples the assumption is that the CNI
configurations are present at <code>/var/lib/mesos/cni/config</code>, and the
plugins are present at <code>/var/lib/mesos/cni/plugins</code>. The Agents
therefore need to be started with the following command:</p>
<pre><code class="language-{.console}">sudo mesos-slave --master=&lt;master IP&gt; --ip=&lt;Agent IP&gt;
--work_dir=/var/lib/mesos
--network_cni_config_dir=/var/lib/mesos/cni/config
--network_cni_plugins_dir=/var/lib/mesos/cni/plugins
--isolation=filesystem/linux,docker/runtime
--image_providers=docker
</code></pre>
<p>Apart from the CNI configuration parameters, we are also starting the
Agent with the ability to launch docker images on
<code>MesosContainerizer</code>. We enable this ability in the
<code>MesosContainerizer</code> by enabling the <code>filesystem/linux</code> and
<code>docker/runtime</code> isolator and setting the image provider to
<code>docker</code>.</p>
<p>To present an example of a framework launching containers on a
specific CNI network, the <code>mesos-execute</code> CLI framework has been
modified to take a <code>--networks</code> flag which will allow this example
framework to launch containers on the specified network. You can find
the <code>mesos-execute</code> framework in your Mesos installation directory at
<code>&lt;mesos installation&gt;/bin/mesos-execute</code>.</p>
<h4 id="a-bridge-network"><a class="header" href="#a-bridge-network"><a name="a-bridge-network"></a>A bridge network</a></h4>
<p>The
<a href="https://github.com/containernetworking/cni/blob/master/Documentation/bridge.md">bridge</a>
plugin attaches containers to a Linux bridge. Linux bridges could be
configured to attach to VLANs and VxLAN allowing containers to be
plugged into existing layer 2 networks. We present an example below,
where the CNI configuration instructs the <code>MesosContainerizer</code> to
invoke a bridge plugin to connect a container to a Linux bridge. The
configuration also instructs the bridge plugin to assign an IP address
to the container by invoking a
<a href="https://github.com/containernetworking/cni/blob/master/Documentation/host-local.md">host-local</a>
IPAM.</p>
<p>First, build the CNI plugin according to the instructions in the <a href="https://github.com/containernetworking/cni">CNI
repository</a> then copy the
bridge binary to the plugins directory on each agent.</p>
<p>Next, create the configuration file and copy this to the CNI
configuration directory on each agent.</p>
<pre><code class="language-{.json}">{
&quot;name&quot;: &quot;cni-test&quot;,
&quot;type&quot;: &quot;bridge&quot;,
&quot;bridge&quot;: &quot;mesos-cni0&quot;,
&quot;isGateway&quot;: true,
&quot;ipMasq&quot;: true,
&quot;ipam&quot;: {
    &quot;type&quot;: &quot;host-local&quot;,
    &quot;subnet&quot;: &quot;192.168.0.0/16&quot;,
    &quot;routes&quot;: [
    { &quot;dst&quot;:
      &quot;0.0.0.0/0&quot; }
    ]
  }
}
</code></pre>
<p>The CNI configuration tells the bridge plugin to attach the
container to a bridge called <code>mesos-cni0</code>. If the bridge does not
exist the bridge plugin will create one.</p>
<p>It is important to note the <code>routes</code> section in the <code>ipam</code> dictionary.
For Mesos, the <code>executors</code> launched as containers need to register
with the Agent in order for a task to be successfully launched.
Hence, it is imperative that the Agent IP is reachable from the
container IP and vice versa. In this specific instance we specified a
default route for the container, allowing containers to reach any
network that will be routeable by the gateway, which for this CNI
configuration is the bridge itself.</p>
<p>Another interesting attribute in the CNI configuration is the <code>ipMasq</code>
option. Setting this to true will install an <code>iptable</code> rule in the
host network namespace that would SNAT all traffic originating from
the container and egressing the Agent. This allows containers to talk
to the outside world even when they are in an address space that is
not routeable from outside the agent.</p>
<p>Below we give an example of launching a <code>Ubuntu</code> container and
attaching it to the <code>mesos-cni0</code> bridge. You can launch the <code>Ubuntu</code>
container using the <code>mesos-execute</code> framework as follows:</p>
<pre><code class="language-{.console}">sudo mesos-execute --command=/bin/bash
  --docker_image=ubuntu:latest --master=&lt;master IP&gt;:5050 --name=ubuntu
  --networks=cni-test --no-shell
</code></pre>
<p>The above command would pull the <code>Ubuntu</code> image from the docker hub
and launch it using the <code>MesosContainerizer</code> and attach it to the
<code>mesos-cni0</code> bridge.</p>
<p>You can verify the network settings of the <code>Ubuntu</code> container by
creating a symlink to the network namespace and running the <code>ip</code>
command as describe in the section &quot;<a href="cni.html#accessing-container-network-namespace">Accessing container network
namespace</a>&quot;.</p>
<p>Assuming we created a reference for the network namespace in
<code>/var/run/netns/5baff64c</code> . The output of the IP address and route table
in the container network namespace would be as follows:</p>
<pre><code class="language-{.console}">$ sudo ip netns exec 5baff64c ip addr show
1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 8a:2c:f9:41:0a:54 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.2/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::882c:f9ff:fe41:a54/64 scope link
       valid_lft forever preferred_lft forever

$ sudo ip netns exec 5baff64c ip route show
default via 192.168.0.1 dev eth0
192.168.0.0/16 dev eth0  proto kernel  scope link  src 192.168.0.2
</code></pre>
<h4 id="a-port-mapper-plugin-for-cni-networks"><a class="header" href="#a-port-mapper-plugin-for-cni-networks"><a name="a-port-mapper-plugin">A port-mapper plugin for CNI networks</a></a></h4>
<p>For private, isolated, networks such as a bridge network where the IP
address of a container is not routeable from outside the host it
becomes imperative to provide containers with DNAT capabilities so
that services running on the container can be exposed outside the host
on which the container is running.</p>
<p>Unfortunately, there is no CNI plugin available in the
<a href="https://github.com/containernetworking/cni">containernetworking/cni</a>
repository that provides port-mapping functionality.
Hence, we have developed a port-mapper CNI plugin that resides
within the Mesos code base called the <code>mesos-cni-port-mapper</code>. The
<code>mesos-cni-port-mapper</code> is designed to work with any other CNI plugin
that requires DNAT capabilities. One of the most obvious being the
<code>bridge</code> CNI plugin.</p>
<p>We explain the operational semantics of the <code>mesos-cni-port-mapper</code>
plugin by taking an example CNI configuration that allows the
<code>mesos-cni-port-mapper</code> to provide DNAT functionality to the <code>bridge</code>
plugin.</p>
<pre><code>{
  &quot;name&quot; : &quot;port-mapper-test&quot;,
  &quot;type&quot; : &quot;mesos-cni-port-mapper&quot;,
  &quot;excludeDevices&quot; : [&quot;mesos-cni0&quot;],
  &quot;chain&quot;: &quot;MESOS-TEST-PORT-MAPPER&quot;,
  &quot;delegate&quot;: {
      &quot;type&quot;: &quot;bridge&quot;,
      &quot;bridge&quot;: &quot;mesos-cni0&quot;,
      &quot;isGateway&quot;: true,
      &quot;ipMasq&quot;: true,
      &quot;ipam&quot;: {
        &quot;type&quot;: &quot;host-local&quot;,
        &quot;subnet&quot;: &quot;192.168.0.0/16&quot;,
        &quot;routes&quot;: [
        { &quot;dst&quot;:
          &quot;0.0.0.0/0&quot; }
        ]
      }
  }
}
</code></pre>
<p>For the CNI configuration above, apart from the parameters that the
<code>mesos-cni-port-mapper</code> plugin accepts, the important point to note in
the CNI configuration of the plugin is the &quot;delegate&quot; field. The
&quot;delegate&quot; field allows the <code>mesos-cni-port-mapper</code> to wrap the CNI
configuration of any other CNI plugin, and allows the plugin to
provide DNAT capabilities to any CNI network. In this specific case
the <code>mesos-cni-port-mapper</code> is providing DNAT capabilities to
containers running on the bridge network <code>mesos-cni0</code>. The parameters
that the <code>mesos-cni-port-mapper</code> accepts are listed below:</p>
<ul>
<li><em><strong>name</strong></em> : Name of the CNI network.</li>
<li><em><strong>type</strong></em> : Name of the port-mapper CNI plugin.</li>
<li><em><strong>chain</strong></em> : The chain in which the iptables DNAT rule
will be added in the NAT table. This allows the operator to group
DNAT rules for a given CNI network under its own chain, allowing for
better management of the iptables rules.</li>
<li><em><strong>excludeDevices</strong></em>: These are a list of ingress devices on which the
DNAT rule should not be applied.</li>
<li><em><strong>delegate</strong></em> : This is a JSON dict that holds the CNI JSON configuration
of a CNI plugin that the port-mapper plugin is expected to invoke.</li>
</ul>
<p>The <code>mesos-cni-port-mapper</code> relies heavily on <code>iptables</code> to provide
the DNAT capabilities to a CNI network. In order for the port-mapper
plugin to function properly we have certain minimum
<strong>requirements for iptables</strong> as listed below:</p>
<ul>
<li><em><strong>iptables 1.4.20 or higher</strong></em>: This because we need to use the -w option
of iptables in order to allow atomic writes to iptables.</li>
<li><em><strong>Require the xt_comments module of iptables</strong></em>: We use the comments
module to tag iptables rules belonging to a container. These tags are used as a
key while deleting iptables rules when the specific container is deleted.</li>
</ul>
<p>Finally, while the CNI configuration of the port-mapper plugin tells
the plugin as to how and where to install the <code>iptables</code> rules, and which
CNI plugin to &quot;delegate&quot; the attachment/detachment of the container, the
port-mapping information itself is learned by looking at the
<code>NetworkInfo</code> set in the <code>args</code> field of the CNI configuration passed
by Mesos to the port-mapper plugin. Please refer to the &quot;<a href="cni.html#passing-network-labels-and-port-mapping-information-to-cni-plugins">Passing
network labels and port-mapping information to CNI
plugins</a>&quot; section for more details.</p>
<h4 id="a-calico-network"><a class="header" href="#a-calico-network"><a name="a-calico-network">A Calico network</a></a></h4>
<p><a href="https://projectcalico.org/">Calico</a> provides 3rd-party CNI plugin
that works out-of-the-box with Mesos CNI.</p>
<p>Calico takes a pure Layer-3 approach to networking, allocating a
unique, routable IP address to each Meso task. Task routes are
distributed by a BGP vRouter run on each Agent, which leverages the
existing Linux kernel forwarding engine without needing tunnels, NAT,
or overlays. Additionally, Calico supports rich and flexible network
policy which it enforces using bookended ACLs on each compute node to
provide tenant isolation, security groups, and external reachability
constraints.</p>
<p>For information on setting up and using Calico-CNI, see <a href="https://docs.projectcalico.org/v2.6/getting-started/mesos/">Calico's
guide on integerating with
Mesos</a>.</p>
<h4 id="a-cilium-network"><a class="header" href="#a-cilium-network"><a name="a-cilium-network">A Cilium network</a></a></h4>
<p><a href="https://www.cilium.io">Cilium</a> provides a CNI plugin that works with Mesos.</p>
<p>Cilium brings HTTP-aware network security filtering to Linux container
frameworks. Using a new Linux kernel technology called BPF, Cilium
provides a simple and efficient way to define and enforce both
network-layer and HTTP-layer security policies.</p>
<p>For more information on using Cilium with Mesos, check out the
<a href="http://docs.cilium.io/try-mesos">Getting Started Using Mesos Guide</a>.</p>
<h4 id="a-weave-network"><a class="header" href="#a-weave-network"><a name="a-weave-network">A Weave network</a></a></h4>
<p><a href="https://weave.works">Weave</a> provides a CNI implementation that works
out-of-the-box with Mesos.</p>
<p>Weave provides hassle free configuration by assigning an
ip-per-container and providing a fast DNS on each node. Weave is fast,
by automatically choosing the fastest path between hosts. Multicast
addressing and routing is fully supported. It has built in NAT
traversal and encryption and continues to work even during a network
partition.  Finally, Multi-cloud deployments are easy to setup and
maintain, even when there are multiple hops.</p>
<p>For more information on setting up and using Weave CNI, see <a href="https://www.weave.works/docs/net/latest/cni-plugin/">Weave's
CNI
documentation</a></p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Port Mapping Network Isolator
layout: documentation</h2>
<h1 id="port-mapping-network-isolator"><a class="header" href="#port-mapping-network-isolator">Port Mapping Network Isolator</a></h1>
<p>The port mapping network isolator provides a way to achieve
per-container network monitoring and isolation without relying on IP
per container.  The network isolator prevents a single container from
exhausting the available network ports, consuming an unfair share of
the network bandwidth or significantly delaying packet transmission
for others. Network statistics for each active container are published
through the
<a href="isolators/../endpoints/slave/monitor/statistics.html">/monitor/statistics</a>
endpoint on the agent. The port mapping network isolator is
transparent for the majority of tasks running on an agent (those that
bind to port 0 and let the kernel allocate their port).</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>Port mapping network isolator is <strong>not</strong> supported by default.  To
enable it you need to install additional dependencies and configure it
during the build process.</p>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<p>Per-container network monitoring and isolation is only supported on Linux kernel
versions 3.6 and above. Additionally, the kernel must include these patches
(merged in kernel version 3.15).</p>
<ul>
<li><a href="https://github.com/torvalds/linux/commit/6a662719c9868b3d6c7d26b3a085f0cd3cc15e64">6a662719c9868b3d6c7d26b3a085f0cd3cc15e64</a></li>
<li><a href="https://github.com/torvalds/linux/commit/0d5edc68739f1c1e0519acbea1d3f0c1882a15d7">0d5edc68739f1c1e0519acbea1d3f0c1882a15d7</a></li>
<li><a href="https://github.com/torvalds/linux/commit/e374c618b1465f0292047a9f4c244bd71ab5f1f0">e374c618b1465f0292047a9f4c244bd71ab5f1f0</a></li>
<li><a href="https://github.com/torvalds/linux/commit/25f929fbff0d1bcebf2e92656d33025cd330cbf8">25f929fbff0d1bcebf2e92656d33025cd330cbf8</a></li>
</ul>
<p>The following packages are required on the agent:</p>
<ul>
<li><a href="https://github.com/thom311/libnl/releases">libnl3</a> &gt;= 3.2.26</li>
<li><a href="http://www.linuxfoundation.org/collaborate/workgroups/networking/iproute2">iproute</a> &gt;= 2.6.39 is advised for debugging purpose but not required.</li>
</ul>
<p>Additionally, if you are building from source, you need will also need the
libnl3 development package to compile Mesos:</p>
<ul>
<li><a href="https://github.com/thom311/libnl/releases">libnl3-devel / libnl3-dev</a> &gt;= 3.2.26</li>
</ul>
<h3 id="build"><a class="header" href="#build">Build</a></h3>
<p>To build Mesos with port mapping network isolator support, you need to
add a configure option:</p>
<pre><code>$ ./configure --with-network-isolator
$ make
</code></pre>
<h2 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h2>
<p>The port mapping network isolator is enabled on the agent by adding
<code>network/port_mapping</code> to the agent command line <code>--isolation</code> flag.</p>
<pre><code>--isolation=&quot;network/port_mapping&quot;
</code></pre>
<p>If the agent has not been compiled with port mapping network isolator
support, it will refuse to start and print an error:</p>
<pre><code>I0708 00:17:08.080271 44267 containerizer.cpp:111] Using isolation: network/port_mapping
Failed to create a containerizer: Could not create MesosContainerizer: Unknown or unsupported
    isolator: network/port_mapping
</code></pre>
<h2 id="configuring-network-ports"><a class="header" href="#configuring-network-ports">Configuring network ports</a></h2>
<p>Without port mapping network isolator, all the containers on a host
share the public IP address of the agent and can bind to any port
allowed by the OS.</p>
<p>When the port mapping network isolator is enabled, each container on
the agent has a separate network stack (via Linux <a href="http://lwn.net/Articles/580893/">network
namespaces</a>).  All containers still
share the same public IP of the agent (so that the service discovery
mechanism does not need to be changed). The agent assigns each
container a non-overlapping range of the ports and only packets
to/from these assigned port ranges will be delivered. Applications
requesting the kernel assign a port (by binding to port 0) will be
given ports from the container assigned range. Applications can bind
to ports outside the container assigned ranges but packets from
to/from these ports will be silently dropped by the host.</p>
<p>Mesos provides two ranges of ports to containers:</p>
<ul>
<li>
<p>OS allocated &quot;<a href="https://en.wikipedia.org/wiki/Ephemeral_port">ephemeral</a>&quot; ports
are assigned by the OS in a range specified for each container by Mesos.</p>
</li>
<li>
<p>Mesos allocated &quot;non-ephemeral&quot; ports are acquired by a framework using the
same Mesos resource offer mechanism used for cpu, memory etc. for allocation to
executors/tasks as required.</p>
</li>
</ul>
<p>Additionally, the host itself will require ephemeral ports for network
communication. You need to configure these three <strong>non-overlapping</strong> port ranges
on the host.</p>
<h3 id="host-ephemeral-port-range"><a class="header" href="#host-ephemeral-port-range">Host ephemeral port range</a></h3>
<p>The currently configured host ephemeral port range can be discovered at any time
using the command <code>sysctl net.ipv4.ip_local_port_range</code>. If ports need to be set
aside for agent containers, the ephemeral port range can be updated in
<code>/etc/sysctl.conf</code>. Rebooting after the update will apply the change and
eliminate the possibility that ports are already in use by other processes. For
example, by adding the following:</p>
<pre><code># net.ipv4.ip_local_port_range defines the host ephemeral port range, by
# default 32768-61000.  We reduce this range to allow the Mesos agent to
# allocate ports 32768-57344
# net.ipv4.ip_local_port_range = 32768 61000
net.ipv4.ip_local_port_range = 57345 61000
</code></pre>
<h3 id="container-port-ranges"><a class="header" href="#container-port-ranges">Container port ranges</a></h3>
<p>The container ephemeral and non-ephemeral port ranges are configured using the
agent <code>--resources</code> flag. The non-ephemeral port range is provided to the
master, which will then offer it to frameworks for allocation.</p>
<p>The ephemeral port range is sub-divided by the agent, giving
<code>ephemeral_ports_per_container</code> (default 1024) to each container. The maximum
number of containers on the agent will therefore be limited to approximately:</p>
<pre><code>number of ephemeral_ports / ephemeral_ports_per_container
</code></pre>
<p>The master <code>--max_executors_per_agent</code> flag is be used to prevent allocation of
more executors on an agent when the ephemeral port range has been exhausted.</p>
<p>It is recommended (but not required) that <code>ephemeral_ports_per_container</code> be set
to a power of 2 (e.g., 512, 1024) and the lower bound of the ephemeral port
range be a multiple of <code>ephemeral_ports_per_container</code> to minimize CPU overhead
in packet processing. For example:</p>
<pre><code>--resources=ports:[31000-32000];ephemeral_ports:[32768-57344] \
--ephemeral_ports_per_container=512
</code></pre>
<h3 id="rate-limiting-container-traffic"><a class="header" href="#rate-limiting-container-traffic">Rate limiting container traffic</a></h3>
<p>Outbound traffic from a container to the network can be rate limited to prevent
a single container from consuming all available network resources with
detrimental effects to the other containers on the host. The
<code>--egress_rate_limit_per_container</code> flag specifies that each container launched
on the host be limited to the specified bandwidth (in bytes per second).
Network traffic which would cause this limit to be exceeded is delayed for later
transmission. The TCP protocol will adjust to the increased latency and reduce
the transmission rate ensuring no packets need be dropped.</p>
<pre><code>--egress_rate_limit_per_container=100MB
</code></pre>
<p>We do not rate limit inbound traffic since we can only modify the network flows
after they have been received by the host and any congestion has already
occurred.</p>
<h3 id="egress-traffic-isolation"><a class="header" href="#egress-traffic-isolation">Egress traffic isolation</a></h3>
<p>Delaying network data for later transmission can increase latency and jitter
(variability) for all traffic on the interface. Mesos can reduce the impact on
other containers on the same host by using flow classification and isolation
using the containers port ranges to maintain unique flows for each container and
sending traffic from these flows fairly (using the
<a href="https://tools.ietf.org/html/draft-hoeiland-joergensen-aqm-fq-codel-00">FQ_Codel</a>
algorithm). Use the <code>--egress_unique_flow_per_container</code> flag to enable.</p>
<pre><code>--egress_unique_flow_per_container
</code></pre>
<h3 id="putting-it-all-together"><a class="header" href="#putting-it-all-together">Putting it all together</a></h3>
<p>A complete agent command line enabling port mapping network isolator,
reserving ports 57345-61000 for host ephemeral ports, 32768-57344 for
container ephemeral ports, 31000-32000 for non-ephemeral ports
allocated by the framework, limiting container transmit bandwidth to
300 Mbits/second (37.5MBytes) with unique flows enabled would thus be:</p>
<pre><code>mesos-agent \
--isolation=network/port_mapping \
--resources=ports:[31000-32000];ephemeral_ports:[32768-57344] \
--ephemeral_ports_per_container=1024 \
--egress_rate_limit_per_container=37500KB \
--egress_unique_flow_per_container
</code></pre>
<h2 id="monitoring-container-network-statistics"><a class="header" href="#monitoring-container-network-statistics">Monitoring container network statistics</a></h2>
<p>Mesos exposes statistics from the Linux network stack for each container network
on the <a href="isolators/../endpoints/slave/monitor/statistics.html">/monitor/statistics</a> agent endpoint.</p>
<p>From the network interface inside the container, we report the following
counters (since container creation) under the <code>statistics</code> key:</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td><code>net_rx_bytes</code></td>
  <td>Received bytes</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_rx_dropped</code></td>
  <td>Packets dropped on receive</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_rx_errors</code></td>
  <td>Errors reported on receive</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_rx_packets</code></td>
  <td>Packets received</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_tx_bytes</code></td>
  <td>Sent bytes</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_tx_dropped</code></td>
  <td>Packets dropped on send</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_tx_errors</code></td>
  <td>Errors reported on send</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>net_tx_packets</code></td>
  <td>Packets sent</td>
  <td>Counter</td>
</tr>
</table>
<p>Additionally, <a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html">Linux Traffic Control</a> can report the following
statistics for the elements which implement bandwidth limiting and bloat
reduction under the <code>statistics/net_traffic_control_statistics</code> key. The entry
for each of these elements includes:</p>
<table class="table table-striped">
<thead>
<tr><th>Metric</th><th>Description</th><th>Type</th>
</thead>
<tr>
  <td><code>backlog</code></td>
  <td>Bytes queued for transmission [1]</td>
  <td>Gauge</td>
</tr>
<tr>
  <td><code>bytes</code></td>
  <td>Sent bytes</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>drops</code></td>
  <td>Packets dropped on send</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>overlimits</code></td>
  <td>Count of times the interface was over its transmit limit when it attempted to send a packet.  Since the normal action when the network is overlimit is to delay the packet, the overlimit counter can be incremented many times for each packet sent on a heavily congested interface. [2]</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>packets</code></td>
  <td>Packets sent</td>
  <td>Counter</td>
</tr>
<tr>
  <td><code>qlen</code></td>
  <td>Packets queued for transmission</td>
  <td>Gauge</td>
</tr>
<tr>
  <td><code>ratebps</code></td>
  <td>Transmit rate in bytes/second [3]</td>
  <td>Gauge</td>
</tr>
<tr>
  <td><code>ratepps</code></td>
  <td>Transmit rate in packets/second [3]</td>
  <td>Gauge</td>
</tr>
<tr>
  <td><code>requeues</code></td>
  <td>Packets failed to send due to resource contention (such as kernel locking) [3]</td>
  <td>Counter</td>
</tr>
</table>
<p>[1] <code>backlog</code> is only reported on the bloat_reduction interface.</p>
<p>[2] <code>overlimits</code> are only reported on the bw_limit interface.</p>
<p>[3] Currently always reported as 0 by the underlying Traffic Control element.</p>
<p>For example, these are the statistics you will get by hitting the <code>/monitor/statistics</code> endpoint on an agent with network monitoring turned on:</p>
<pre><code>$ curl -s http://localhost:5051/monitor/statistics | python2.6 -mjson.tool
[
    {
        &quot;executor_id&quot;: &quot;job.1436298853&quot;,
        &quot;executor_name&quot;: &quot;Command Executor (Task: job.1436298853) (Command: sh -c 'iperf ....')&quot;,
        &quot;framework_id&quot;: &quot;20150707-195256-1740121354-5150-29801-0000&quot;,
        &quot;source&quot;: &quot;job.1436298853&quot;,
        &quot;statistics&quot;: {
            &quot;cpus_limit&quot;: 1.1,
            &quot;cpus_nr_periods&quot;: 16314,
            &quot;cpus_nr_throttled&quot;: 16313,
            &quot;cpus_system_time_secs&quot;: 2667.06,
            &quot;cpus_throttled_time_secs&quot;: 8036.840845388,
            &quot;cpus_user_time_secs&quot;: 123.49,
            &quot;mem_anon_bytes&quot;: 8388608,
            &quot;mem_cache_bytes&quot;: 16384,
            &quot;mem_critical_pressure_counter&quot;: 0,
            &quot;mem_file_bytes&quot;: 16384,
            &quot;mem_limit_bytes&quot;: 167772160,
            &quot;mem_low_pressure_counter&quot;: 0,
            &quot;mem_mapped_file_bytes&quot;: 0,
            &quot;mem_medium_pressure_counter&quot;: 0,
            &quot;mem_rss_bytes&quot;: 8388608,
            &quot;mem_total_bytes&quot;: 9945088,
            &quot;net_rx_bytes&quot;: 10847,
            &quot;net_rx_dropped&quot;: 0,
            &quot;net_rx_errors&quot;: 0,
            &quot;net_rx_packets&quot;: 143,
            &quot;net_traffic_control_statistics&quot;: [
                {
                    &quot;backlog&quot;: 0,
                    &quot;bytes&quot;: 163206809152,
                    &quot;drops&quot;: 77147,
                    &quot;id&quot;: &quot;bw_limit&quot;,
                    &quot;overlimits&quot;: 210693719,
                    &quot;packets&quot;: 107941027,
                    &quot;qlen&quot;: 10236,
                    &quot;ratebps&quot;: 0,
                    &quot;ratepps&quot;: 0,
                    &quot;requeues&quot;: 0
                },
                {
                    &quot;backlog&quot;: 15481368,
                    &quot;bytes&quot;: 163206874168,
                    &quot;drops&quot;: 27081494,
                    &quot;id&quot;: &quot;bloat_reduction&quot;,
                    &quot;overlimits&quot;: 0,
                    &quot;packets&quot;: 107941070,
                    &quot;qlen&quot;: 10239,
                    &quot;ratebps&quot;: 0,
                    &quot;ratepps&quot;: 0,
                    &quot;requeues&quot;: 0
                }
            ],
            &quot;net_tx_bytes&quot;: 163200529816,
            &quot;net_tx_dropped&quot;: 0,
            &quot;net_tx_errors&quot;: 0,
            &quot;net_tx_packets&quot;: 107936874,
            &quot;perf&quot;: {
                &quot;duration&quot;: 0,
                &quot;timestamp&quot;: 1436298855.82807
            },
            &quot;timestamp&quot;: 1436300487.41595
        }
    }
]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Multiple Disks
layout: documentation</h2>
<h1 id="multiple-disks"><a class="header" href="#multiple-disks">Multiple Disks</a></h1>
<p>Mesos provides a mechanism for operators to expose multiple disk resources. When
creating <a href="persistent-volume.html">persistent volumes</a> frameworks can decide
whether to use specific disks by examining the <code>source</code> field on the disk
resources offered.</p>
<h2 id="types-of-disk-resources"><a class="header" href="#types-of-disk-resources">Types of Disk Resources</a></h2>
<p><code>Disk</code> resources come in three forms:</p>
<ul>
<li>A <code>Root</code> disk is presented by not having the <code>source</code> set in <code>DiskInfo</code>.</li>
<li>A <code>Path</code> disk is presented by having the <code>PATH</code> enum set for <code>source</code> in
<code>DiskInfo</code>. It also has a <code>root</code> which the operator uses to specify the
directory to be used to store data.</li>
<li>A <code>Mount</code> disk is presented by having the <code>MOUNT</code> enum set for <code>source</code> in
<code>DiskInfo</code>. It also has a <code>root</code> which the operator uses to specify the
mount point used to store data.</li>
</ul>
<p>Operators can use the JSON-formated <code>--resources</code> option on the agent to provide
these different kind of disk resources on agent start-up. Example resource
values in JSON format can be found below. By default (if <code>--resources</code> is not
specified), the Mesos agent will only make the root disk available to the
cluster.</p>
<p><strong>NOTE:</strong> Once you specify any <code>Disk</code> resource manually (i.e., via the
<code>--resources</code> flag), Mesos will stop auto-detecting the <code>Root</code> disk resource.
Hence if you want to use the <code>Root</code> disk you will need to manually specify it
using the format described below.</p>
<h3 id="root-disk"><a class="header" href="#root-disk"><code>Root</code> disk</a></h3>
<p>A <code>Root</code> disk is the basic disk resource in Mesos. It usually maps to the
storage on the main operating system drive that the operator has presented to
the agent. Data is mapped into the <code>work_dir</code> of the agent.</p>
<p>An example resources value for a root disk is shown below. Note that the
operator could optionally specify a <code>role</code> for the disk, which would result in
<a href="reservation.html">statically reserving</a> the disk for a single <a href="roles.html">role</a>.</p>
<pre><code>    [
      {
        &quot;name&quot; : &quot;disk&quot;,
        &quot;type&quot; : &quot;SCALAR&quot;,
        &quot;scalar&quot; : { &quot;value&quot; : 2048 }
      }
    ]
</code></pre>
<h3 id="path-disks"><a class="header" href="#path-disks"><code>Path</code> disks</a></h3>
<p>A <code>Path</code> disk is an auxiliary disk resource provided by the operator. This
can be carved up into smaller chunks by creating persistent volumes that use
less than the total available space on the disk. Common uses for this kind of
disk are extra logging space, file archives or caches, or other non
performance-critical applications.  Operators can present extra disks on their
agents as <code>Path</code> disks simply by creating a directory and making that the <code>root</code>
of the <code>Path</code> in <code>DiskInfo</code>'s <code>source</code>.</p>
<p><code>Path</code> disks are also useful for mocking up a multiple disk environment by
creating some directories on the operating system drive. This should only be
done in a testing or staging environment. Note that creating multiple <code>Path</code>
disks on the same filesystem requires statically partitioning the available disk
space. For example, suppose a 10GB storage device is mounted to <code>/foo</code> and the
Mesos agent is configured with two <code>Path</code> disks at <code>/foo/disk1</code> and
<code>/foo/disk2</code>. To avoid the risk of running out of space on the device, <code>disk1</code>
and <code>disk2</code> should be configured (when the Mesos agent is started) to use at
most 10GB of disk space in total.</p>
<p>An example resources value for a <code>Path</code> disk is shown below. Note that the
operator could optionally specify a <code>role</code> for the disk, which would result in
<a href="reservation.html">statically reserving</a> the disk for a single <a href="roles.html">role</a>.</p>
<pre><code>    [
      {
        &quot;name&quot; : &quot;disk&quot;,
        &quot;type&quot; : &quot;SCALAR&quot;,
        &quot;scalar&quot; : { &quot;value&quot; : 2048 },
        &quot;disk&quot; : {
          &quot;source&quot; : {
            &quot;type&quot; : &quot;PATH&quot;,
            &quot;path&quot; : { &quot;root&quot; : &quot;/mnt/data&quot; }
          }
        }
      }
    ]
</code></pre>
<h3 id="mount-disks"><a class="header" href="#mount-disks"><code>Mount</code> disks</a></h3>
<p>A <code>Mount</code> disk is an auxiliary disk resource provided by the operator. This
<strong>cannot</strong> be carved up into smaller chunks by frameworks. This lack of
flexibility allows operators to provide assurances to frameworks that they will
have exclusive access to the disk device. Common uses for this kind of disk
include database storage, write-ahead logs, or other performance-critical
applications.</p>
<p>On Linux, <code>Mount</code> disks must map to a <code>mount</code> point in the <code>/proc/mounts</code>
table. Operators should mount a physical disk with their preferred file system
and provide the mount point as the <code>root</code> of the <code>Mount</code> in <code>DiskInfo</code>'s
<code>source</code>.</p>
<p>Aside from the performance advantages of <code>Mount</code> disks, applications running on
them should be able to rely on disk errors when they attempt to exceed the
capacity of the volume. This holds true as long as the file system in use
correctly propagates these errors. Due to this expectation, the <code>disk/du</code>
isolation is disabled for <code>Mount</code> disks.</p>
<p>An example resources value for a <code>Mount</code> disk is shown below. Note that the
operator could optionally specify a <code>role</code> for the disk, which would result in
<a href="reservation.html">statically reserving</a> the disk for a single <a href="roles.html">role</a>.</p>
<pre><code>    [
      {
        &quot;name&quot; : &quot;disk&quot;,
        &quot;type&quot; : &quot;SCALAR&quot;,
        &quot;scalar&quot; : { &quot;value&quot; : 2048 },
        &quot;disk&quot; : {
          &quot;source&quot; : {
            &quot;type&quot; : &quot;MOUNT&quot;,
            &quot;mount&quot; : { &quot;root&quot; : &quot;/mnt/data&quot; }
          }
        }
      }
    ]
</code></pre>
<h4 id="block-disks"><a class="header" href="#block-disks"><code>Block</code> disks</a></h4>
<p>Mesos currently does not allow operators to expose raw block devices. It may do
so in the future, but there are security and flexibility concerns that need to
be addressed in a design document first.</p>
<h3 id="implementation-1"><a class="header" href="#implementation-1">Implementation</a></h3>
<p>A <code>Path</code> disk will have sub-directories created within the <code>root</code> which will be
used to differentiate the different volumes that are created on it. When a
persistent volume on a <code>Path</code> disk is destroyed, Mesos will remove all the files
and directories stored in the volume, as well as the sub-directory within <code>root</code>
that was created by Mesos for the volume.</p>
<p>A <code>Mount</code> disk will <strong>not</strong> have sub-directories created, allowing applications
to use the full file system mounted on the device. This construct allows Mesos
tasks to access volumes that contain pre-existing directory structures. This can
be useful to simplify ingesting data such as a pre-existing Postgres database or
HDFS data directory. Note that when a persistent volume on a <code>Mount</code> disk is
destroyed, Mesos will remove all the files and directories stored in the volume,
but will <strong>not</strong> remove the root directory (i.e., the mount point).</p>
<p>Operators should be aware of these distinctions when inspecting or cleaning up
remnant data.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Persistent Volumes
layout: documentation</h2>
<h1 id="persistent-volumes"><a class="header" href="#persistent-volumes">Persistent Volumes</a></h1>
<p>Mesos supports creating persistent volumes from disk resources. When
launching a task, you can create a volume that exists outside the
task's sandbox and will persist on the node even after the task dies or
completes. When the task exits, its resources -- including the persistent volume
-- can be offered back to the framework, so that the framework can launch the
same task again, launch a recovery task, or launch a new task that consumes the
previous task's output as its input.</p>
<p>Persistent volumes enable stateful services such as HDFS and Cassandra
to store their data within Mesos rather than having to resort to
workarounds (e.g., writing task state to a distributed filesystem that
is mounted at a well-known location outside the task's sandbox).</p>
<h2 id="usage-3"><a class="header" href="#usage-3">Usage</a></h2>
<p>Persistent volumes can only be created from <strong>reserved</strong> disk resources, whether
it be statically reserved or dynamically reserved. A dynamically reserved
persistent volume also cannot be unreserved without first explicitly destroying
the volume. These rules exist to limit accidental mistakes, such as a persistent
volume containing sensitive data being offered to other frameworks in the
cluster. Similarly, a persistent volume cannot be destroyed if there is an
active task that is still using the volume.</p>
<p>Please refer to the <a href="reservation.html">Reservation</a> documentation for details
regarding reservation mechanisms available in Mesos.</p>
<p>Persistent volumes can also be created on isolated and auxiliary disks by
reserving <a href="multiple-disk.html">multiple disk resources</a>.</p>
<p>By default, a persistent volume cannot be shared between tasks running
under different executors: that is, once a task is launched using a
persistent volume, that volume will not appear in any resource offers
until the task has finished running. <em>Shared</em> volumes are a type of
persistent volumes that can be accessed by multiple tasks at the same
agent simultaneously; see the documentation on <a href="shared-resources.html">shared
volumes</a> for more information.</p>
<p>Persistent volumes can be created by <strong>operators</strong> and <strong>frameworks</strong>.
By default, frameworks and operators can create volumes for <em>any</em>
role and destroy <em>any</em> persistent volume. <a href="authorization.html">Authorization</a>
allows this behavior to be limited so that volumes can only be created for
particular roles and only particular volumes can be destroyed. For these
operations to be authorized, the framework or operator should provide a
<code>principal</code> to identify itself. To use authorization with reserve, unreserve,
create, and destroy operations, the Mesos master must be configured with the
appropriate ACLs. For more information, see the
<a href="authorization.html">authorization documentation</a>.</p>
<ul>
<li>The following messages are available for <strong>frameworks</strong> to send back via the
<code>acceptOffers</code> API as a response to a resource offer:
<ul>
<li><code>Offer::Operation::Create</code></li>
<li><code>Offer::Operation::Destroy</code></li>
<li><code>Offer::Operation::GrowVolume</code></li>
<li><code>Offer::Operation::ShrinkVolume</code></li>
</ul>
</li>
<li>For each message in above list, a corresponding call in
<a href="operator-http-api.html">HTTP Operator API</a> is available for operators or
administrative tools;</li>
<li><code>/create-volumes</code> and <code>/destroy-volumes</code> HTTP endpoints allow
<strong>operators</strong> to manage persistent volumes through the master.</li>
</ul>
<p>When a persistent volume is destroyed, all the data on that volume is removed
from the agent's filesystem. Note that for persistent volumes created on <code>Mount</code>
disks, the root directory is not removed, because it is typically the mount
point used for a separate storage device.</p>
<p>In the following sections, we will walk through examples of each of the
interfaces described above.</p>
<h2 id="framework-api-2"><a class="header" href="#framework-api-2">Framework API</a></h2>
<p><a name="offer-operation-create"></a></p>
<h3 id="offeroperationcreate"><a class="header" href="#offeroperationcreate"><code>Offer::Operation::Create</code></a></h3>
<p>A framework can create volumes through the resource offer cycle.  Suppose we
receive a resource offer with 2048 MB of dynamically reserved disk:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    }
  ]
}
</code></pre>
<p>We can create a persistent volume from the 2048 MB of disk resources by sending
an <code>Offer::Operation</code> message via the <code>acceptOffers</code> API.
<code>Offer::Operation::Create</code> has a <code>volumes</code> field which specifies the persistent
volume information. We need to specify the following:</p>
<ol>
<li>
<p>The ID for the persistent volume; this must be unique per role on each agent.</p>
</li>
<li>
<p>The non-nested relative path within the container to mount the volume.</p>
</li>
<li>
<p>The permissions for the volume. Currently, <code>&quot;RW&quot;</code> is the only possible value.</p>
</li>
<li>
<p>If the framework provided a principal when registering with the master, then
the <code>disk.persistence.principal</code> field must be set to that principal. If the
framework did not provide a principal when registering, then the
<code>disk.persistence.principal</code> field can take any value, or can be left unset.
Note that the <code>principal</code> field determines the &quot;creator principal&quot; when
<a href="authorization.html">authorization</a> is enabled, even if authentication is
disabled.</p>
<pre><code> {
   &quot;type&quot; : Offer::Operation::CREATE,
   &quot;create&quot;: {
     &quot;volumes&quot; : [
       {
         &quot;name&quot; : &quot;disk&quot;,
         &quot;type&quot; : &quot;SCALAR&quot;,
         &quot;scalar&quot; : { &quot;value&quot; : 2048 },
         &quot;role&quot; : &lt;offer's allocation role&gt;,
         &quot;reservation&quot; : {
           &quot;principal&quot; : &lt;framework_principal&gt;
         },
         &quot;disk&quot;: {
           &quot;persistence&quot;: {
             &quot;id&quot; : &lt;persistent_volume_id&gt;,
             &quot;principal&quot; : &lt;framework_principal&gt;
           },
           &quot;volume&quot; : {
             &quot;container_path&quot; : &lt;container_path&gt;,
             &quot;mode&quot; : &lt;mode&gt;
           }
         }
       }
     ]
   }
 }
</code></pre>
</li>
</ol>
<p>If this succeeds, a subsequent resource offer will contain the following
persistent volume:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    }
  ]
}
</code></pre>
<h3 id="offeroperationdestroy"><a class="header" href="#offeroperationdestroy"><code>Offer::Operation::Destroy</code></a></h3>
<p>A framework can destroy persistent volumes through the resource offer cycle. In
<a href="persistent-volume.html#offer-operation-create">Offer::Operation::Create</a>, we created a persistent
volume from 2048 MB of disk resources. The volume will continue to exist until
it is explicitly destroyed. Suppose we would like to destroy the volume we
created. First, we receive a resource offer (copy/pasted from above):</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    }
  ]
}
</code></pre>
<p>We can destroy the persistent volume by sending an <code>Offer::Operation</code> message
via the <code>acceptOffers</code> API. <code>Offer::Operation::Destroy</code> has a <code>volumes</code> field
which specifies the persistent volumes to be destroyed.</p>
<pre><code>{
  &quot;type&quot; : Offer::Operation::DESTROY,
  &quot;destroy&quot; : {
    &quot;volumes&quot; : [
      {
        &quot;name&quot; : &quot;disk&quot;,
        &quot;type&quot; : &quot;SCALAR&quot;,
        &quot;scalar&quot; : { &quot;value&quot; : 2048 },
        &quot;role&quot; : &lt;offer's allocation role&gt;,
        &quot;reservation&quot; : {
          &quot;principal&quot; : &lt;framework_principal&gt;
        },
        &quot;disk&quot;: {
          &quot;persistence&quot;: {
            &quot;id&quot; : &lt;persistent_volume_id&gt;
          },
          &quot;volume&quot; : {
            &quot;container_path&quot; : &lt;container_path&gt;,
            &quot;mode&quot; : &lt;mode&gt;
          }
        }
      }
    ]
  }
}
</code></pre>
<p>If this request succeeds, the persistent volume will be destroyed, and all
files and directories associated with the volume will be deleted. However, the
disk resources will still be reserved. As such, a subsequent resource offer will
contain the following reserved disk resources:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    }
  ]
}
</code></pre>
<p>Those reserved resources can then be used as normal: e.g., they can be used to
create another persistent volume or can be unreserved.</p>
<p><a name="offer-operation-grow-volume"></a></p>
<h3 id="offeroperationgrowvolume"><a class="header" href="#offeroperationgrowvolume"><code>Offer::Operation::GrowVolume</code></a></h3>
<p>Sometimes, a framework or an operator may find that the size of an existing
persistent volume may be too small (possibly due to increased usage). In
<a href="persistent-volume.html#offer-operation-create">Offer::Operation::Create</a>, we created a persistent
volume from 2048 MB of disk resources. Suppose we want to grow the size of
the volume to 4096 MB, we first need resource offer(s) with at least 2048 MB of
disk resources with the same reservation and disk information:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    },
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    }
  ]
}
</code></pre>
<p>We can grow the persistent volume by sending an <code>Offer::Operation</code> message.
<code>Offer::Operation::GrowVolume</code> has a <code>volume</code> field which specifies the
persistent volume to grow, and an <code>addition</code> field which specifies the
additional disk space resource.</p>
<pre><code>{
  &quot;type&quot; : Offer::Operation::GROW_VOLUME,
  &quot;grow_volume&quot; : {
    &quot;volume&quot; : {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    },
   &quot;addition&quot; : {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    }
  }
}
</code></pre>
<p>If this request succeeds, the persistent volume will be grown to the new size,
and all files and directories associated with the volume will not be touched.
A subsequent resource offer will contain the grown volume:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 4096 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    }
  ]
}
</code></pre>
<p><a name="offer-operation-shrink-volume"></a></p>
<h3 id="offeroperationshrinkvolume"><a class="header" href="#offeroperationshrinkvolume"><code>Offer::Operation::ShrinkVolume</code></a></h3>
<p>Similarly, a framework or an operator may find that the size of an existing
persistent volume may be too large (possibly due to over provisioning), and want
to free up unneeded disk space resources.
In <a href="persistent-volume.html#offer-operation-create">Offer::Operation::Create</a>, we created a persistent
volume from 2048 MB of disk resources. Suppose we want to shrink the size of
the volume to 1024 MB, we first need a resource offer with the volume to shrink:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    }
  ]
}
</code></pre>
<p>We can shrink the persistent volume by sending an <code>Offer::Operation</code> message via
the <code>acceptOffers</code> API. <code>Offer::Operation::ShrinkVolume</code> has a <code>volume</code> field
which specifies the persistent volume to grow, and a <code>subtract</code> field which
specifies the scalar value of disk space to subtract from the volume:</p>
<pre><code>{
  &quot;type&quot; : Offer::Operation::SHRINK_VOLUME,
  &quot;shrink_volume&quot; : {
    &quot;volume&quot; : {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 2048 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    },
   &quot;subtract&quot; : {
      &quot;value&quot; : 1024
    }
  }
}
</code></pre>
<p>If this request succeeds, the persistent volume will be shrunk to the new size,
and all files and directories associated with the volume will not be touched.
A subsequent resource offer will contain the shrunk volume as well as freed up
disk resources with the same reservation information:</p>
<pre><code>{
  &quot;id&quot; : &lt;offer_id&gt;,
  &quot;framework_id&quot; : &lt;framework_id&gt;,
  &quot;slave_id&quot; : &lt;slave_id&gt;,
  &quot;hostname&quot; : &lt;hostname&gt;,
  &quot;resources&quot; : [
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 1024 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      }
    },
    {
      &quot;name&quot; : &quot;disk&quot;,
      &quot;type&quot; : &quot;SCALAR&quot;,
      &quot;scalar&quot; : { &quot;value&quot; : 1024 },
      &quot;role&quot; : &lt;offer's allocation role&gt;,
      &quot;reservation&quot; : {
        &quot;principal&quot; : &lt;framework_principal&gt;
      },
      &quot;disk&quot;: {
        &quot;persistence&quot;: {
          &quot;id&quot; : &lt;persistent_volume_id&gt;
        },
        &quot;volume&quot; : {
          &quot;container_path&quot; : &lt;container_path&gt;,
          &quot;mode&quot; : &lt;mode&gt;
        }
      }
    }
  ]
}
</code></pre>
<p>Some restrictions about resizing a volume (applicable to both
<a href="persistent-volume.html#offer-operation-grow-volume">Offer::Operation::GrowVolume</a> and
<a href="persistent-volume.html#offer-operation-shrink-volume">Offer::Operation::ShrinkVolume</a>):</p>
<ul>
<li>Only persistent volumes created on an agent's local disk space with <code>ROOT</code> or
<code>PATH</code> type can be resized;</li>
<li>A persistent volume cannot be actively used by a task when being resized;</li>
<li>A persistent volume cannot be shared when being resized;</li>
<li>Volume resize operations cannot be included in an ACCEPT call with other
operations which make use of the resized volume.</li>
</ul>
<h2 id="versioned-http-operator-api"><a class="header" href="#versioned-http-operator-api">Versioned HTTP Operator API</a></h2>
<p>As described above, persistent volumes can be created by a framework scheduler
as part of the resource offer cycle. Persistent volumes can also be managed
using the <a href="operator-http-api.html">HTTP Operator API</a>.</p>
<p>This capability is intended for use by operators and administrative tools.</p>
<p>For each offer operation which interacts with persistent volume, there is an
equivalent call in master's <a href="operator-http-api.html">HTTP Operator API</a>.</p>
<h2 id="unversioned-operator-http-endpoints"><a class="header" href="#unversioned-operator-http-endpoints">Unversioned Operator HTTP Endpoints</a></h2>
<p>Several HTTP endpoints like
<a href="endpoints/master/create-volumes.html">/create-volumes</a> and
<a href="endpoints/master/destroy-volumes.html">/destroy-volumes</a> can still be used to
manage persisent volumes, but we generally encourage operators to use
versioned <a href="operator-http-api.html">HTTP Operator API</a> instead, as new features
like resize support may not be backported.</p>
<h3 id="create-volumes"><a class="header" href="#create-volumes"><code>/create-volumes</code></a></h3>
<p>To use this endpoint, the operator should first ensure that a reservation for
the necessary resources has been made on the appropriate agent (e.g., by using
the <a href="endpoints/master/reserve.html">/reserve</a> HTTP endpoint or by configuring a
static reservation). The information that must be included in a request to this
endpoint is similar to that of the <code>CREATE</code> offer operation. One difference is
the required value of the <code>disk.persistence.principal</code> field: when HTTP
authentication is enabled on the master, the field must be set to the same
principal that is provided in the request's HTTP headers. When HTTP
authentication is disabled, the <code>disk.persistence.principal</code> field can take any
value, or can be left unset. Note that the <code>principal</code> field determines the
&quot;creator principal&quot; when <a href="authorization.html">authorization</a> is enabled, even if
HTTP authentication is disabled.</p>
<p>To create a 512MB persistent volume for the <code>ads</code> role on a dynamically reserved
disk resource, we can send an HTTP POST request to the master's
<a href="endpoints/master/create-volumes.html">/create-volumes</a> endpoint like so:</p>
<pre><code>curl -i \
     -u &lt;operator_principal&gt;:&lt;password&gt; \
     -d slaveId=&lt;slave_id&gt; \
     -d volumes='[
       {
         &quot;name&quot;: &quot;disk&quot;,
         &quot;type&quot;: &quot;SCALAR&quot;,
         &quot;scalar&quot;: { &quot;value&quot;: 512 },
         &quot;role&quot;: &quot;ads&quot;,
         &quot;reservation&quot;: {
           &quot;principal&quot;: &lt;operator_principal&gt;
         },
         &quot;disk&quot;: {
           &quot;persistence&quot;: {
             &quot;id&quot; : &lt;persistence_id&gt;,
             &quot;principal&quot; : &lt;operator_principal&gt;
           },
           &quot;volume&quot;: {
             &quot;mode&quot;: &quot;RW&quot;,
             &quot;container_path&quot;: &lt;path&gt;
           }
         }
       }
     ]' \
     -X POST http://&lt;ip&gt;:&lt;port&gt;/master/create-volumes
</code></pre>
<p>The user receives one of the following HTTP responses:</p>
<ul>
<li><code>202 Accepted</code>: Request accepted (see below).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., missing parameters).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
<li><code>409 Conflict</code>: Insufficient resources to create the volumes.</li>
</ul>
<p>A single <code>/create-volumes</code> request can create multiple persistent volumes, but
all of the volumes must be on the same agent.</p>
<p>This endpoint returns the 202 ACCEPTED HTTP status code, which indicates that
the create operation has been validated successfully by the master. The request
is then forwarded asynchronously to the Mesos agent where the reserved
resources are located. That asynchronous message may not be delivered or
creating the volumes at the agent might fail, in which case no volumes will be
created. To determine if a create operation has succeeded, the user can examine
the state of the appropriate Mesos agent (e.g., via the agent's
<a href="endpoints/slave/state.html">/state</a> HTTP endpoint).</p>
<h3 id="destroy-volumes"><a class="header" href="#destroy-volumes"><code>/destroy-volumes</code></a></h3>
<p>To destroy the volume created above, we can send an HTTP POST to the master's
<a href="endpoints/master/destroy-volumes.html">/destroy-volumes</a> endpoint like so:</p>
<pre><code>curl -i \
     -u &lt;operator_principal&gt;:&lt;password&gt; \
     -d slaveId=&lt;slave_id&gt; \
     -d volumes='[
       {
         &quot;name&quot;: &quot;disk&quot;,
         &quot;type&quot;: &quot;SCALAR&quot;,
         &quot;scalar&quot;: { &quot;value&quot;: 512 },
         &quot;role&quot;: &quot;ads&quot;,
         &quot;reservation&quot;: {
           &quot;principal&quot;: &lt;operator_principal&gt;
         },
         &quot;disk&quot;: {
           &quot;persistence&quot;: {
             &quot;id&quot; : &lt;persistence_id&gt;
           },
           &quot;volume&quot;: {
             &quot;mode&quot;: &quot;RW&quot;,
             &quot;container_path&quot;: &lt;path&gt;
           }
         }
       }
     ]' \
     -X POST http://&lt;ip&gt;:&lt;port&gt;/master/destroy-volumes
</code></pre>
<p>Note that the <code>volume</code> JSON in the <code>/destroy-volumes</code> request must
<em>exactly</em> match the definition of the volume. The JSON definition of a
volume can be found via the <code>reserved_resources_full</code> key in the
master's <a href="endpoints/master/slaves.html">/slaves</a> endpoint (see below).</p>
<p>The user receives one of the following HTTP responses:</p>
<ul>
<li><code>202 Accepted</code>: Request accepted (see below).</li>
<li><code>400 BadRequest</code>: Invalid arguments (e.g., missing parameters).</li>
<li><code>401 Unauthorized</code>: Unauthenticated request.</li>
<li><code>403 Forbidden</code>: Unauthorized request.</li>
<li><code>409 Conflict</code>: Insufficient resources to destroy the volumes.</li>
</ul>
<p>A single <code>/destroy-volumes</code> request can destroy multiple persistent volumes, but
all of the volumes must be on the same agent.</p>
<p>This endpoint returns the 202 ACCEPTED HTTP status code, which indicates that
the destroy operation has been validated successfully by the master. The
request is then forwarded asynchronously to the Mesos agent where the
volumes are located. That asynchronous message may not be delivered or
destroying the volumes at the agent might fail, in which case no volumes will
be destroyed. To determine if a destroy operation has succeeded, the user can
examine the state of the appropriate Mesos agent (e.g., via the agent's
<a href="endpoints/slave/state.html">/state</a> HTTP endpoint).</p>
<h2 id="listing-persistent-volumes"><a class="header" href="#listing-persistent-volumes">Listing Persistent Volumes</a></h2>
<p>Information about the persistent volumes at each agent in the cluster can be
found by querying the <a href="endpoints/master/slaves.html">/slaves</a> master endpoint,
under the <code>reserved_resources_full</code> key.</p>
<p>The same information can also be found in the <a href="endpoints/slave/state.html">/state</a>
agent endpoint (under the <code>reserved_resources_full</code> key). The agent
endpoint is useful to confirm if changes to persistent volumes have been
propagated to the agent (which can fail in the event of network partition or
master/agent restarts).</p>
<h2 id="programming-with-persistent-volumes"><a class="header" href="#programming-with-persistent-volumes">Programming with Persistent Volumes</a></h2>
<p>Some suggestions to keep in mind when building applications that use persistent
volumes:</p>
<ul>
<li>
<p>A single <code>acceptOffers</code> call make a dynamic reservation (via
<code>Offer::Operation::Reserve</code>) and create a new persistent volume on the
newly reserved resources (via <code>Offer::Operation::Create</code>). However,
these operations are not executed atomically (i.e., either operation
or both operations could fail).</p>
</li>
<li>
<p>Volume IDs must be unique per role on each agent. However, it is strongly
recommended that frameworks use globally unique volume IDs, to avoid potential
confusion between volumes on different agents with the same volume
ID. Note also that the agent ID where a volume resides might change over
time. For example, suppose a volume is created on an agent and then the
agent's host machine is rebooted. When the agent registers with Mesos after
the reboot, it will be assigned a new AgentID---but it will retain the same
volume it had previously. Hence, frameworks should not assume that using the
pair &lt;AgentID, VolumeID&gt; is a stable way to identify a volume in a cluster.</p>
</li>
<li>
<p>Attempts to dynamically reserve resources or create persistent volumes might
fail---for example, because the network message containing the operation did
not reach the master or because the master rejected the operation.
Applications should be prepared to detect failures and correct for them (e.g.,
by retrying the operation).</p>
</li>
<li>
<p>When using HTTP endpoints to reserve resources or create persistent volumes,
<em>some</em> failures can be detected by examining the HTTP response code returned
to the client. However, it is still possible for a <code>202</code> response code to be
returned to the client but for the associated operation to fail---see
discussion above.</p>
</li>
<li>
<p>When using the scheduler API, detecting that a dynamic reservation has failed
is a little tricky: reservations do not have unique identifiers, and the Mesos
master does not provide explicit feedback on whether a reservation request has
succeeded or failed. Hence, framework schedulers typically use a combination
of two techniques:</p>
<ol>
<li>
<p>They use timeouts to detect that a reservation request may have failed
(because they don't receive a resource offer containing the expected
resources after a given period of time).</p>
</li>
<li>
<p>To check whether a resource offer includes the effect of a dynamic
reservation, applications <em>cannot</em> check for the presence of a &quot;reservation
ID&quot; or similar value (because reservations do not have IDs). Instead,
applications should examine the resource offer and check that it contains
sufficient reserved resources for the application's role. If it does not,
the application should make additional reservation requests as necessary.</p>
</li>
</ol>
</li>
<li>
<p>When a scheduler issues a dynamic reservation request, the reserved resources
might <em>not</em> be present in the next resource offer the scheduler receives.
There are two reasons for this: first, the reservation request might fail or
be dropped by the network, as discussed above. Second, the reservation request
might simply be delayed, so that the next resource offer from the master will
be issued before the reservation request is received by the master. This is
why the text above suggests that applications wait for a timeout before
assuming that a reservation request should be retried.</p>
</li>
<li>
<p>A consequence of using timeouts to detect failures is that an application
might submit more reservation requests than intended (e.g., a timeout fires
and an application makes another reservation request; meanwhile, the original
reservation request is also processed). Recall that two reservations for the
same role at the same agent are &quot;merged&quot;: for example, role <code>foo</code> makes two
requests to reserve 2 CPUs at a single agent and both reservation requests
succeed, the result will be a single reservation of 4 CPUs. To handle this
situation, applications should be prepared for resource offers that contain
more resources than expected. Some applications may also want to detect this
situation and unreserve any additional reserved resources that will not be
required.</p>
</li>
<li>
<p>It often makes sense to structure application logic as a &quot;state machine&quot;,
where the application moves from its initial state (no reserved resources and
no persistent volumes) and eventually transitions toward a single terminal
state (necessary resources reserved and persistent volume created). As new
events (such as timeouts and resource offers) are received, the application
compares the event with its current state and decides what action to take
next.</p>
</li>
<li>
<p>Because persistent volumes are associated with roles, a volume might be
offered to <em>any</em> of the frameworks that are subscribed to that role. For
example, a persistent volume might be created by one framework and then
offered to a different framework subscribed to the same role. This can be
used to pass large volumes of data between frameworks in a convenient way.
However, this behavior might also allow sensitive data created by one
framework to be read or modified by another framework subscribed to the
same role. It can also make it more difficult for frameworks to determine
whether a dynamic reservation has succeeded: as discussed above, frameworks
need to wait for an offer that contains the &quot;expected&quot; reserved resources
to determine when a reservation request has succeeded. Determining what a
framework should &quot;expect&quot; to find in an offer is more difficult when
multiple frameworks can make reservations for the same role concurrently.
In general, whenever multiple frameworks are allowed to subscribe to the
same role, the operator should ensure that those frameworks are configured
to collaborate with one another when using role-specific resources. For
more information, see the discussion of
<a href="roles.html#roles-multiple-frameworks">multiple frameworks in the same role</a>.</p>
</li>
</ul>
<h2 id="version-history"><a class="header" href="#version-history">Version History</a></h2>
<p>Persistent volumes were introduced in Mesos 0.23. Mesos 0.27 introduced HTTP
endpoints for creating and destroying volumes. Mesos 0.28 introduced support for
<a href="multiple-disk.html">multiple disk resources</a>, and also enhanced the <code>/slaves</code>
master endpoint to include detailed information about persistent volumes and
dynamic reservations. Mesos 1.0 changed the semantics of destroying a volume:
in previous releases, destroying a volume would remove the Mesos-level metadata
but would not remove the volume's data from the agent's filesystem. Mesos 1.1
introduced support for <a href="shared-resources.html">shared persistent volumes</a>. Mesos
1.6 introduced experimental support for resizing persistent volumes.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Container Storage Interface (CSI) Support
layout: documentation</h2>
<h1 id="container-storage-interface-csi-support"><a class="header" href="#container-storage-interface-csi-support">Container Storage Interface (CSI) Support</a></h1>
<p>This document describes the <a href="https://github.com/container-storage-interface/spec">Container Storage Interface</a>
(CSI) support in Mesos.</p>
<p>Currently, only CSI spec version 0.2 is supported in Mesos 1.6+ due to
incompatible changes between CSI version 0.1 and version 0.2. CSI version 0.1 is
supported in Mesos 1.5.</p>
<h2 id="motivation-5"><a class="header" href="#motivation-5">Motivation</a></h2>
<h3 id="current-limitations-of-storage-support-in-mesos"><a class="header" href="#current-limitations-of-storage-support-in-mesos">Current Limitations of Storage Support in Mesos</a></h3>
<p>Prior to 1.5, Mesos supports both <a href="persistent-volume.html">local persistent volumes</a>
as well as <a href="isolators/docker-volume.html">external persistent volumes</a>. However,
both of them have limitations.</p>
<p><a href="persistent-volume.html">Local persistent volumes</a> do not support offering
physical or logical block devices directly. Frameworks do not have the choice to
select filesystems for their local persistent volumes. Although Mesos does
support <a href="multiple-disk.html">multiple local disks</a>, it's a big burden for
operators to configure each agent properly to be able to leverage this feature.
Finally, there is no well-defined interface allowing third-party storage vendors
to plug into Mesos.</p>
<p><a href="isolators/docker-volume.html">External persistent volumes</a> support in Mesos
bypasses the resource management part. In other words, using an external
persistent volume does not go through the usual offer cycle. Mesos does not
track resources associated with the external volumes. This makes quota control,
reservation, and fair sharing almost impossible to enforce. Also, the current
interface Mesos uses to interact with storage vendors is the
<a href="https://docs.docker.com/engine/extend/plugins_volume/">Docker Volume Driver Interface</a>
(DVDI), which has several <a href="https://docs.google.com/document/d/125YWqg_5BB5OY9a6M7LZcby5RSqBwo2PZzpVLuxYXh4/edit?usp=sharing">limitations</a>.</p>
<h3 id="container-storage-interface-csi"><a class="header" href="#container-storage-interface-csi">Container Storage Interface (CSI)</a></h3>
<p><a href="https://github.com/container-storage-interface/spec">Container Storage Interface</a>
(CSI) is a specification that defines a common set of APIs for all interactions
between the storage vendors and the container orchestration platforms. It is the
result of a <a href="https://github.com/container-storage-interface/community">close collaboration</a>
among representatives from the <a href="https://kubernetes.io/">Kubernetes</a>,
<a href="https://www.cloudfoundry.org/">CloudFoundry</a>, <a href="https://www.docker.com/">Docker</a>
and Mesos communities. The primary goal of CSI is to allow storage vendors to
write one plugin that works with all container orchestration platforms.</p>
<p>It was an easy decision to build the storage support in Mesos using CSI. The
benefits are clear: it will fit Mesos into the larger storage ecosystem in a
consistent way. In other words, users will be able to use any storage system
with Mesos using a consistent API. The out-of-tree plugin model of CSI decouples
the release cycle of Mesos from that of the storage systems, making the
integration itself more sustainable and maintainable.</p>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<p>The following figure provides an overview about how Mesos supports CSI.</p>
<p><img src="images/csi-architecture.png" alt="CSI Architecture" /></p>
<h3 id="first-class-storage-resource-provider"><a class="header" href="#first-class-storage-resource-provider">First Class Storage Resource Provider</a></h3>
<p>The <a href="resource-provider.html">resource provider</a> abstraction is a natural fit for
supporting storage and CSI. Since CSI standardizes the interface between
container orchestrators and storage vendors, the implementation for the storage
resource provider should be the same for all storage systems that are
CSI-compatible.</p>
<p>As a result, Mesos provides a default implementation of LRP, called Storage
Local Resource Provider (SLRP), to provide general support for storage and CSI.
Storage External Resource Provider (SERP) support is <a href="https://issues.apache.org/jira/browse/MESOS-8371">coming soon</a>.
The storage resource providers serve as the bridges between Mesos and CSI plugins.</p>
<p>More details about SLRP can be found in the following <a href="csi.html#storage-local-resource-provider">section</a>.</p>
<h3 id="standalone-containers-for-csi-plugins"><a class="header" href="#standalone-containers-for-csi-plugins">Standalone Containers for CSI Plugins</a></h3>
<p>CSI plugins are long-running <a href="https://grpc.io/">gRPC</a> services, like daemons.
Those CSI plugins are packaged as containers, and are launched by SLRPs using
the <a href="standalone-containers.html">standalone containers</a> API from the agent.
Standalone containers can be launched without any tasks or executors. They use
the same isolation mechanism provided by the agent for task and executor
containers.</p>
<p>There is a component in each SLRP that is responsible for monitoring the health
of the CSI plugin containers and restarting them if needed.</p>
<h2 id="framework-api-3"><a class="header" href="#framework-api-3">Framework API</a></h2>
<h3 id="new-disk-source-types"><a class="header" href="#new-disk-source-types">New Disk Source Types</a></h3>
<p>Two new types of disk sources have been added: <code>RAW</code> and <code>BLOCK</code>.</p>
<pre><code class="language-protobuf">message Resource {
  message DiskInfo {
    message Source {
      enum Type {
        PATH = 1;
        MOUNT = 2;
        BLOCK = 3;  // New in 1.5
        RAW = 4;    // New in 1.5
      }
      optional Type type = 1;
    }
  }
}
</code></pre>
<p>The disk source type (i.e., <code>DiskInfo::Source::Type</code>) specifies the property of
a disk resource and how it can be consumed.</p>
<ul>
<li><code>PATH</code>: The disk resource can be accessed using the Volume API (backed by a
POSIX compliant filesystem). The disk resource can be carved up into smaller
chunks.</li>
<li><code>MOUNT</code>: The disk resource can be accessed using the Volume API (backed by a
POSIX compliant filesystem). The disk resource cannot be carved up into
smaller chunks.</li>
<li><code>BLOCK</code>: (New in 1.5) The disk resource can be directly accessed on Linux
without any filesystem (e.g., <code>/dev/sdb</code>). The disk resource cannot be carved
up into smaller chunks.</li>
<li><code>RAW</code>: (New in 1.5) The disk resource cannot be accessed by the framework yet.
It has to be <a href="csi.html#new-offer-operations-for-disk-resources">converted</a> into any of
the above types before it can be accessed. The disk resource cannot be carved
up into smaller chunks if it has an <a href="csi.html#disk-id-and-metadata">ID</a> (i.e.,
<a href="csi.html#pre-existing-disks">pre-existing disks</a>), and can be carved up into smaller
chunks if it does not have an <a href="csi.html#disk-id-and-metadata">ID</a> (i.e.,
<a href="csi.html#storage-pool">storage pool</a>).</li>
</ul>
<h3 id="disk-id-and-metadata"><a class="header" href="#disk-id-and-metadata">Disk ID and Metadata</a></h3>
<p>Two more fields have been added to <code>DiskInfo.Source</code> to further describe the
disk source. It also allows CSI plugins to propagate plugin-specific information
to the framework.</p>
<pre><code class="language-protobuf">message Resource {
  message DiskInfo {
    message Source {
      // An identifier for this source. This field maps onto CSI
      // volume IDs and is not expected to be set by frameworks.
      optional string id = 4;

      // Additional metadata for this source. This field maps onto CSI
      // volume metadata and is not expected to be set by frameworks.
      optional Labels metadata = 5;
    }
  }
}
</code></pre>
<ul>
<li><code>id</code>: This maps to CSI <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#createvolume">Volume ID</a>
if the disk resource is backed by a <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#terminology">Volume</a>
from a CSI plugin. This field must not be set by frameworks.</li>
<li><code>metadata</code>: This maps to CSI <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#createvolume">Volume Attributes</a>
if the disk resource is backed by a <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#terminology">Volume</a>
from a CSI plugin. This field must not be set by frameworks.</li>
</ul>
<h3 id="storage-pool"><a class="header" href="#storage-pool">Storage Pool</a></h3>
<p>A <code>RAW</code> disk resource may or may not have an ID (i.e., <code>DiskInfo.Source.id</code>),
depending on whether or not the <code>RAW</code> disk resource is backed by a CSI Volume. A
<code>RAW</code> disk resource not backed by a CSI Volume is usually referred to as a
storage pool (e.g., an LVM volume group, or EBS storage space, etc.).</p>
<p>The size of the storage pool is reported by the CSI plugin using the
<a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#getcapacity"><code>GetCapacity</code> interface</a>.</p>
<p>Currently, a storage pool must have a <a href="csi.html#profiles">profile</a> defined. Any disk
resource created from the storage pool inherits the same profile as the storage
pool. See more details in the <a href="csi.html#profiles">profiles</a> section.</p>
<h3 id="pre-existing-disks"><a class="header" href="#pre-existing-disks">Pre-existing Disks</a></h3>
<p>A <code>RAW</code> disk resource with an ID (i.e., <code>DiskInfo.Source.id</code>) is referred to as
a <a href="csi.html#pre-existing-disks">pre-existing disk</a>.  Pre-existing disks are those
<a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#terminology">CSI Volumes</a>
that are detected by the corresponding CSI plugin using the
<a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#listvolumes"><code>ListVolumes</code> interface</a>,
but have not gone through the dynamic provisioning process (i.e., via <code>CREATE_DISK</code>).</p>
<p>For example, operators might pre-create some LVM logical volumes before
launching Mesos. Those pre-created LVM logical volumes will be reported by the
LVM CSI plugin when Mesos invokes the <code>ListVolumes</code> interface, thus will be
reported as pre-existing disks in Mesos.</p>
<p>Currently, pre-existing disks do not have <a href="csi.html#profiles">profiles</a>. This may change
in the near future. See more details in the <a href="csi.html#profiles">profiles</a> section.</p>
<h3 id="new-offer-operations-for-disk-resources"><a class="header" href="#new-offer-operations-for-disk-resources">New Offer Operations for Disk Resources</a></h3>
<p>To allow dynamic provisioning of disk resources, two new offer operations have
been added to the <a href="scheduler-http-api.html#accept">scheduler API</a>:
<code>CREATE_DISK</code> and <code>DESTROY_DISK</code>.</p>
<p>To learn how to use the offer operations, please refer to the
<a href="scheduler-http-api.html#accept"><code>ACCEPT</code></a> Call in the v1 scheduler API, or
<a href="app-framework-development-guide.html#api"><code>acceptOffers</code></a> method in the v0
scheduler API for more details.</p>
<pre><code class="language-protobuf">message Offer {
  message Operation {
    enum Type {
      UNKNOWN = 0;
      LAUNCH = 1;
      LAUNCH_GROUP = 6;
      RESERVE = 2;
      UNRESERVE = 3;
      CREATE = 4;
      DESTROY = 5;
      GROW_VOLUME = 11;
      SHRINK_VOLUME = 12;
      CREATE_DISK = 13;   // New in 1.7.
      DESTROY_DISK = 14;  // New in 1.7.
    }
    optional Type type = 1;
  }
}
</code></pre>
<h4 id="create_disk-operation"><a class="header" href="#create_disk-operation"><code>CREATE_DISK</code> operation</a></h4>
<p>The offer operation <code>CREATE_DISK</code> takes a <code>RAW</code> disk resource
(<code>create_disk.source</code>), and create a <code>MOUNT</code> or a <code>BLOCK</code> disk resource
(<code>create_disk.target_type</code>) from the source. The source <code>RAW</code> disk resource can
either be a storage pool (i.e., a <code>RAW</code> disk resource without an ID) or a
pre-existing disk (i.e., a <code>RAW</code> disk resource with an ID). The quantity of the
converted resource (either <code>MOUNT</code> or <code>BLOCK</code> disk resource) will be the same as
the source <code>RAW</code> resource.</p>
<pre><code class="language-protobuf">message Offer {
  message Operation {
    message CreateDisk {
      required Resource source = 1;
      required Resource.DiskInfo.Source.Type target_type = 2;
    }
    optional CreateDisk create_disk = 15;
  }
}
</code></pre>
<p>The created disk resource will have the disk <a href="csi.html#disk-id-and-metadata"><code>id</code> and <code>metadata</code></a>
set accordingly to uniquely identify the volume reported by the CSI plugin.</p>
<p>Note that <code>CREATE_DISK</code> is different than <a href="persistent-volume.html"><code>CREATE</code></a>.
<code>CREATE</code> creates a <a href="persistent-volume.html">persistent volume</a> which indicates
that the data stored in the volume will be persisted until the framework
explicitly destroys it. It must operate on a non-<code>RAW</code> disk resource (i.e.,
<code>PATH</code>, <code>MOUNT</code> or <code>BLOCK</code>).</p>
<h4 id="destroy_disk-operation"><a class="header" href="#destroy_disk-operation"><code>DESTROY_DISK</code> operation</a></h4>
<p>The offer operation <code>DESTROY_DISK</code> destroys a <code>MOUNT</code> or a <code>BLOCK</code> disk resource
(<code>destroy_disk.source</code>), which will result in a <code>RAW</code> disk resource. The
quantity of the <code>RAW</code> disk resource will be the same as the specified <code>source</code>,
unless it has an invalid profile (described later), in which case the
<code>DESTROY_DISK</code> operation will completely remove the disk resource.</p>
<pre><code class="language-protobuf">message Offer {
  message Operation {
    message DestroyDisk {
      required Resource source = 1;
    }
    optional DestroyDisk destroy_disk = 16;
  }
}
</code></pre>
<p>This operation is intended to be a reverse operation of <code>CREATE_DISK</code>. In
other words, if the volume is created from a storage pool (i.e., a <code>RAW</code> disk
resource without an ID), the result of the corresponding <code>DESTROY_DISK</code> should
be a storage pool. And if the volume is created from a <a href="csi.html#pre-existing-disks">pre-existing disk</a>
(i.e., a <code>RAW</code> disk resource with an ID), the result of the corresponding
<code>DESTROY_DISK</code> should be a pre-existing disk.</p>
<p>Currently, Mesos infers the result based on the presence of an assigned
<a href="csi.html#profiles">profile</a> in the disk resource. In other words, if the volume to be
destroyed has a profile, the converted <code>RAW</code> disk resource will be a storage
pool (i.e., <code>RAW</code> disk resource without an ID). Otherwise, the converted <code>RAW</code>
disk resource will be a pre-existing disk (i.e., <code>RAW</code> disk resource with an
ID). This leverages the fact that currently, each storage pool must have a
profile, and pre-existing disks do not have profiles.</p>
<h4 id="getting-operation-results"><a class="header" href="#getting-operation-results">Getting Operation Results</a></h4>
<p>It is important for the frameworks to get the results of the above offer
operations so that they know if the dynamic disk provisioning is successful or
not.</p>
<p>Starting with Mesos 1.6.0 it is possible to opt-in to receive status updates
related to operations that affect resources managed by a resource provider. In
order to do so, the framework has to set the <code>id</code> field in the operation.
Support for operations affecting the agent default resources is <a href="https://issues.apache.org/jira/browse/MESOS-8194">coming
soon</a>.</p>
<h2 id="profiles"><a class="header" href="#profiles">Profiles</a></h2>
<p>The primary goal of introducing profiles is to provide an indirection to a set
of storage vendor-specific parameters for the disk resources. It provides a way
for the cluster operator to describe the classes of storage they offer and
abstracts away the low-level details of a storage system.</p>
<p>Each profile is just a simple string (e.g., &quot;fast&quot;, &quot;slow&quot;, &quot;gold&quot;), as
described below:</p>
<pre><code class="language-protobuf">message Resource {
  message DiskInfo {
    message Source {
      // This field serves as an indirection to a set of storage
      // vendor specific disk parameters which describe the properties
      // of the disk. The operator will setup mappings between a
      // profile name to a set of vendor specific disk parameters. And
      // the framework will do disk selection based on profile names,
      // instead of vendor specific disk parameters.
      //
      // Also see the DiskProfile module.
      optional string profile = 6;
    }
  }
}
</code></pre>
<p>A typical framework that needs storage is expected to perform disk
resource selection based on the <code>profile</code> of a disk resource, rather
than low-level storage vendor specific parameters.</p>
<h3 id="disk-profile-adaptor-module"><a class="header" href="#disk-profile-adaptor-module">Disk Profile Adaptor Module</a></h3>
<p>In order to let cluster operators customize the mapping between profiles and
storage system-specific parameters, Mesos provides a <a href="modules.html">module</a>
interface called <code>DiskProfileAdaptor</code>.</p>
<pre><code class="language-cpp">class DiskProfileAdaptor
{
public:
  struct ProfileInfo
  {
    csi::VolumeCapability capability;
    google::protobuf::Map&lt;std::string, std::string&gt; parameters;
  };

  virtual Future&lt;ProfileInfo&gt; translate(
      const std::string&amp; profile,
      const ResourceProviderInfo&amp; resourceProviderInfo) = 0;

  virtual Future&lt;hashset&lt;std::string&gt;&gt; watch(
      const hashset&lt;std::string&gt;&amp; knownProfiles,
      const ResourceProviderInfo&amp; resourceProviderInfo) = 0;
};
</code></pre>
<p>The module interface has a <code>translate</code> method that takes a profile and returns
the corresponding <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#createvolume">CSI volume capability</a>
(i.e., the <code>capability</code> field) and <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#createvolume">CSI volume creation parameters</a>
(i.e., the <code>parameters</code> field) for that profile. These two fields will be used to
call the CSI <code>CreateVolume</code> interface during dynamic provisioning (i.e.,
<code>CREATE_DISK</code>), or CSI <code>ControllerPublishVolume</code> and
<code>NodePublishVolume</code> when publishing (i.e., when a task using the disk resources
is being launched on a Mesos agent).</p>
<p>The <code>watch</code> method in the module interface allows Mesos to get notified about
the changes on the profiles. It takes a list of known profiles and returns a
future which will be set if the module detects changes to the known profiles
(e.g., a new profile is added).  Currently, all profiles are immutable, thus are
safe to cache.</p>
<p>Since <code>ProfileInfo</code> uses protobuf from the CSI spec directly, there is an
implicit dependency between backward compatibility of the module interface and
the CSI spec version. Since CSI doesn't provide a backward compatibility
promise, modules have to be re-built against each release of Mesos.</p>
<h3 id="uri-disk-profile-adaptor"><a class="header" href="#uri-disk-profile-adaptor">URI Disk Profile Adaptor</a></h3>
<p>To demonstrate how to use the disk profile adaptor module, Mesos ships with a
default disk profile adaptor, called <code>UriDiskProfileAdaptor</code>. This module
polls the profile information (in JSON) from a configurable URI. Here are the
module parameters that can be used to configure the module:</p>
<ul>
<li><code>uri</code>: URI to a JSON object containing the profile mapping. The module
supports both HTTP(s) and file URIs. The JSON object should consist of some
top-level string keys corresponding to the disk profile name. Each value
should contain a <code>ResourceProviderSelector</code> under <code>resource_provider_selector</code>
or a <code>CSIPluginTypeSelector</code> under <code>csi_plugin_type_selector</code> to specify the
set of resource providers this profile applies to, followed by a
<code>VolumeCapability</code> under <code>volume_capabilities</code> and arbitrary key-value pairs
under <code>create_parameters</code>. For example:</li>
</ul>
<pre><code class="language-json">{
  &quot;profile_matrix&quot;: {
    &quot;my-profile&quot;: {
      &quot;csi_plugin_type_selector&quot;: {
        &quot;plugin_type&quot;: &quot;org.apache.mesos.csi.test&quot;
      },
      &quot;volume_capabilities&quot;: {
        &quot;mount&quot;: {
          &quot;fs_type&quot;: &quot;xfs&quot;
        },
        &quot;access_mode&quot;: {
          &quot;mode&quot;: &quot;SINGLE_NODE_WRITER&quot;
        }
      },
      &quot;create_parameters&quot;: {
        &quot;type&quot;: &quot;raid5&quot;,
        &quot;stripes&quot;: &quot;3&quot;,
        &quot;stripesize&quot;: &quot;64&quot;
      }
    }
  }
}
</code></pre>
<ul>
<li><code>poll_interval</code>: How long to wait between polling the specified <code>uri</code>.  If the
poll interval has elapsed since the last fetch, then the URI is re-fetched;
otherwise, a cached <code>ProfileInfo</code> is returned. If not specified, the URI is
only fetched once.</li>
<li><code>max_random_wait</code>: How long at most to wait between discovering a new set of
profiles and notifying the callers of <code>watch</code>. The actual wait time is a
uniform random value between 0 and this value. If the <code>--uri</code> points to a
centralized location, it may be good to scale this number according to the
number of resource providers in the cluster.  [default: 0secs]</li>
</ul>
<p>To enable this module, please follow the <a href="modules.html">modules documentation</a>:
add the following JSON to the <code>--modules</code> agent flag, and set agent flag
<code>--disk_profile_adaptor</code> to <code>org_apache_mesos_UriDiskProfileAdaptor</code>.</p>
<pre><code class="language-json">{
  &quot;libraries&quot;: [
    {
      &quot;file&quot;: &quot;/PATH/TO/liburi_disk_profile.so&quot;,
      &quot;modules&quot;: [
        {
          &quot;name&quot;: &quot;org_apache_mesos_UriDiskProfileAdaptor&quot;,
          &quot;parameters&quot;: [
            {
              &quot;key&quot;: &quot;uri&quot;,
              &quot;value&quot;: &quot;/PATH/TO/my_profile.json&quot;
            },
            {
              &quot;key&quot;: &quot;poll_interval&quot;,
              &quot;value&quot;: &quot;1secs&quot;
            }
          ]
        }
      ]
    }
  ]
}
</code></pre>
<h3 id="storage-pool-capacity-and-profiles"><a class="header" href="#storage-pool-capacity-and-profiles">Storage Pool Capacity and Profiles</a></h3>
<p>The capacity of a <a href="csi.html#storage-pool">storage pool</a> is usually tied to the profiles
of the volumes that the users want to provision from the pool. For instance,
consider an LVM volume group (a storage pool) backed by 1000G of physical
volumes. The capacity of the storage pool will be 1000G if the logical volumes
provisioned from the pool have <code>&quot;raid0&quot;</code> configuration, and will be 500G if the
logical volumes provisioned from the pool have <code>&quot;raid1&quot;</code> configuration.</p>
<p>In fact, it does not make sense to have a storage pool that does not have a
profile because otherwise the allocator or the framework will not be able to
predict how much space a volume will take, making resource management almost
impossible to implement.</p>
<p>Therefore, each storage pool must have a profile associated with it. The profile
of a storage pool is the profile of the volumes that can be provisioned from the
pool. In other words, the volumes provisioned from a storage pool inherit the
profile of the storage pool.</p>
<p>Mesos gets the capacity of a storage pool with a given profile by invoking the
CSI <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#getcapacity"><code>GetCapacity</code> interface</a>
with the corresponding volume capability and parameters associated with the
profile.</p>
<p>It is possible that a storage system is able to provide volumes with different
profiles. For example, the LVM volume group is able to produce both raid0 and
raid1 logical volumes, backed by the same physical volumes. In that case, Mesos
will report one storage pool per profile. In this example, assuming there are
two profiles: <code>&quot;raid0&quot;</code> and <code>&quot;raid1&quot;</code>, Mesos will report 2 <code>RAW</code> disk resources:</p>
<ol>
<li>1000G <code>RAW</code> disk resource with profile <code>&quot;raid0&quot;</code></li>
<li>500G <code>RAW</code> disk resource with profile <code>&quot;raid1&quot;</code>.</li>
</ol>
<p>TODO(jieyu): Discuss correlated resources.</p>
<h2 id="storage-local-resource-provider"><a class="header" href="#storage-local-resource-provider">Storage Local Resource Provider</a></h2>
<p><a href="resource-provider.html">Resource Provider</a> is an abstraction in Mesos allowing
cluster administrators to customize the providing of resources and the handling
of operations related to the provided resources.</p>
<p>For storage and CSI support, Mesos provides a default implementation of the
resource provider interface that serves as the bridge between Mesos and the CSI
plugins. It is called the Storage Resource Provider. It is responsible for
launching CSI plugins, talking to CSI plugins using the gRPC protocol, reporting
available disk resources, handling offer operations from frameworks, and making
disk resources available on the agent where the disk resources are used.</p>
<p>Currently, each Storage Resource Provider instance manages exactly one CSI
plugin. This simplifies reasoning and implementation.</p>
<p>In Mesos 1.5, only the Storage Local Resource Provider (SLRP) is supported. This
means the disk resources it reports are tied to a particular agent node, and
thus cannot be used on other nodes. The Storage External Resource Provider
(SERP) is <a href="https://issues.apache.org/jira/browse/MESOS-8371">coming soon</a>.</p>
<h3 id="enable-grpc-support"><a class="header" href="#enable-grpc-support">Enable gRPC Support</a></h3>
<p><a href="https://grpc.io/">gRPC</a> must be enabled to support SLRP. To enable gRPC
support, configure Mesos with <code>--enable-grpc</code>.</p>
<h3 id="enable-agent-resource-provider-capability"><a class="header" href="#enable-agent-resource-provider-capability">Enable Agent Resource Provider Capability</a></h3>
<p>In order to use SLRPs, the agent needs to be configured to enable resource
provider support. Since resource provider support is an experimental feature, it
is not turned on by default in 1.5. To enable that, please set the agent flag
<code>--agent_features</code> to the following JSON:</p>
<pre><code class="language-json">{
  &quot;capabilities&quot;: [
    {&quot;type&quot;: &quot;MULTI_ROLE&quot;},
    {&quot;type&quot;: &quot;HIERARCHICAL_ROLE&quot;},
    {&quot;type&quot;: &quot;RESERVATION_REFINEMENT&quot;},
    {&quot;type&quot;: &quot;RESOURCE_PROVIDER&quot;}
  ]
}
</code></pre>
<p>Note that although capabilities <code>MULTI_ROLE</code>, <code>HIERARCHICAL_ROLE</code> and
<code>RESERVATION_REFINEMENT</code> are not strictly necessary for supporting resources
providers, these must be specified because the agent code already assumes those
capabilities are set, and the old code that assumes those capabilities not being
set has already been removed.</p>
<h3 id="slrp-configuration"><a class="header" href="#slrp-configuration">SLRP Configuration</a></h3>
<p>Each SLRP configures itself according to its <code>ResourceProviderInfo</code> which is
specified by the operator.</p>
<pre><code class="language-protobuf">message ResourceProviderInfo {
  required string type = 3;
  required string name = 4;
  repeated Resource.ReservationInfo default_reservations = 5;

  // Storage resource provider related information.
  message Storage {
    required CSIPluginInfo plugin = 1;
  }

  optional Storage storage = 6;
}
</code></pre>
<ul>
<li><code>type</code>: The type of the resource provider. This uniquely identifies a resource
provider implementation. For instance: <code>&quot;org.apache.mesos.rp.local.storage&quot;</code>.
The naming of the <code>type</code> field should follow the
<a href="https://en.wikipedia.org/wiki/Java_package#Package_naming_conventions">Java package naming convention</a>
to avoid conflicts on the type names.</li>
<li><code>name</code>: The name of the resource provider. There could be multiple instances
of a type of resource provider. The name field is used to distinguish these
instances. It should be a legal <a href="https://docs.oracle.com/javase/tutorial/java/nutsandbolts/variables.html">Java identifier</a>
to avoid conflicts on concatenation of type and name.</li>
<li><code>default_reservations</code>: If set, any new resources from this resource provider
will be reserved by default. The first <code>ReservationInfo</code> may have type
<code>STATIC</code> or <code>DYNAMIC</code>, but the rest must have <code>DYNAMIC</code>. One can create a new
reservation on top of an existing one by pushing a new <code>ReservationInfo</code> to
the back. The last <code>ReservationInfo</code> in this stack is the &quot;current&quot;
reservation. The new reservation's role must be a child of the current one.</li>
<li><code>storage</code>: Storage resource provider specific information (see more details
below).</li>
</ul>
<pre><code class="language-protobuf">message CSIPluginInfo {
  required string type = 1;
  required string name = 2;
  repeated CSIPluginContainerInfo containers = 3;
}
</code></pre>
<ul>
<li><code>type</code>: The type of the CSI plugin. This uniquely identifies a CSI plugin
implementation. For instance: <code>&quot;org.apache.mesos.csi.test&quot;</code>.  The naming
should follow the <a href="https://en.wikipedia.org/wiki/Java_package#Package_naming_conventions">Java package naming convention</a>
to avoid conflicts on type names.</li>
<li><code>name</code>: The name of the CSI plugin. There could be multiple instances of the
same type of CSI plugin. The name field is used to distinguish these
instances. It should be a legal <a href="https://docs.oracle.com/javase/tutorial/java/nutsandbolts/variables.html">Java identifier</a>
to avoid conflicts on concatenation of type and name.</li>
<li><code>containers</code>: CSI plugin container configurations (see more details below).
The <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#controller-service-rpc">CSI controller service</a>
will be served by the first that contains <code>CONTROLLER_SERVICE</code>, and the
<a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#node-service-rpc">CSI node service</a>
will be served by the first that contains <code>NODE_SERVICE</code>.</li>
</ul>
<pre><code class="language-protobuf">message CSIPluginContainerInfo {
  enum Service {
    UNKNOWN = 0;
    CONTROLLER_SERVICE = 1;
    NODE_SERVICE = 2;
  }

  repeated Service services = 1;
  optional CommandInfo command = 2;
  repeated Resource resources = 3;
  optional ContainerInfo container = 4;
}
</code></pre>
<ul>
<li><code>services</code>: Whether the CSI plugin container provides the
<a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#controller-service-rpc">CSI controller service</a>,
the <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#node-service-rpc">CSI node service</a>
or both.</li>
<li><code>command</code>: The command to launch the CSI plugin container.</li>
<li><code>resources</code>: The resources to be used for the CSI plugin container.</li>
<li><code>container</code>: The additional <code>ContainerInfo</code> about the CSI plugin container.</li>
</ul>
<p>Note that each CSI plugin will have all isolation mechanisms configured on the
agent applied to it.</p>
<h4 id="sample-slrp-configuration"><a class="header" href="#sample-slrp-configuration">Sample SLRP Configuration</a></h4>
<p>The following is a sample SLRP configuration that uses the <a href="https://github.com/apache/mesos/blob/1.5.x/src/examples/test_csi_plugin.cpp">test CSI plugin</a>
provided by Mesos that provides both CSI controller and node services, and sets
the default reservation to <code>&quot;test-role&quot;</code>. The test CSI plugin will be built if
you configure Mesos with <code>--enable-tests-install</code>.</p>
<pre><code class="language-json">{
  &quot;type&quot;: &quot;org.apache.mesos.rp.local.storage&quot;,
  &quot;name&quot;: &quot;test_slrp&quot;,
  &quot;default_reservations&quot;: [
    {
      &quot;type&quot;: &quot;DYNAMIC&quot;,
      &quot;role&quot;: &quot;test-role&quot;
    }
  ],
  &quot;storage&quot;: {
    &quot;plugin&quot;: {
      &quot;type&quot;: &quot;org.apache.mesos.csi.test&quot;,
      &quot;name&quot;: &quot;test_plugin&quot;,
      &quot;containers&quot;: [
        {
          &quot;services&quot;: [ &quot;CONTROLLER_SERVICE&quot;, &quot;NODE_SERVICE&quot; ],
          &quot;command&quot;: {
            &quot;shell&quot;: true,
            &quot;value&quot;: &quot;./test-csi-plugin --available_capacity=2GB --work_dir=workdir&quot;,
            &quot;uris&quot;: [
              {
                &quot;value&quot;: &quot;/PATH/TO/test-csi-plugin&quot;,
                &quot;executable&quot;: true
              }
            ]
          },
          &quot;resources&quot;: [
            { &quot;name&quot;: &quot;cpus&quot;, &quot;type&quot;: &quot;SCALAR&quot;, &quot;scalar&quot;: { &quot;value&quot;: 0.1 } },
            { &quot;name&quot;: &quot;mem&quot;, &quot;type&quot;: &quot;SCALAR&quot;, &quot;scalar&quot;: { &quot;value&quot;: 200.0 } }
          ]
        }
      ]
    }
  }
}
</code></pre>
<h3 id="slrp-management"><a class="header" href="#slrp-management">SLRP Management</a></h3>
<h4 id="launching-slrp"><a class="header" href="#launching-slrp">Launching SLRP</a></h4>
<p>To launch a SLRP, place the SLRP configuration JSON described in the
<a href="csi.html#slrp-configuration">previous section</a> in a directory (e.g.,
<code>/etc/mesos/resource-providers</code>) and set the agent flag
<code>--resource_provider_config_dir</code> to point to that directory. The corresponding
SLRP will be loaded by the agent. It is possible to put multiple SLRP
configuration JSON files under that directory to instruct the agent to load
multiple SLRPs.</p>
<p>Alternatively, it is also possible to dynamically launch a SLRP using the <a href="operator-http-api.html#agent-api">agent
v1 operator API</a>. To use that, still set the
agent flag <code>--resource_provider_config_dir</code> to point to a configuration
directory (the directory maybe empty). Once the agent is launched, hit the agent
<code>/api/v1</code> endpoint using the <a href="operator-http-api.html#add_resource_provider_config"><code>ADD_RESOURCE_PROVIDER_CONFIG</code></a>
call:</p>
<p>For example, here is the <code>curl</code> command to launch a SLRP:</p>
<pre><code class="language-shell">curl -X POST -H 'Content-Type: application/json' -d '{&quot;type&quot;:&quot;ADD_RESOURCE_PROVIDER_CONFIG&quot;,&quot;add_resource_provider_config&quot;:{&quot;info&quot;:&lt;SLRP_JSON_CONFIG&gt;}}' http://&lt;agent_ip&gt;:&lt;agent_port&gt;/api/v1
</code></pre>
<h4 id="updating-slrp"><a class="header" href="#updating-slrp">Updating SLRP</a></h4>
<p>A SLRP can be updated by modifying the JSON configuration file. Once the
modification is done, restart the agent to pick up the new configuration.</p>
<p>Alternatively, the operator can dynamically update a SLRP using the <a href="operator-http-api.html#agent-api">agent v1
operator API</a>. When the agent is running, hit
the agent <code>/api/v1</code> endpoint using the
<a href="operator-http-api.html#update_resource_provider_config"><code>UPDATE_RESOURCE_PROVIDER_CONFIG</code></a>
call:</p>
<p>For example, here is the <code>curl</code> command to update a SLRP:</p>
<pre><code class="language-shell">curl -X POST -H 'Content-Type: application/json' -d '{&quot;type&quot;:&quot;UPDATE_RESOURCE_PROVIDER_CONFIG&quot;,&quot;update_resource_provider_config&quot;:{&quot;info&quot;:&lt;NEW_SLRP_JSON_CONFIG&gt;}}' http://&lt;agent_ip&gt;:&lt;agent_port&gt;/api/v1
</code></pre>
<p><em>NOTE</em>: Currently, only <code>storage.containers</code> in the <code>ResourceProviderInfo</code> can
be updated. This allows operators to update the CSI plugin (e.g., upgrading)
without affecting running tasks and executors.</p>
<h4 id="removing-slrp"><a class="header" href="#removing-slrp">Removing SLRP</a></h4>
<p>Removing a SLRP means that the agent will terminate the existing SLRP if it is
still running, and will no longer launch the SLRP during startup. The master and
the agent will think the SLRP has disconnected, similar to agent disconnection.
If there exists a task that is using the disk resources provided by the SLRP,
its execution will not be affected. However, offer operations (e.g.,
<code>CREATE_DISK</code>) for the SLRP will not be successful. In fact, if a SLRP is
disconnected, the master will rescind the offers related to that SLRP,
effectively disallowing frameworks to perform operations on the disconnected
SLRP.</p>
<p>The SLRP can be re-added after its removal following the same instructions of
<a href="csi.html#launching-slrp">launching a SLRP</a>. Note that removing a SLRP is different than
marking a SLRP as gone, in which case the SLRP will not be allowed to be
re-added. Marking a SLRP as gone is not yet supported.</p>
<p>A SLRP can be removed by removing the JSON configuration file from the
configuration directory (<code>--resource_provider_config_dir</code>). Once the removal is
done, restart the agent to pick up the removal.</p>
<p>Alternatively, the operator can dynamically remove a SLRP using the
<a href="operator-http-api.html#agent-api">agent v1 operator API</a>. When the agent is
running, hit the agent <code>/api/v1</code> endpoint using the
<a href="operator-http-api.html#remove_resource_provider_config"><code>REMOVE_RESOURCE_PROVIDER_CONFIG</code></a>
call:</p>
<p>For example, here is the <code>curl</code> command to update a SLRP:</p>
<pre><code class="language-shell">curl -X POST -H 'Content-Type: application/json' -d '{&quot;type&quot;:&quot;REMOVE_RESOURCE_PROVIDER_CONFIG&quot;,&quot;remove_resource_provider_config&quot;:{&quot;type&quot;:&quot;org.apache.mesos.rp.local.storage&quot;,&quot;name&quot;:&lt;SLRP_NAME&gt;}}' http://&lt;agent_ip&gt;:&lt;agent_port&gt;/api/v1
</code></pre>
<h4 id="authorization-2"><a class="header" href="#authorization-2">Authorization</a></h4>
<p>A new authorization action <code>MODIFY_RESOURCE_PROVIDER_CONFIG</code> has been added.
This action applies to adding/updating/removing a SLRP.</p>
<p>For the default Mesos local authorizer, a new ACL
<code>ACL.ModifyResourceProviderConfig</code> has been added, allowing operators limit the
access to the above API endpoints.</p>
<pre><code class="language-protobuf">message ACL {
  // Which principals are authorized to add, update and remove resource
  // provider config files.
  message ModifyResourceProviderConfig {
    // Subjects: HTTP Username.
    required Entity principals = 1;

    // Objects: Given implicitly.
    // Use Entity type ANY or NONE to allow or deny access.
    required Entity resource_providers = 2;
  }
}
</code></pre>
<p>Currently, the <code>Objects</code> has to be either <code>ANY</code> or <code>NONE</code>. Fine-grained
authorization of specific resource provider objects is not yet supported. Please
refer to the <a href="authorization.html">authorization doc</a> for more details about the
default Mesos local authorizer.</p>
<h3 id="standalone-containers-for-csi-plugins-1"><a class="header" href="#standalone-containers-for-csi-plugins-1">Standalone Containers for CSI Plugins</a></h3>
<p>As already mentioned earlier, each SLRP instance manages exactly one CSI plugin.
Each CSI plugin consists of one or more containers containing run processes that
implement both the <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#controller-service-rpc">CSI controller service</a>
and the <a href="https://github.com/container-storage-interface/spec/blob/v0.1.0/spec.md#node-service-rpc">CSI node service</a>.</p>
<p>The CSI plugin containers are managed by the SLRP automatically. The operator
does not need to deploy them manually. The SLRP will make sure that the CSI
plugin containers are running and restart them if needed (e.g., failed).</p>
<p>The CSI plugin containers are launched using the standalone container API
provided by the Mesos agent. See more details about standalone container in the
<a href="standalone-container.html">standalone container doc</a>.</p>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<ul>
<li>Only local disk resources are supported currently. That means the disk
resources are tied to a particular agent node and cannot be used on a
different agent node. The external disk resources support is coming soon.</li>
<li>The CSI plugin container cannot be a Docker container yet. Storage vendors
currently should package the CSI plugins in binary format and use the
<a href="fetcher.html">fetcher</a> to fetch the binary executable.</li>
<li><code>BLOCK</code> type disk resources are not supported yet.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Running Workloads in Mesos
layout: documentation</h2>
<h1 id="workloads-in-mesos"><a class="header" href="#workloads-in-mesos">Workloads in Mesos</a></h1>
<p>The goal of most Mesos schedulers is to launch workloads on Mesos agents. Once
a scheduler has subscribed with the Mesos master using the
<a href="scheduler-http-api.html#subscribe"><code>SUBSCRIBE</code> call</a>, it will begin to receive
offers. To launch a workload, the scheduler can submit an
<a href="scheduler-http-api.html#accept"><code>ACCEPT</code> call</a> to the master, including the offer
ID of an offer that it previously received which contains resources it can use
to run the workload.</p>
<p>The basic unit of work in a Mesos cluster is the &quot;task&quot;. A single command or
container image and accompanying artifacts can be packaged into a task which is
sent to a Mesos agent for execution. To launch a task, a scheduler can place it
into a task group and pass it to the Mesos master inside a <code>LAUNCH_GROUP</code>
operation. <code>LAUNCH_GROUP</code> is one of the offer operations that can be specified
in the <code>ACCEPT</code> call.</p>
<p>An older call in the same API, the <code>LAUNCH</code> call, allows schedulers to launch
single tasks as well; this legacy method of launching tasks will be covered at
the end of this document.</p>
<h1 id="task-groups-1"><a class="header" href="#task-groups-1">Task Groups</a></h1>
<p>Task groups, or &quot;pods&quot;, allow a scheduler to group one or more tasks into a
single workload. When one task is specified alongside an executor that has a
unique executor ID, the task group is simply executed as a single isolated OS
process; this is the simple case of a single task.</p>
<p>When multiple tasks are specified for a single task group, all of the tasks will
be launched together on the same agent, and their lifecycles are coupled such
that if a single task fails, they are all killed. On Linux, the tasks will also
share network and mount namespaces by default so that they can communicate over
the network and access the same volumes (note that custom container networks may
be used as well). The resource constraints specified may be enforced for the
tasks collectively or individually depending on other settings; for more
information, see below, as well as the documentation on
<a href="nested-container-and-task-group.html">nested containers and task groups</a>.</p>
<h2 id="the-executor"><a class="header" href="#the-executor">The Executor</a></h2>
<p>The Mesos &quot;executor&quot; is responsible for managing the tasks. The executor must be
specified in the <code>LAUNCH_GROUP</code> operation, including an executor ID, the
framework ID, and some resources for the executor to perform its work. The
minimum resources required for an executor are shown in the example below.</p>
<h2 id="the-workload"><a class="header" href="#the-workload">The Workload</a></h2>
<p>You can specify your workload using a shell command, one or more artifacts to
be fetched before task launch, a container image, or some combination of these.
The example below shows a simple shell command and a URI pointing to a tarball
which presumably contains the script invoked in the command.</p>
<h2 id="resource-requests-and-limits-1"><a class="header" href="#resource-requests-and-limits-1">Resource Requests and Limits</a></h2>
<p>In each task, the resources required by that task can be specified. Common
resource types are <code>cpus</code>, <code>mem</code>, and <code>disk</code>. The resources listed in the
<code>resources</code> field are known as resource &quot;requests&quot; and represent the minimum
resource guarantee required by the task; these resources will always be
available to the task if they are needed. The quantities specified in the
<code>limits</code> field are the resource &quot;limits&quot;, which represent the maximum amount of
<code>cpus</code> and/or <code>mem</code> that the task may use. Setting a CPU or memory limit higher
than the corresponding request allows the task to consume more than its
allocated amount of CPU or memory when there are unused resources available on
the agent. For important Linux-specific settings related to resource limits, see
the section below on Linux resource isolation.</p>
<p>In addition to finite numeric values, the resource limits may be set to
infinity, indicating that the task will be permitted to consume any available
CPU and/or memory on the agent. This is represented in the JSON example below
using the string &quot;Infinity&quot;, though when submitting scheduler calls in protobuf
format the standard IEEE-defined floating point infinity value may be used.</p>
<p>When a task consumes extra available memory on an agent but then other task
processes on the machine which were guaranteed access to that memory suddenly
need it, it's possible that processes will have to be killed in order to
reclaim memory. When a task has a memory limit higher than its memory request,
the task process's OOM score adjustment is set so that it is OOM-killed
preferentially if it exceeds its memory request in such cases.</p>
<h2 id="linux-resource-isolation"><a class="header" href="#linux-resource-isolation">Linux Resource Isolation</a></h2>
<p>When workloads are executed on Linux agents, resource isolation is likely
provided by the Mesos agent's manipulation of cgroup subsystems. In the simple
case of an executor running a single task group with a single task (like the
example below), enforcement of resource requests and limits is straightforward,
since there is only one task process to isolate.</p>
<p>When multiple tasks or task groups run under a single executor, the enforcement
of resource constraints is more complex. Some control over this is allowed by
the <code>container.linux_info.share_cgroups</code> field in each task. When this boolean
field is <code>true</code> (this is the default), each task is constrained by the cgroups
of its executor. This means that if multiple tasks run underneath one executor,
their resource constraints will be enforced as a sum of all the task resource
constraints, applied collectively to those task processes. In this case, task
resource consumption is collectively managed via one set of cgroup subsystem
control files associated with the executor.</p>
<p>When the <code>share_cgroups</code> field is set to <code>false</code>, the resource consumption of
each task is managed via a unique set of cgroups associated with that task,
which means that each task process is subject to its own resource requests and
limits. Note that if you want to specify <code>limits</code> on a task, the task MUST set
<code>share_cgroups</code> to <code>false</code>. Also note that all tasks under a single executor
must share the same value of <code>share_cgroups</code>.</p>
<h2 id="example-launching-a-task-group"><a class="header" href="#example-launching-a-task-group">Example: Launching a Task Group</a></h2>
<p>The following could be submitted by a registered scheduler in the body of a POST
request to the Mesos master's <code>/api/v1/scheduler</code> endpoint:</p>
<pre><code>{
  &quot;framework_id&quot;: { &quot;value&quot; : &quot;12220-3440-12532-2345&quot; },
  &quot;type&quot;: &quot;ACCEPT&quot;,
  &quot;accept&quot;: {
    &quot;offer_ids&quot;: [ { &quot;value&quot; : &quot;12220-3440-12532-O12&quot; } ],
    &quot;operations&quot;: [
      {
        &quot;type&quot;: &quot;LAUNCH_GROUP&quot;,
        &quot;launch_group&quot;: {
          &quot;executor&quot;: {
            &quot;type&quot;: &quot;DEFAULT&quot;,
            &quot;executor_id&quot;: { &quot;value&quot;: &quot;28649-27G5-291H9-3816-04&quot; },
            &quot;framework_id&quot;: { &quot;value&quot; : &quot;12220-3440-12532-2345&quot; },
            &quot;resources&quot;: [
              {
                &quot;name&quot;: &quot;cpus&quot;,
                &quot;type&quot;: &quot;SCALAR&quot;,
                &quot;scalar&quot;: { &quot;value&quot;: 0.1 }
              }, {
                &quot;name&quot;: &quot;mem&quot;,
                &quot;type&quot;: &quot;SCALAR&quot;,
                &quot;scalar&quot;: { &quot;value&quot;: 32 }
              }, {
                &quot;name&quot;: &quot;disk&quot;,
                &quot;type&quot;: &quot;SCALAR&quot;,
                &quot;scalar&quot;: { &quot;value&quot;: 32 }
              }
            ]
          },
          &quot;task_group&quot;: {
            &quot;tasks&quot;: [
              {
                &quot;name&quot;: &quot;Name of the task&quot;,
                &quot;task_id&quot;: {&quot;value&quot; : &quot;task-000001&quot;},
                &quot;agent_id&quot;: {&quot;value&quot; : &quot;83J792-S8FH-W397K-2861-S01&quot;},
                &quot;resources&quot;: [
                  {
                    &quot;name&quot;: &quot;cpus&quot;,
                    &quot;type&quot;: &quot;SCALAR&quot;,
                    &quot;scalar&quot;: { &quot;value&quot;: 1.0 }
                  }, {
                    &quot;name&quot;: &quot;mem&quot;,
                    &quot;type&quot;: &quot;SCALAR&quot;,
                    &quot;scalar&quot;: { &quot;value&quot;: 512 }
                  }, {
                    &quot;name&quot;: &quot;disk&quot;,
                    &quot;type&quot;: &quot;SCALAR&quot;,
                    &quot;scalar&quot;: { &quot;value&quot;: 1024 }
                  }
                ],
                &quot;limits&quot;: {
                  &quot;cpus&quot;: &quot;Infinity&quot;,
                  &quot;mem&quot;: 4096
                }
                &quot;command&quot;: { &quot;value&quot;: &quot;./my-artifact/run.sh&quot; },
                &quot;container&quot;: {
                  &quot;type&quot;: &quot;MESOS&quot;,
                  &quot;linux_info&quot;: { &quot;share_cgroups&quot;: false }
                },
                &quot;uris&quot;: [
                  { &quot;value&quot;: &quot;https://my-server.com/my-artifact.tar.gz&quot; }
                ]
              }
            ]
          }
        }
      }
    ],
    &quot;filters&quot;: { &quot;refuse_seconds&quot; : 5.0 }
  }
}
</code></pre>
<h1 id="command-tasks"><a class="header" href="#command-tasks">Command Tasks</a></h1>
<p>One or more simple tasks which specify a single container image and/or command
to execute can be launched using the <code>LAUNCH</code> operation. The same <code>TaskInfo</code>
message type is used in both the <code>LAUNCH_GROUP</code> and <code>LAUNCH</code> calls to describe
tasks, so the operations look similar and identical fields in the task generally
behave in the same way. Depending on the container type specified within the
task's <code>container</code> field, the task will be launched using either the Mesos
containerizer (Mesos in-tree container runtime) or the Docker containerizer
(wrapper around Docker runtime). Note that the
<code>container.linux_info.share_cgroups</code> field, if set, must be set to <code>true</code> for
command tasks.</p>
<p>The below example could be used as the payload of a POST request to the
scheduler API endpoint:</p>
<pre><code>{
  &quot;framework_id&quot;: { &quot;value&quot; : &quot;12220-3440-12532-2345&quot; },
  &quot;type&quot;: &quot;ACCEPT&quot;,
  &quot;accept&quot;: {
    &quot;offer_ids&quot;: [ { &quot;value&quot; : &quot;12220-3440-12532-O12&quot; } ],
    &quot;operations&quot;: [
      {
        &quot;type&quot;: &quot;LAUNCH&quot;,
        &quot;launch&quot;: {
          &quot;task_infos&quot;: [
            {
              &quot;name&quot;: &quot;Name of the task&quot;,
              &quot;task_id&quot;: {&quot;value&quot; : &quot;task-000001&quot;},
              &quot;agent_id&quot;: {&quot;value&quot; : &quot;83J792-S8FH-W397K-2861-S01&quot;},
              &quot;resources&quot;: [
                {
                  &quot;name&quot;: &quot;cpus&quot;,
                  &quot;type&quot;: &quot;SCALAR&quot;,
                  &quot;scalar&quot;: { &quot;value&quot;: 1.0 }
                }, {
                  &quot;name&quot;: &quot;mem&quot;,
                  &quot;type&quot;: &quot;SCALAR&quot;,
                  &quot;scalar&quot;: { &quot;value&quot;: 512 }
                }, {
                  &quot;name&quot;: &quot;disk&quot;,
                  &quot;type&quot;: &quot;SCALAR&quot;,
                  &quot;scalar&quot;: { &quot;value&quot;: 1024 }
                }
              ],
              &quot;limits&quot;: {
                &quot;cpus&quot;: &quot;Infinity&quot;,
                &quot;mem&quot;: 4096
              }
              &quot;command&quot;: { &quot;value&quot;: &quot;./my-artifact/run.sh&quot; },
              &quot;container&quot;: {
                &quot;type&quot;: &quot;MESOS&quot;,
                &quot;linux_info&quot;: { &quot;share_cgroups&quot;: false }
              },
              &quot;uris&quot;: [
                { &quot;value&quot;: &quot;https://my-server.com/my-artifact.tar.gz&quot; }
              ]
            }
          ]
        }
      }
    ],
    &quot;filters&quot;: { &quot;refuse_seconds&quot; : 5.0 }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Framework Development Guide
layout: documentation</h2>
<h1 id="framework-development-guide"><a class="header" href="#framework-development-guide">Framework Development Guide</a></h1>
<p>In this document we refer to Mesos applications as &quot;frameworks&quot;.</p>
<p>See one of the example framework schedulers in <code>MESOS_HOME/src/examples/</code> to
get an idea of what a Mesos framework scheduler and executor in the language
of your choice looks like. <a href="https://github.com/mesosphere/RENDLER">RENDLER</a>
provides example framework implementations in C++, Go, Haskell, Java, Python
and Scala.</p>
<h2 id="create-your-framework-scheduler"><a class="header" href="#create-your-framework-scheduler">Create your Framework Scheduler</a></h2>
<h3 id="api-1"><a class="header" href="#api-1">API</a></h3>
<p>If you are writing a scheduler against Mesos 1.0 or newer, it is recommended
to use the new <a href="scheduler-http-api.html">HTTP API</a> to talk to Mesos.</p>
<p>If your framework needs to talk to Mesos 0.28.0 or older, or you have not updated to the
<a href="scheduler-http-api.html">HTTP API</a>, you can write the scheduler in C++, Java/Scala, or Python.
Your framework scheduler should inherit from the <code>Scheduler</code> class
(see: <a href="https://github.com/apache/mesos/blob/1.6.0/include/mesos/scheduler.hpp#L58-L177">C++</a>,
<a href="http://mesos.apache.org/api/latest/java/org/apache/mesos/Scheduler.html">Java</a>,
<a href="https://github.com/apache/mesos/blob/1.6.0/src/python/interface/src/mesos/interface/__init__.py#L34-L137">Python</a>). Your scheduler should create a SchedulerDriver (which will mediate
communication between your scheduler and the Mesos master) and then call <code>SchedulerDriver.run()</code>
(see: <a href="https://github.com/apache/mesos/blob/1.6.0/include/mesos/scheduler.hpp#L180-L317">C++</a>,
<a href="http://mesos.apache.org/api/latest/java/org/apache/mesos/SchedulerDriver.html">Java</a>,
<a href="https://github.com/apache/mesos/blob/1.6.0/src/python/interface/src/mesos/interface/__init__.py#L140-L278">Python</a>).</p>
<h3 id="high-availability"><a class="header" href="#high-availability">High Availability</a></h3>
<p>How to build Mesos frameworks that are highly available in the face of failures is
discussed in a <a href="high-availability-framework-guide.html">separate document</a>.</p>
<h3 id="multi-scheduler-scalability"><a class="header" href="#multi-scheduler-scalability">Multi-Scheduler Scalability</a></h3>
<p>When implementing a scheduler, it's important to adhere to the following guidelines
in order to ensure that the scheduler can run in a scalable manner alongside other
schedulers in the same Mesos cluster:</p>
<ol>
<li><strong>Use <code>Suppress</code></strong>: The scheduler must stay in a suppressed state whenever it has
no additional tasks to launch or offer operations to perform. This ensures
that Mesos can more efficiently offer resources to those frameworks that do
have work to perform.</li>
<li><strong>Do not hold onto offers</strong>: If an offer cannot be used, decline it immediately.
Otherwise the resources cannot be offered to other schedulers and the scheduler
itself will receive fewer additional offers.</li>
<li><strong><code>Decline</code> resources using a large timeout</strong>: when declining an offer, use a
large <code>Filters.refuse_seconds</code> timeout (e.g. 1 hour). This ensures that Mesos
will have time to try offering the resources to other scheduler before trying
the same scheduler again. However, if the scheduler is unable to eventually
enter a <code>SUPPRESS</code>ed state, and it has new workloads to run after having declined,
it should consider <code>REVIVE</code>ing if it is not receiving sufficient resources for
some time.</li>
<li><strong>Do not <code>REVIVE</code> frequently</strong>: <code>REVIVE</code>ing clears all filters, and therefore
if <code>REVIVE</code> occurs frequently it is similar to always declining with a very
short timeout (violation of guideline (3)).</li>
<li><strong>Use <code>FrameworkInfo.offer_filters</code></strong>: This allows the scheduler to specify
global offer filters (<code>Decline</code> filters, on the other hand, are per-agent).
Currently supported is <code>OfferFilters.min_allocatable_resources</code> which acts as
an override of the cluster level <code>--min_allocatable_resources</code> master flag for
each of the scheduler's roles. Keeping the <code>FrameworkInfo.offer_filters</code>
up-to-date with the minimum desired offer shape for each role will ensure that
the sccheduler gets a better chance to receive offers sized with sufficient
resources.</li>
<li>Consider specifying <strong>offer constraints</strong> via <code>SUBSCRIBE</code>/<code>UPDATE_FRAMEWORK</code>
calls so that the framework role's quota is not consumed by offers that the
scheduler will have to decline anyway based on agent attributes.
See <a href="https://issues.apache.org/jira/browse/MESOS-10161%5D">MESOS-10161</a>
and <a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/scheduler/scheduler.proto">scheduler.proto</a>
for more details.</li>
</ol>
<p>Operationally, the following can be done to ensure that schedulers get the resources
they need when co-existing with other schedulers:</p>
<ol>
<li><strong>Do not share a role between schedulers</strong>: Roles are the level at which controls
are available (e.g. quota, weight, reservation) that affect resource allocation.
Within a role, there are no controls to alter the behavior should one scheduler
not receive enough resources.</li>
<li><strong>Set quota if roles need a guarantee</strong>: If a role (either an entire scheduler or
a &quot;job&quot;/&quot;service&quot;/etc within a multi-tenant scheduler) needs a certain amount of
resources guaranteed to it, setting a quota ensures that Mesos will try its best
to allocate to satisfy the guarantee.</li>
<li><strong>Set the minimum allocatable resources</strong>: Once quota is used, the
<code>--min_allocatable_resources</code> flag should be set
(e.g. <code>--min_allocatable_resources=cpus:0.1,mem:32:disk:32</code>) to prevent offers
that are missing cpu, memory, or disk
(see <a href="https://issues.apache.org/jira/browse/MESOS-8935">MESOS-8935</a>).</li>
<li><strong>Consider enabling the random sorter</strong>: Depending on the use case, DRF can prove
problematic in that it will try to allocate to frameworks with a low share of the
cluster and penalize frameworks with a high share of the cluster. This can lead
to offer starvation for higher share frameworks. To allocate using a weighted
random uniform distribution instead of fair sharing, set <code>--role_sorter=random</code>
and <code>--framework_sorter=random</code> (see
<a href="https://issues.apache.org/jira/browse/MESOS-8936">MESOS-8936</a>).</li>
</ol>
<p>See the <a href="https://docs.google.com/document/d/1uvTmBo_21Ul9U_mijgWyh7hE0E_yZXrFr43JIB9OCl8">Offer Starvation Design Document</a>
in <a href="https://issues.apache.org/jira/browse/MESOS-3202">MESOS-3202</a> for more
information about the pitfalls and future plans for running multiple schedulers.</p>
<h2 id="working-with-executors"><a class="header" href="#working-with-executors">Working with Executors</a></h2>
<h3 id="using-the-mesos-command-executor"><a class="header" href="#using-the-mesos-command-executor">Using the Mesos Command Executor</a></h3>
<p>Mesos provides a simple executor that can execute shell commands and Docker
containers on behalf of the framework scheduler; enough functionality for a
wide variety of framework requirements.</p>
<p>Any scheduler can make use of the Mesos command executor by filling in the
optional <code>CommandInfo</code> member of the <code>TaskInfo</code> protobuf message.</p>
<pre><code class="language-{.proto}">message TaskInfo {
  ...
  optional CommandInfo command = 7;
  ...
}
</code></pre>
<p>The Mesos slave will fill in the rest of the <code>ExecutorInfo</code> for you when tasks
are specified this way.</p>
<p>Note that the agent will derive an <code>ExecutorInfo</code> from the <code>TaskInfo</code> and
additionally copy fields (e.g., <code>Labels</code>) from <code>TaskInfo</code> into the new
<code>ExecutorInfo</code>. This <code>ExecutorInfo</code> is only visible on the agent.</p>
<h3 id="using-the-mesos-default-executor"><a class="header" href="#using-the-mesos-default-executor">Using the Mesos Default Executor</a></h3>
<p>Since Mesos 1.1, a new built-in default executor (<strong>experimental</strong>) is available that
can execute a group of tasks. Just like the command executor the tasks can be shell
commands or Docker containers.</p>
<p>The current semantics of the default executor are as folows:</p>
<p>-- Task group is an atomic unit of deployment of a scheduler onto the default executor.</p>
<p>-- The default executor can run one or more task groups (since Mesos 1.2) and each task group can be launched by the scheduler at different points in time.</p>
<p>-- All task groups' tasks are launched as nested containers underneath the executor container.</p>
<p>-- Task containers and executor container share resources like cpu, memory,
network and volumes.</p>
<p>-- Each task can have its own separate root file system (e.g., Docker image).</p>
<p>-- There is no resource isolation between different tasks or task groups within an executor.
Tasks' resources are added to the executor container.</p>
<p>-- If any of the tasks exits with a non-zero exit code or killed by the scheduler, all the tasks in the task group
are killed automatically. The default executor commits suicide if there are no active task groups.</p>
<p>Once the default executor is considered <strong>stable</strong>, the command executor will be deprecated in favor of it.</p>
<p>Any scheduler can make use of the Mesos default executor by setting <code>ExecutorInfo.type</code>
to <code>DEFAULT</code> when launching a group of tasks using the <code>LAUNCH_GROUP</code> offer operation.
If <code>DEFAULT</code> executor is explicitly specified when using <code>LAUNCH</code> offer operation, command
executor is used instead of the default executor. This might change in the future when the default
executor gets support for handling <code>LAUNCH</code> operation.</p>
<pre><code class="language-{.proto}">message ExecutorInfo {
  ...
    optional Type type = 15;
  ...
}
</code></pre>
<h3 id="creating-a-custom-framework-executor"><a class="header" href="#creating-a-custom-framework-executor">Creating a custom Framework Executor</a></h3>
<p>If your framework has special requirements, you might want to provide your own
Executor implementation. For example, you may not want a 1:1 relationship
between tasks and processes.</p>
<p>If you are writing an executor against Mesos 1.0 or newer, it is recommended
to use the new <a href="executor-http-api.html">HTTP API</a> to talk to Mesos.</p>
<p>If writing against Mesos 0.28.0 or older, your framework executor must inherit
from the Executor class (see (see: <a href="https://github.com/apache/mesos/blob/1.6.0/include/mesos/executor.hpp#L60-L137">C++</a>,
<a href="http://mesos.apache.org/api/latest/java/org/apache/mesos/Executor.html">Java</a>,
<a href="https://github.com/apache/mesos/blob/1.6.0/src/python/interface/src/mesos/interface/__init__.py#L280-L344">Python</a>). It must override the launchTask() method. You can use
the $MESOS_HOME environment variable inside of your executor to determine where
Mesos is running from. Your executor should create an ExecutorDriver (which will
mediate communication between your executor and the Mesos agent) and then call
<code>ExecutorDriver.run()</code>
(see: <a href="https://github.com/apache/mesos/blob/1.6.0/include/mesos/executor.hpp#L140-L188">C++</a>,
<a href="http://mesos.apache.org/api/latest/java/org/apache/mesos/ExecutorDriver.html">Java</a>,
<a href="https://github.com/apache/mesos/blob/1.6.0/src/python/interface/src/mesos/interface/__init__.py#L348-L401">Python</a>).</p>
<h4 id="install-your-custom-framework-executor"><a class="header" href="#install-your-custom-framework-executor">Install your custom Framework Executor</a></h4>
<p>After creating your custom executor, you need to make it available to all slaves
in the cluster.</p>
<p>One way to distribute your framework executor is to let the
<a href="fetcher.html">Mesos fetcher</a> download it on-demand when your scheduler launches
tasks on that slave. <code>ExecutorInfo</code> is a Protocol Buffer Message class (defined
in <code>include/mesos/mesos.proto</code>), and it contains a field of type <code>CommandInfo</code>.
<code>CommandInfo</code> allows schedulers to specify, among other things, a number of
resources as URIs. These resources are fetched to a sandbox directory on the
slave before attempting to execute the <code>ExecutorInfo</code> command. Several URI
schemes are supported, including HTTP, FTP, HDFS, and S3 (e.g. see
src/examples/java/TestFramework.java for an example of this).</p>
<p>Alternatively, you can pass the <code>frameworks_home</code> configuration option
(defaults to: <code>MESOS_HOME/frameworks</code>) to your <code>mesos-slave</code> daemons when you
launch them to specify where your framework executors are stored (e.g. on an
NFS mount that is available to all slaves), then use a relative path in
<code>CommandInfo.uris</code>, and the slave will prepend the value of <code>frameworks_home</code>
to the relative path provided.</p>
<p>Once you are sure that your executors are available to the mesos-slaves, you
should be able to run your scheduler, which will register with the Mesos master,
and start receiving resource offers!</p>
<h2 id="labels"><a class="header" href="#labels">Labels</a></h2>
<p><code>Labels</code> can be found in the <code>FrameworkInfo</code>, <code>TaskInfo</code>, <code>DiscoveryInfo</code> and
<code>TaskStatus</code> messages; framework and module writers can use Labels to tag and
pass unstructured information around Mesos. Labels are free-form key-value pairs
supplied by the framework scheduler or label decorator hooks. Below is the
protobuf definitions of labels:</p>
<pre><code class="language-{.proto}">  optional Labels labels = 11;
</code></pre>
<pre><code class="language-{.proto}">/**
 * Collection of labels.
 */
message Labels {
    repeated Label labels = 1;
}

/**
 * Key, value pair used to store free form user-data.
 */
message Label {
  required string key = 1;
  optional string value = 2;
}
</code></pre>
<p>Labels are not interpreted by Mesos itself, but will be made available over
master and slave state endpoints. Further more, the executor and scheduler can
introspect labels on the <code>TaskInfo</code> and <code>TaskStatus</code> programmatically.
Below is an example of how two label pairs (<code>&quot;environment&quot;: &quot;prod&quot;</code> and
<code>&quot;bananas&quot;: &quot;apples&quot;</code>) can be fetched from the master state endpoint.</p>
<pre><code class="language-{.sh}">$ curl http://master/state.json
...
{
  &quot;executor_id&quot;: &quot;default&quot;,
  &quot;framework_id&quot;: &quot;20150312-120017-16777343-5050-39028-0000&quot;,
  &quot;id&quot;: &quot;3&quot;,
  &quot;labels&quot;: [
    {
      &quot;key&quot;: &quot;environment&quot;,
      &quot;value&quot;: &quot;prod&quot;
    },
    {
      &quot;key&quot;: &quot;bananas&quot;,
      &quot;value&quot;: &quot;apples&quot;
    }
  ],
  &quot;name&quot;: &quot;Task 3&quot;,
  &quot;slave_id&quot;: &quot;20150312-115625-16777343-5050-38751-S0&quot;,
  &quot;state&quot;: &quot;TASK_FINISHED&quot;,
  ...
},
</code></pre>
<h2 id="service-discovery"><a class="header" href="#service-discovery">Service discovery</a></h2>
<p>When your framework registers an executor or launches a task, it can provide
additional information for service discovery. This information is stored by
the Mesos master along with other imporant information such as the slave
currently running the task. A service discovery system can programmatically
retrieve this information in order to set up DNS entries, configure proxies,
or update any consistent store used for service discovery in a Mesos cluster
that runs multiple frameworks and multiple tasks.</p>
<p>The optional <code>DiscoveryInfo</code> message for <code>TaskInfo</code> and <code>ExecutorInfo</code> is
declared in  <code>MESOS_HOME/include/mesos/mesos.proto</code></p>
<pre><code class="language-{.proto}">message DiscoveryInfo {
  enum Visibility {
    FRAMEWORK = 0;
    CLUSTER = 1;
    EXTERNAL = 2;
  }

  required Visibility visibility = 1;
  optional string name = 2;
  optional string environment = 3;
  optional string location = 4;
  optional string version = 5;
  optional Ports ports = 6;
  optional Labels labels = 7;
}
</code></pre>
<p><code>Visibility</code> is the key parameter that instructs the service discovery system
whether a service should be discoverable. We currently differentiate between
three cases:</p>
<ul>
<li>a task should not be discoverable for anyone but its framework.</li>
<li>a task should be discoverable for all frameworks running on the Mesos cluster
but not externally.</li>
<li>a task should be made discoverable broadly.</li>
</ul>
<p>Many service discovery systems provide additional features that manage the
visibility of services (e.g., ACLs in proxy based systems, security extensions
to DNS, VLAN or subnet selection). It is not the intended use of the visibility
field to manage such features. When a service discovery system retrieves the
task or executor information from the master, it can decide how to handle tasks
without <code>DiscoveryInfo</code>. For instance, tasks may be made non discoverable to
other frameworks (equivalent to <code>visibility=FRAMEWORK</code>) or discoverable to all
frameworks (equivalent to <code>visibility=CLUSTER</code>).</p>
<p>The <code>name</code> field is a string that provides the service discovery system
with the name under which the task is discoverable. The typical use of the name
field will be to provide a valid hostname. If name is not provided, it is up to
the service discovery system to create a name for the task based on the name
field in <code>taskInfo</code> or other information.</p>
<p>The <code>environment</code>, <code>location</code>, and <code>version</code> fields provide first class support
for common attributes used to differentiate between similar services in large
deployments. The <code>environment</code> may receive values such as <code>PROD/QA/DEV</code>, the
<code>location</code> field may receive values like <code>EAST-US/WEST-US/EUROPE/AMEA</code>, and the
<code>version</code> field may receive values like v2.0/v0.9. The exact use of these fields
is up to the service discovery system.</p>
<p>The <code>ports</code> field allows the framework to identify the ports a task listens to
and explicitly name the functionality they represent and the layer-4 protocol
they use (TCP, UDP, or other). For example, a Cassandra task will define ports
like <code>&quot;7000,Cluster,TCP&quot;</code>, <code>&quot;7001,SSL,TCP&quot;</code>, <code>&quot;9160,Thrift,TCP&quot;</code>,
<code>&quot;9042,Native,TCP&quot;</code>, and <code>&quot;7199,JMX,TCP&quot;</code>. It is up to the service discovery
system to use these names and protocol in appropriate ways, potentially
combining them with the <code>name</code> field in <code>DiscoveryInfo</code>.</p>
<p>The <code>labels</code> field allows a framework to pass arbitrary labels to the service
discovery system in the form of key/value pairs. Note that anything passed
through this field is not guaranteed to be supported moving forward.
Nevertheless, this field provides extensibility. Common uses of this field will
allow us to identify use cases that require first class support.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Designing Highly Available Mesos Frameworks
layout: documentation</h2>
<h1 id="designing-highly-available-mesos-frameworks"><a class="header" href="#designing-highly-available-mesos-frameworks">Designing Highly Available Mesos Frameworks</a></h1>
<p>A Mesos framework manages tasks. For a Mesos framework to be highly available,
it must continue to manage tasks correctly in the presence of a variety of
failure scenarios. The most common failure conditions that framework authors
should consider include:</p>
<ul>
<li>
<p>The Mesos master that a framework scheduler is connected to might fail, for
example by crashing or by losing network connectivity. If the master has been
configured to use <a href="high-availability.html">high-availability mode</a>, this will
result in promoting another Mesos master replica to become the current
leader. In this situation, the scheduler should reregister with the new
master and ensure that task state is consistent.</p>
</li>
<li>
<p>The host where a framework scheduler is running might fail. To ensure that the
framework remains available and can continue to schedule new tasks, framework
authors should ensure that multiple copies of the scheduler run on different
nodes, and that a backup copy is promoted to become the new leader when the
previous leader fails. Mesos itself does not dictate how framework authors
should handle this situation, although we provide some suggestions below. It
can be useful to deploy multiple copies of your framework scheduler using
a long-running task scheduler such as Apache Aurora or Marathon.</p>
</li>
<li>
<p>The host where a task is running might fail. Alternatively, the node itself
might not have failed but the Mesos agent on the node might be unable to
communicate with the Mesos master, e.g., due to a network partition.</p>
</li>
</ul>
<p>Note that more than one of these failures might occur simultaneously.</p>
<h2 id="mesos-architecture-1"><a class="header" href="#mesos-architecture-1">Mesos Architecture</a></h2>
<p>Before discussing the specific failure scenarios outlined above, it is worth
highlighting some aspects of how Mesos is designed that influence high
availability:</p>
<ul>
<li>
<p>Mesos provides unreliable messaging between components by default: messages
are delivered &quot;at-most-once&quot; (they might be dropped). Framework authors should
expect that messages they send might not be received and be prepared to take
appropriate corrective action. To detect that a message might be lost,
frameworks typically use timeouts. For example, if a framework attempts to
launch a task, that message might not be received by the Mesos master (e.g.,
due to a transient network failure). To address this, the framework scheduler
should set a timeout after attempting to launch a new task. If the scheduler
hasn't seen a status update for the new task before the timeout fires, it
should take corrective action---for example, by performing <a href="reconciliation.html">task state reconciliation</a>,
and then launching a new copy of the task if necessary.</p>
<ul>
<li>
<p>In general, distributed systems cannot distinguish between &quot;lost&quot; messages
and messages that are merely delayed. In the example above, the scheduler
might see a status update for the first task launch attempt immediately
<em>after</em> its timeout has fired and it has already begun taking corrective
action. Scheduler authors should be aware of this possibility and program
accordingly.</p>
</li>
<li>
<p>Mesos actually provides ordered (but unreliable) message delivery between
any pair of processes: for example, if a framework sends messages M1 and M2
to the master, the master might receive no messages, just M1, just M2, or M1
followed by M2 -- it will <em>not</em> receive M2 followed by M1.</p>
</li>
<li>
<p>As a convenience for framework authors, Mesos provides reliable delivery of
task status updates and operation status updates. The agent persists these
updates to disk and then forwards them to the master. The master sends
status updates to the appropriate framework scheduler. When a scheduler
acknowledges a status update, the master forwards the acknowledgment back to
the agent, which allows the stored status update to be garbage collected. If
the agent does not receive an acknowledgment for a status update within a
certain amount of time, it will repeatedly resend the update to the master,
which will again forward the update to the scheduler. Hence, task and
operation status updates will be delivered &quot;at least once&quot;, assuming that
the agent and the scheduler both remain available. To handle the fact that
task and operation status updates might be delivered more than once, it can
be helpful to make the framework logic that processes them
<a href="https://en.wikipedia.org/wiki/Idempotence">idempotent</a>.</p>
</li>
</ul>
</li>
<li>
<p>The Mesos master stores information about the active tasks and registered
frameworks <em>in memory</em>: it does not persist it to disk or attempt to ensure
that this information is preserved after a master failover. This helps the
Mesos master scale to large clusters with many tasks and frameworks. A
downside of this design is that after a failure, more work is required to
recover the lost in-memory master state.</p>
</li>
<li>
<p>If all the Mesos masters are unavailable (e.g., crashed or unreachable), the
cluster should continue to operate: existing Mesos agents and user tasks should
continue running. However, new tasks cannot be scheduled, and frameworks will
not receive resource offers or status updates about previously launched tasks.</p>
</li>
<li>
<p>Mesos does not dictate how frameworks should be implemented and does not try
to assume responsibility for how frameworks should deal with failures.
Instead, Mesos tries to provide framework developers with the tools they need
to implement this behavior themselves. Different frameworks might choose to
handle failures differently, depending on their exact requirements.</p>
</li>
</ul>
<h2 id="recommendations-for-highly-available-frameworks"><a class="header" href="#recommendations-for-highly-available-frameworks">Recommendations for Highly Available Frameworks</a></h2>
<p>Highly available framework designs typically follow a few common patterns:</p>
<ol>
<li>
<p>To tolerate scheduler failures, frameworks run multiple scheduler instances
(three instances is typical). At any given time, only one of these scheduler
instances is the <em>leader</em>: this instance is connected to the Mesos master,
receives resource offers and task status updates, and launches new tasks. The
other scheduler replicas are <em>followers</em>: they are used only when the leader
fails, in which case one of the followers is chosen to become the new leader.</p>
</li>
<li>
<p>Schedulers need a mechanism to decide when the current scheduler leader has
failed and to elect a new leader. This is typically accomplished using a
coordination service like <a href="https://zookeeper.apache.org/">Apache ZooKeeper</a>
or <a href="https://github.com/coreos/etcd">etcd</a>. Consult the documentation of the
coordination system you are using for more information on how to correctly
implement leader election.</p>
</li>
<li>
<p>After electing a new leading scheduler, the new leader should reconnect to
the Mesos master. When registering with the master, the framework should set
the <code>id</code> field in its <code>FrameworkInfo</code> to the ID that was assigned to the
failed scheduler instance. This ensures that the master will recognize that
the connection does not start a new session, but rather continues (and
replaces) the session used by the failed scheduler instance.</p>
<blockquote>
<p>NOTE: When the old scheduler leader disconnects from the master, by default
the master will immediately kill all the tasks and executors associated with
the failed framework. For a typical production framework, this default
behavior is very undesirable! To avoid this, highly available frameworks
should set the <code>failover_timeout</code> field in their <code>FrameworkInfo</code> to a
generous value. To avoid accidental destruction of tasks in production
environments, many frameworks use a <code>failover_timeout</code> of 1 week or more.</p>
</blockquote>
<ul>
<li>In the current implementation, a framework's <code>failover_timeout</code> is not
preserved during master failover. Hence, if a framework fails but the
leading master fails before the <code>failover_timeout</code> is reached, the newly
elected leading master won't know that the framework's tasks should be
killed after a period of time. Hence, if the framework never
reregisters, those tasks will continue to run indefinitely but will be
orphaned. This behavior will likely be fixed in a future version of
Mesos (<a href="https://issues.apache.org/jira/browse/MESOS-4659">MESOS-4659</a>).</li>
</ul>
</li>
<li>
<p>After connecting to the Mesos master, the new leading scheduler should ensure
that its local state is consistent with the current state of the cluster. For
example, suppose that the previous leading scheduler attempted to launch a
new task and then immediately failed. The task might have launched
successfully, at which point the newly elected leader will begin to receive
status updates about it. To handle this situation, frameworks typically use a
strongly consistent distributed data store to record information about active
and pending tasks. In fact, the same coordination service that is used for
leader election (such as ZooKeeper or etcd) can often be used for this
purpose. Some Mesos frameworks (such as Apache Aurora) use the Mesos
<a href="replicated-log-internals.html">replicated log</a> for this purpose.</p>
<ul>
<li>
<p>The data store should be used to record the actions that the scheduler
<em>intends</em> to take, before it takes them. For example, if a scheduler
decides to launch a new task, it <em>first</em> writes this intent to its data
store. Then it sends a &quot;launch task&quot; message to the Mesos master. If this
instance of the scheduler fails and a new scheduler is promoted to become
the leader, the new leader can consult the data store to find <em>all possible
tasks</em> that might be running on the cluster. This is an instance of the
<a href="https://en.wikipedia.org/wiki/Write-ahead_logging">write-ahead logging</a>
pattern often employed by database systems and filesystems to improve
reliability. Two aspects of this design are worth emphasizing.</p>
<ol>
<li>
<p>The scheduler must persist its intent <em>before</em> launching the task: if
the task is launched first and then the scheduler fails before it can
write to the data store, the new leading scheduler won't know about the
new task. If this occurs, the new scheduler instance will begin
receiving task status updates for a task that it has no knowledge of;
there is often not a good way to recover from this situation.</p>
</li>
<li>
<p>Second, the scheduler should ensure that its intent has been durably
recorded in the data store before continuing to launch the task (for
example, it should wait for a quorum of replicas in the data store to
have acknowledged receipt of the write operation). For more details on
how to do this, consult the documentation for the data store you are
using.</p>
</li>
</ol>
</li>
</ul>
</li>
</ol>
<h2 id="the-life-cycle-of-a-task"><a class="header" href="#the-life-cycle-of-a-task">The Life Cycle of a Task</a></h2>
<p>A Mesos task transitions through a sequence of states. The authoritative &quot;source
of truth&quot; for the current state of a task is the agent on which the task is
running. A framework scheduler learns about the current state of a task by
communicating with the Mesos master---specifically, by listening for task status
updates and by performing task state reconciliation.</p>
<p>Frameworks can represent the state of a task using a state machine, with one
initial state and several possible terminal states:</p>
<ul>
<li>
<p>A task begins in the <code>TASK_STAGING</code> state. A task is in this state when the
master has received the framework's request to launch the task but the task
has not yet started to run. In this state, the task's dependencies are
fetched---for example, using the <a href="fetcher.html">Mesos fetcher cache</a>.</p>
</li>
<li>
<p>The <code>TASK_STARTING</code> state is optional. It can be used to describe the fact
that an executor has learned about the task (and maybe started fetching its
dependencies) but has not yet started to run it. Custom executors are
encouraged to send it, to provide a more detailed description of the current
task state to outside observers.</p>
</li>
<li>
<p>A task transitions to the <code>TASK_RUNNING</code> state after it has begun running
successfully (if the task fails to start, it transitions to one of the
terminal states listed below).</p>
<ul>
<li>
<p>If a framework attempts to launch a task but does not receive a status
update for it within a timeout, the framework should perform
<a href="reconciliation.html">reconciliation</a>. That is, it should ask the master for
the current state of the task. The master will reply with <code>TASK_LOST</code> status
updates for unknown tasks. The framework can then use this to distinguish
between tasks that are slow to launch and tasks that the master has never
heard about (e.g., because the task launch message was dropped).</p>
<ul>
<li>Note that the correctness of this technique depends on the fact that
messaging between the scheduler and the master is ordered.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>The <code>TASK_KILLING</code> state is optional and is intended to indicate that the
request to kill the task has been received by the executor, but the task has
not yet been killed. This is useful for tasks that require some time to
terminate gracefully. Executors must not generate this state unless the
framework has the <code>TASK_KILLING_STATE</code> framework capability.</p>
</li>
<li>
<p>There are several terminal states:</p>
<ul>
<li><code>TASK_FINISHED</code> is used when a task completes successfully.</li>
<li><code>TASK_FAILED</code> indicates that a task aborted with an error.</li>
<li><code>TASK_KILLED</code> indicates that a task was killed by the executor.</li>
<li><code>TASK_LOST</code> indicates that the task was running on an agent that has lost
contact with the current master (typically due to a network partition or an
agent host failure). This case is described further below.</li>
<li><code>TASK_ERROR</code> indicates that a task launch attempt failed because of an error
in the task specification.</li>
</ul>
</li>
</ul>
<p>Note that the same task status can be used in several different (but usually
related) situations. For example, <code>TASK_ERROR</code> is used when the framework's
principal is not authorized to launch tasks as a certain user, and also when the
task description is syntactically malformed (e.g., the task ID contains an
invalid character). The <code>reason</code> field of the <code>TaskStatus</code> message can be used
to disambiguate between such situations.</p>
<h2 id="performing-operations-on-offered-resources"><a class="header" href="#performing-operations-on-offered-resources">Performing operations on offered resources</a></h2>
<p>The scheduler API provides a number of operations which can be applied to
resources included in offers sent to a framework scheduler. Schedulers which use
the <a href="scheduler-http-api.html">v1 scheduler API</a> may set the <code>id</code> field in an offer
operation in order to request feedback for the operation. When this is done, the
scheduler will receive <code>UPDATE_OPERATION_STATUS</code> events on its HTTP event stream
when the operation transitions to a new state. Additionally, the scheduler may
use the <code>RECONCILE_OPERATIONS</code> call to perform explicit or implicit
<a href="reconciliation.html">reconciliation</a> of its operations' states, similar to task
state reconciliation.</p>
<p>Unlike tasks, which occur as the result of <code>LAUNCH</code> or <code>LAUNCH_GROUP</code>
operations, other operations do not currently have intermediate states that they
transition through:</p>
<ul>
<li>
<p>An operation begins in the <code>OPERATION_PENDING</code> state. In the absence of any
system failures, it remains in this state until it transitions to a terminal
state.</p>
</li>
<li>
<p>There exist several terminal states that an operation may transition to:</p>
<ul>
<li><code>OPERATION_FINISHED</code> is used when an operation completes successfully.</li>
<li><code>OPERATION_FAILED</code> is used when an operation was attempted but failed to
complete.</li>
<li><code>OPERATION_ERROR</code> is used when an operation failed because it was not
specified correctly and was thus never attempted.</li>
<li><code>OPERATION_DROPPED</code> is used when an operation was not successfully delivered
to the agent.</li>
</ul>
</li>
<li>
<p>When performing operation reconciliation, the scheduler may encounter other
non-terminal states due to various failures in the system:</p>
<ul>
<li><code>OPERATION_UNREACHABLE</code> is used when an operation was previously pending on
an agent which is not currently reachable by the Mesos master.</li>
<li><code>OPERATION_RECOVERING</code> is used when an operation was previously pending on
an agent which has been recovered from the master's checkpointed state after
a master failover, but which has not yet reregistered.</li>
<li><code>OPERATION_UNKNOWN</code> is used when Mesos does not recognize an operation ID
included in an explicit reconciliation request. This may be because an
operation with that ID was never received by the master, or because the
operation state is gone due to garbage collection or a system/network
failure.</li>
<li><code>OPERATION_GONE_BY_OPERATOR</code> is used when an operation was previously
pending on an agent which was marked as &quot;gone&quot; by an operator.</li>
</ul>
</li>
</ul>
<h2 id="dealing-with-partitioned-or-failed-agents"><a class="header" href="#dealing-with-partitioned-or-failed-agents">Dealing with Partitioned or Failed Agents</a></h2>
<p>The Mesos master tracks the availability and health of the registered agents
using two different mechanisms:</p>
<ol>
<li>
<p>The state of a persistent TCP connection between the master and the agent.</p>
</li>
<li>
<p><em>Health checks</em> using periodic ping messages to the agent. The master sends
&quot;ping&quot; messages to the agent and expects a &quot;pong&quot; response message within a
configurable timeout. The agent is considered to have failed if it does not
respond promptly to a certain number of ping messages in a row. This behavior
is controlled by the <code>--agent_ping_timeout</code> and <code>--max_agent_ping_timeouts</code>
master flags.</p>
</li>
</ol>
<p>If the persistent TCP connection to the agent breaks or the agent fails health
checks, the master decides that the agent has failed and takes steps to remove
it from the cluster. Specifically:</p>
<ul>
<li>
<p>If the TCP connection breaks, the agent is considered disconnected. The
semantics when a registered agent gets disconnected are as follows for each
framework running on that agent:</p>
<ul>
<li>
<p>If the framework is <a href="agent-recovery.html">checkpointing</a>: no immediate action
is taken. The agent is given a chance to reconnect until health checks time
out.</p>
</li>
<li>
<p>If the framework is not checkpointing: all the framework's tasks and
executors are considered lost. The master immediately sends <code>TASK_LOST</code>
status updates for the tasks. These updates are not delivered reliably to
the scheduler (see NOTE below). The agent is given a chance to reconnect
until health checks timeout. If the agent does reconnect, any tasks for
which <code>TASK_LOST</code> updates were previously sent will be killed.</p>
<ul>
<li>The rationale for this behavior is that, using typical TCP settings, an
error in the persistent TCP connection between the master and the agent is
more likely to correspond to an agent error (e.g., the <code>mesos-agent</code>
process terminating unexpectedly) than a network partition, because the
Mesos health-check timeouts are much smaller than the typical values of
the corresponding TCP-level timeouts. Since non-checkpointing frameworks
will not survive a restart of the <code>mesos-agent</code> process, the master sends
<code>TASK_LOST</code> status updates so that these tasks can be rescheduled
promptly.  Of course, the heuristic that TCP errors do not correspond to
network partitions may not be true in some environments.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>If the agent fails health checks, it is scheduled for removal. The removals can
be rate limited by the master (see <code>--agent_removal_rate_limit</code> master flag)
to avoid removing a slew of agents at once (e.g., during a network partition).</p>
</li>
<li>
<p>When it is time to remove an agent, the master removes the agent from the list
of registered agents in the master's <a href="replicated-log-internals.html">durable state</a>
(this will survive master failover). The master sends a <code>slaveLost</code> callback
to every registered scheduler driver; it also sends <code>TASK_LOST</code> status updates
for every task that was running on the removed agent.</p>
<blockquote>
<p>NOTE: Neither the callback nor the task status updates are delivered
reliably by the master. For example, if the master or scheduler fails over
or there is a network connectivity issue during the delivery of these
messages, they will not be resent.</p>
</blockquote>
</li>
<li>
<p>Meanwhile, any tasks at the removed agent will continue to run and the agent
will repeatedly attempt to reconnect to the master. Once a removed agent is
able to reconnect to the master (e.g., because the network partition has
healed), the reregistration attempt will be refused and the agent will be
asked to shutdown. The agent will then shutdown all running tasks and
executors.  Persistent volumes and dynamic reservations on the removed agent
will be preserved.</p>
<ul>
<li>A removed agent can rejoin the cluster by restarting the <code>mesos-agent</code>
process. When a removed agent is shutdown by the master, Mesos ensures that
the next time <code>mesos-agent</code> is started (using the same work directory at the
same host), the agent will receive a new agent ID; in effect, the agent will
be treated as a newly joined agent. The agent will retain any previously
created persistent volumes and dynamic reservations, although the agent ID
associated with these resources will have changed.</li>
</ul>
</li>
</ul>
<p>Typically, frameworks respond to failed or partitioned agents by scheduling new
copies of the tasks that were running on the lost agent. This should be done
with caution, however: it is possible that the lost agent is still alive, but is
partitioned from the master and is unable to communicate with it. Depending on
the nature of the network partition, tasks on the agent might still be able to
communicate with external clients or other hosts in the cluster. Frameworks can
take steps to prevent this (e.g., by having tasks connect to ZooKeeper and cease
operation if their ZooKeeper session expires), but Mesos leaves such details to
framework authors.</p>
<h2 id="dealing-with-partitioned-or-failed-masters"><a class="header" href="#dealing-with-partitioned-or-failed-masters">Dealing with Partitioned or Failed Masters</a></h2>
<p>The behavior described above does not apply during the period immediately after
a new Mesos master is elected. As noted above, most Mesos master state is only
kept in memory; hence, when the leading master fails and a new master is
elected, the new master will have little knowledge of the current state of the
cluster.  Instead, it rebuilds this information as the frameworks and agents
notice that a new master has been elected and then <em>reregister</em> with it.</p>
<h3 id="framework-reregistration"><a class="header" href="#framework-reregistration">Framework Reregistration</a></h3>
<p>When master failover occurs, frameworks that were connected to the previous
leading master should reconnect to the new leading
master. <code>MesosSchedulerDriver</code> handles most of the details of detecting when the
previous leading master has failed and connecting to the new leader; when the
framework has successfully reregistered with the new leading master, the
<code>reregistered</code> scheduler driver callback will be invoked.</p>
<h3 id="agent-reregistration"><a class="header" href="#agent-reregistration">Agent Reregistration</a></h3>
<p>During the period after a new master has been elected but before a given agent
has reregistered or the <code>agent_reregister_timeout</code> has fired, attempting to
reconcile the state of a task running on that agent will not return any
information (because the master cannot accurately determine the state of the
task).</p>
<p>If an agent does not reregister with the new master within a timeout (controlled
by the <code>--agent_reregister_timeout</code> configuration flag), the master marks the
agent as failed and follows the same steps described above. However, there is
one difference: by default, agents are <em>allowed to reconnect</em> following master
failover, even after the <code>agent_reregister_timeout</code> has fired. This means that
frameworks might see a <code>TASK_LOST</code> update for a task but then later discover
that the task is running (because the agent where it was running was allowed to
reconnect).</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Reconciliation
layout: documentation</h2>
<h1 id="task-reconciliation"><a class="header" href="#task-reconciliation">Task Reconciliation</a></h1>
<p>Messages between framework schedulers and the Mesos master may be dropped due to
failures and network partitions. This may cause a framework scheduler and the
master to have different views of the current state of the cluster. For example,
consider a launch task request sent by a framework.  There are many ways that
failures can prevent the task launch operation from succeeding, such as:</p>
<ul>
<li>Framework fails after persisting its intent to launch the task, but
before the launch task message was sent.</li>
<li>Master fails before receiving the message.</li>
<li>Master fails after receiving the message but before sending it to the
agent.</li>
</ul>
<p>In these cases, the framework believes the task to be staging but the task is
unknown to the master. To cope with such situations, Mesos frameworks should use
<em>reconciliation</em> to ask the master for the current state of their tasks.</p>
<h2 id="how-to-reconcile"><a class="header" href="#how-to-reconcile">How To Reconcile</a></h2>
<p>Frameworks can use the scheduler driver's <code>reconcileTasks</code> method to send a
reconciliation request to the master:</p>
<pre><code class="language-{.cpp}">// Allows the framework to query the status for non-terminal tasks.
// This causes the master to send back the latest task status for
// each task in 'statuses', if possible. Tasks that are no longer
// known will result in a TASK_LOST update. If statuses is empty,
// then the master will send the latest status for each task
// currently known.
virtual Status reconcileTasks(const std::vector&lt;TaskStatus&gt;&amp; statuses);
</code></pre>
<p>Currently, the master will only examine two fields in <code>TaskStatus</code>:</p>
<ul>
<li><code>TaskID</code>: This is required.</li>
<li><code>SlaveID</code>: Optional but recommended. This leads to faster reconciliation in
the presence of agents that are transitioning between states.</li>
</ul>
<p>Mesos provides two forms of reconciliation:</p>
<ul>
<li>&quot;Explicit&quot; reconciliation: the scheduler sends a list of non-terminal task IDs
and the master responds with the latest state for each task, if possible.</li>
<li>&quot;Implicit&quot; reconciliation: the scheduler sends an empty list of tasks
and the master responds with the latest state for all currently known
non-terminal tasks.</li>
</ul>
<p>Reconciliation results are returned as task status updates (e.g., via the
scheduler driver's <code>statusUpdate</code> callback). Status updates that result from
reconciliation requests will their <code>reason</code> field set to
<code>REASON_RECONCILIATION</code>. Note that most of the other fields in the returned
<code>TaskStatus</code> message will not be set: for example, reconciliation cannot be used
to retrieve the <code>labels</code> or <code>data</code> fields associated with a running task.</p>
<h2 id="when-to-reconcile"><a class="header" href="#when-to-reconcile">When To Reconcile</a></h2>
<p>Framework schedulers should periodically reconcile <em>all</em> of their tasks (for
example, every fifteen minutes). This serves two purposes:</p>
<ol>
<li>It is necessary to account for dropped messages between the framework and
the master; for example, see the task launch scenario described above.</li>
<li>It is a defensive programming technique to catch bugs in both the framework
and the Mesos master.</li>
</ol>
<p>As an optimization, framework schedulers should reconcile <em>more frequently</em> when
they have reason to suspect that their local state differs from that of the
master. For example, after a framework launches a task, it should expect to
receive a <code>TASK_RUNNING</code> status update for the new task fairly promptly. If no
such update is received, the framework should perform explicit reconciliation
more quickly than usual.</p>
<p>Similarly, frameworks should initiate reconciliation after both framework
failovers and master failovers. Note that the scheduler driver notifies
frameworks when master failover has occurred (via the <code>reregistered()</code>
callback). For more information, see the
<a href="high-availability-framework-guide.html">guide to designing highly available frameworks</a>.</p>
<h2 id="algorithm"><a class="header" href="#algorithm">Algorithm</a></h2>
<p>This technique for explicit reconciliation reconciles all non-terminal tasks
until an update is received for each task, using exponential backoff to retry
tasks that remain unreconciled. Retries are needed because the master temporarily
may not be able to reply for a particular task. For example, during master
failover the master must reregister all of the agents to rebuild its
set of known tasks (this process can take minutes for large clusters, and
is bounded by the <code>--agent_reregister_timeout</code> flag on the master).</p>
<p>Steps:</p>
<ol>
<li>let <code>start = now()</code></li>
<li>let <code>remaining = { T in tasks | T is non-terminal }</code></li>
<li>Perform reconciliation: <code>reconcile(remaining)</code></li>
<li>Wait for status updates to arrive (use truncated exponential backoff). For each update, note the time of arrival.</li>
<li>let <code>remaining = { T in remaining | T.last_update_arrival() &lt; start }</code></li>
<li>If <code>remaining</code> is non-empty, go to 3.</li>
</ol>
<p>This reconciliation algorithm <strong>must</strong> be run after each (re-)registration.</p>
<p>Implicit reconciliation (passing an empty list) should also be used
periodically, as a defense against data loss in the framework. Unless a
strict registry is in use on the master, its possible for tasks to resurrect
from a LOST state (without a strict registry the master does not enforce
agent removal across failovers). When an unknown task is encountered, the
scheduler should kill or recover the task.</p>
<p>Notes:</p>
<ul>
<li>When waiting for updates to arrive, <strong>use a truncated exponential backoff</strong>.
This will avoid a snowball effect in the case of the driver or master being
backed up.</li>
<li>It is beneficial to ensure that only 1 reconciliation is in progress at a
time, to avoid a snowball effect in the face of many re-registrations.
If another reconciliation should be started while one is in-progress,
then the previous reconciliation algorithm should stop running.</li>
</ul>
<h1 id="offer-reconciliation"><a class="header" href="#offer-reconciliation">Offer Reconciliation</a></h1>
<p>Offers are reconciled automatically after a failure:</p>
<ul>
<li>Offers do not persist beyond the lifetime of a Master.</li>
<li>If a disconnection occurs, offers are no longer valid.</li>
<li>Offers are rescinded and regenerated each time the framework (re-)registers.</li>
</ul>
<h1 id="operation-reconciliation"><a class="header" href="#operation-reconciliation">Operation Reconciliation</a></h1>
<p>When a scheduler specifies an <code>id</code> on an offer operation, the master will
provide updates on the status of that operation. If the scheduler needs to
reconcile its view of the current states of operations with the master's view,
it can do so via the <code>RECONCILE_OPERATIONS</code> call in the v1 scheduler API.</p>
<p>Operation reconciliation is similar to task reconciliation in that the scheduler
can perform either explicit or implicit reconciliation by specifying particular
operation IDs or by leaving the <code>operations</code> field unset, respectively.</p>
<p>In order to explicitly reconcile particular operations, the scheduler should
include in the <code>RECONCILE_OPERATIONS</code> call a list of operations, specifying an
operation ID, agent ID, and resource provider ID (if applicable) for each one.
While the agent and resource provider IDs are optional, the master will be able
to provide the highest quality reconciliation information when they are set. For
example, if the relevant agent is not currently registered, inclusion of the
agent ID will allow the master to respond with states like
<code>OPERATION_RECOVERING</code>, <code>OPERATION_UNREACHABLE</code>, or <code>OPERATION_GONE_BY_OPERATOR</code>
when the agent is recovering, unreachable, or gone, respectively. Inclusion of
the resource provider ID provides the same benefit for cases where the
resource provider is recovering or gone.</p>
<p>Similar to task reconciliation, we recommend that schedulers implement a
periodic reconciliation loop for operations in order to defend against network
failures and bugs in the scheduler and/or Mesos master.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Task State Reasons
layout: documentation</h2>
<h1 id="task-state-reasons"><a class="header" href="#task-state-reasons">Task State Reasons</a></h1>
<p>Some TaskStatus messages will arrive with the <code>reason</code> field set to a value
that can allow frameworks to display better error messages and to implement
special behaviour for some of the reasons.</p>
<p>For most reasons, the <code>message</code> field of the TaskStatus message will give a
more detailed, human-readable error description.</p>
<p>Not all status updates will contain a reason.</p>
<h1 id="guidelines-for-framework-authors"><a class="header" href="#guidelines-for-framework-authors">Guidelines for Framework Authors</a></h1>
<p>Frameworks that implement their own executors are free to set the reason field
on any status messages they produce.</p>
<p>Note that executors can not generally rely on the fact that the scheduler will
see the status update with the reason set by the executor, since only the
latest update for each different task state is stored and re-transmitted. See
in particular the description of <code>REASON_RECONCILIATION</code> below.</p>
<p>Most reasons describe conditions that can only be detected in the master or
agent code, and will accompany automatically generated status updates from
either of these.</p>
<p>For consistency with the existing usages of the different task reasons, we
recommend that executors restrict themselves to the following subset if they
use a non-default reason in their status updates.</p>
<table class="table table-striped">
<tr><td><code>     REASON_TASK_CHECK_STATUS_UPDATED
</code></td><td>   For executors that support running task checks, it is
                   recommended to generate a status update with this reason
                   every time the task check status changes, together with a
                   human-readable description of the change in
                   the <code> message </code> field.
</td></tr>
<tr><td><code>     REASON_TASK_HEALTH_CHECK_STATUS_UPDATED
</code></td><td>   For executors that support running task health checks, it
                   is recommended to generate a status update with this reason
                   every time the health check status changes, together with a
                   human-readable description of the change in
                   the <code> message </code> field.
<strong>           Note:
</strong>          The built-in executors additionally send an update with
                   this reason every time a health check is unhealthy.
</td></tr>
<tr><td><code>     REASON_TASK_INVALID
</code></td><td>   For executors that implement their own task validation
                   logic, this reason can be used when the validation check
                   fails, together with a human-readable description of the
                   failed check in the <code> message </code> field.
</td></tr>
<tr><td><code>     REASON_TASK_UNAUTHORIZED
</code></td><td>   For executors that implement their own authorization logic,
                   this reason can be used when authorization fails, together
                   with a human-readable description in
                   the <code> message </code> field.
</td></tr></table>
<h1 id="reference-of-reasons-currently-used-in-mesos"><a class="header" href="#reference-of-reasons-currently-used-in-mesos">Reference of Reasons Currently Used in Mesos</a></h1>
<h2 id="deprecated-reasons"><a class="header" href="#deprecated-reasons">Deprecated Reasons</a></h2>
<p>The reason <code>REASON_COMMAND_EXECUTOR_FAILED</code> is deprecated and will be removed
in the future. It should not be referenced by newly written code.</p>
<h2 id="unused-reasons"><a class="header" href="#unused-reasons">Unused Reasons</a></h2>
<p>The reasons <code>REASON_CONTAINER_LIMITATION</code>, <code>REASON_INVALID_FRAMEWORKID</code>,
<code>REASON_SLAVE_UNKNOWN</code>, <code>REASON_TASK_UNKNOWN</code> and
<code>REASON_EXECUTOR_UNREGISTERED</code> are not used as of Mesos 1.4.</p>
<h2 id="reasons-for-terminal-status-updates"><a class="header" href="#reasons-for-terminal-status-updates">Reasons for Terminal Status Updates</a></h2>
<p>For these status updates, the reason indicates <em>why</em> the task state changed.
Typically, a given reason will always appear together
with the same state.</p>
<p>Typically they are generated by mesos when an error occurs that prevents
the executor from sending its own status update messages.</p>
<p>Below, a partition-aware framework means a framework which has the
<code>Capability::PARTITION_AWARE</code> capability bit set in its <code>FrameworkInfo</code>.
Messages generated on the master will have the <code>source</code> field set to
<code>SOURCE_MASTER</code> and messages generated on the agent will have it set
to <code>SOURCE_AGENT</code> in the v1 API or <code>SOURCE_SLAVE</code> in the v0 API.</p>
<p>As of Mesos 1.4, the following reasons are being used.</p>
<h3 id="for-state-task_failed"><a class="header" href="#for-state-task_failed">For state <code>TASK_FAILED</code></a></h3>
<h4 id="in-status-updates-generated-on-the-agent"><a class="header" href="#in-status-updates-generated-on-the-agent">In status updates generated on the agent:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_CONTAINER_LAUNCH_FAILED
</code></td><td>   The task could not be launched because its container failed
                   to launch.
</td></tr>
<tr><td><code>     REASON_CONTAINER_LIMITATION_MEMORY
</code></td><td>   The container in which the task was running exceeded its
                   memory allocation.
</td></tr>
<tr><td><code>     REASON_CONTAINER_LIMITATION_DISK
</code></td><td>   The container in which the task was running exceeded its
                   disk quota.
</td></tr>
<tr> <td><code>    REASON_IO_SWITCHBOARD_EXITED
</code></td><td>   The I/O switchboard server terminated unexpectedly.
</td></tr>
<tr><td><code>     REASON_EXECUTOR_REGISTRATION_TIMEOUT
</code></td><td>   The executor for this task didn't register with the agent
                   within the allowed time limit.
</td></tr>
<tr><td><code>     REASON_EXECUTOR_REREGISTRATION_TIMEOUT
</code></td><td>   The executor for this task lost connection and didn't
                   reregister within the allowed time limit.
</td></tr>
<tr><td><code>     REASON_EXECUTOR_TERMINATED
</code></td><td>   The tasks' executor terminated abnormally, and no more
                   specific reason could be determined.
</td></tr></table>
<h3 id="for-state-task_killed"><a class="header" href="#for-state-task_killed">For state <code>TASK_KILLED</code></a></h3>
<h4 id="in-status-updates-generated-on-the-master"><a class="header" href="#in-status-updates-generated-on-the-master">In status updates generated on the master:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_FRAMEWORK_REMOVED
</code></td><td>   The framework to which this task belonged was removed.
<br/><strong>      Note:
</strong>          The status update will be sent out before the task is
                   actually killed.
</td></tr>
<tr><td><code>     REASON_TASK_KILLED_DURING_LAUNCH
</code></td><td>   This task, or a task within this task group, was killed
                   before delivery to the agent.
</td></tr></table>
<h4 id="in-status-updates-generated-on-the-agent-1"><a class="header" href="#in-status-updates-generated-on-the-agent-1">In status updates generated on the agent:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_TASK_KILLED_DURING_LAUNCH
</code></td><td>   This task, or a task within this task group, was killed
                   before delivery to the executor.
<br/><strong>      Note:
</strong>          Prior to version 1.5, the agent would in this situation
                   sometimes send status updates with reason set
                   to <code> REASON_EXECUTOR_UNREGISTERED </code> and
                   sometimes without any reason set, depending on details of
                   the timing of the executor launch and the kill command.
</td></tr></table>
<h3 id="for-state-task_error"><a class="header" href="#for-state-task_error">For state <code>TASK_ERROR</code></a></h3>
<h4 id="in-status-updates-generated-on-the-master-1"><a class="header" href="#in-status-updates-generated-on-the-master-1">In status updates generated on the master:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_TASK_INVALID
</code></td><td>   Task or resource validation checks failed.
</td></tr>
<tr><td><code>     REASON_TASK_GROUP_INVALID
</code></td><td>   Task group or resource validation checks failed.
</td></tr>
<tr><td><code>     REASON_TASK_UNAUTHORIZED
</code></td><td>   Task authorization failed on the master.
</td></tr>
<tr><td><code>     REASON_TASK_GROUP_UNAUTHORIZED
</code></td><td>   Task group authorization failed on the master.
</td></tr></table>
<h4 id="in-status-updates-generated-on-the-agent-2"><a class="header" href="#in-status-updates-generated-on-the-agent-2">In status updates generated on the agent:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_TASK_UNAUTHORIZED
</code></td><td>   Task authorization failed on the agent.
</td></tr>
<tr><td><code>     REASON_TASK_GROUP_UNAUTHORIZED
</code></td><td>   Task group authorization failed on the agent.
</td></tr></table>
<h3 id="for-state-task_lost"><a class="header" href="#for-state-task_lost">For state <code>TASK_LOST</code></a></h3>
<h4 id="in-status-updates-generated-on-the-master-2"><a class="header" href="#in-status-updates-generated-on-the-master-2">In status updates generated on the master:</a></h4>
<table class="table table-striped">
<tr><td rowspan="2">
<code>             REASON_SLAVE_DISCONNECTED
</code></td><td>   The agent on which the task was running disconnected, and
                   didn't reconnect in time.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead
</td></tr>
<tr><td>           The task was part of an accepted offer, but the agent
                   sending the offer disconnected in the meantime.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
</td></tr>
<tr><td><code>     REASON_MASTER_DISCONNECTED
</code></td><td>   The task was part of an accepted offer which couldn't be
                   sent to the master, because it was disconnected.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
<br/><strong>      Note:
</strong>          Despite the source being set to <code> SOURCE_MASTER </code>,
                   the message is not sent from the master but locally from
                   the scheduler driver.
<strong>           Note:
</strong>          This reason is only used in the v0 API.
</td></tr>
<tr><td rowspan="3">
<code>             REASON_SLAVE_REMOVED
</code></td><td>   The agent on which the task was running was removed.
</td></tr>
<tr><td>           The task was part of an accepted offer, but the agent
                   sending the offer was disconnected in the meantime.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will be
                   to <code> TASK_DROPPED </code> instead.
</td></tr>
<tr><td>           The agent on which the task was running was marked
                   unreachable.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_UNREACHABLE </code> instead.
</td></tr>
<tr><td><code>     REASON_RESOURCES_UNKNOWN
</code></td><td>   The task was part of an accepted offer which used
                   checkpointed resources that are not known to the master.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
</td></tr></table>
<h4 id="in-status-updates-generated-on-the-agent-3"><a class="header" href="#in-status-updates-generated-on-the-agent-3">In status updates generated on the agent:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_SLAVE_RESTARTED
</code></td><td>   The task was launched during an agent restart, and never
                   got forwarded to the executor.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
</td></tr>
<tr><td><code>     REASON_CONTAINER_PREEMPTED
</code></td><td>   The container in which the task was running was pre-empted
                   by a QoS correction.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will be changed
                   to <code> TASK_GONE </code> instead.
</td></tr>
<tr><td><code>     REASON_CONTAINER_UPDATE_FAILED
</code></td><td>   The container in which the task was running was discarded
                   because a resource update failed.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_GONE </code> instead.
</td></tr>
<tr><td><code>     REASON_EXECUTOR_TERMINATED
</code></td><td>   The executor which was supposed to execute this task was
                   already terminated, or the agent receives an instruction to
                   kill the task before the executor was started.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
</td></tr>
<tr><td><code>     REASON_GC_ERROR
</code></td><td>   A directory to be used by this task was scheduled for GC
                   and it could not be unscheduled.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
</td></tr>
<tr><td><code>     REASON_INVALID_OFFERS
</code></td><td>   This task belonged to an accepted offer that didn't pass
                   validation checks.
<br/><strong>      Note:
</strong>          For partition-aware frameworks, the state will
                   be <code> TASK_DROPPED </code> instead.
</td></tr></table>
<h3 id="for-state-task_dropped"><a class="header" href="#for-state-task_dropped">For state <code>TASK_DROPPED</code>:</a></h3>
<h4 id="in-status-updates-generated-on-the-master-3"><a class="header" href="#in-status-updates-generated-on-the-master-3">In status updates generated on the master:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_SLAVE_DISCONNECTED
</code></td><td>   See <code> TASK_LOST </code>
</td></tr>
<tr><td><code>     REASON_SLAVE_REMOVED
</code></td><td>   See <code> TASK_LOST </code>
</td></tr>
<tr><td><code>     REASON_RESOURCES_UNKNOWN
</code></td><td>   See <code> TASK_LOST </code>
</td></tr></table>
<h4 id="in-status-updates-generated-on-the-agent-4"><a class="header" href="#in-status-updates-generated-on-the-agent-4">In status updates generated on the agent:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_SLAVE_RESTARTED
</code></td><td>   See <code> TASK_LOST </code>
</td></tr>
<tr><td><code>     REASON_GC_ERROR
</code></td><td>   See <code> TASK_LOST </code>
</td></tr>
<tr><td><code>     REASON_INVALID_OFFERS
</code></td><td>   See <code> TASK_LOST </code>
</td></tr></table>
<h3 id="for-state-task_unreachable"><a class="header" href="#for-state-task_unreachable">For state <code>TASK_UNREACHABLE</code>:</a></h3>
<h4 id="in-status-updates-generated-on-the-master-4"><a class="header" href="#in-status-updates-generated-on-the-master-4">In status updates generated on the master:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_SLAVE_REMOVED
</code></td><td>   See <code> TASK_LOST <code>
</td></tr></table>
<h3 id="for-state-task_gone"><a class="header" href="#for-state-task_gone">For state <code>TASK_GONE</code></a></h3>
<h4 id="in-status-updates-generated-on-the-agent-5"><a class="header" href="#in-status-updates-generated-on-the-agent-5">In status updates generated on the agent:</a></h4>
<table class="table table-striped">
<tr><td><code>     REASON_CONTAINER_UPDATE_FAILED
</code></td><td>   See <code> TASK_LOST </code>
</td></tr>
<tr><td><code>     REASON_CONTAINER_PREEMPTED
</code></td><td>   See <code> TASK_LOST </code>
</td></tr>
<tr><td><code>     REASON_EXECUTOR_PREEMPTED
</code></td><td>   Renamed to <code> REASON_CONTAINER_PREEMPTED </code> in
                   Mesos 0.26.
</td></tr></table>
<h2 id="reasons-for-non-terminal-status-updates"><a class="header" href="#reasons-for-non-terminal-status-updates">Reasons for Non-Terminal Status Updates</a></h2>
<p>These reasons do not cause a state change, and will be sent along with the
last known state of the task. The reason field indicates <em>why</em> the status
update was sent.</p>
<table class="table table-striped">
<tr><td><code>     REASON_RECONCILIATION
</code></td><td>   A framework requested implicit or explicit reconciliation
                   for this task.
<br/><strong>      Note:
</strong>          Status updates with this reason are not the original ones,
                   but rather a modified copy that is re-sent from the master.
                   In particular, the original <code> data </code>
                   and <code> message </code> fields are erased and the
                   original <code> reason </code> field is overwritten
                   by <code> REASON_RECONCILIATION </code>.
</td></tr>
<tr><td><code>     REASON_TASK_CHECK_STATUS_UPDATED
</code></td><td>   A task check notified the agent that its state changed.
<br/><strong>      Note:
</strong>          This reason is set by the executor, so for tasks that are
                   running with a custom executor, whether or not status
                   updates with this reasons are sent depends on that
                   executors implementation.
<strong>           Note:
</strong>          Currently, when using one of the built-in executors, this
                   reason is only used within status updates with task
                   state <code> TASK_RUNNING </code>.
</td></tr>
<tr><td><code>     REASON_TASK_HEALTH_CHECK_STATUS_UPDATED
</code></td><td>   A task health check notified the agent that its
                   state changed.
<br/><strong>      Note:
</strong>          This reason is set by the executor, so for tasks that are
                   running with a custom executor, whether or not status
                   updates with this reasons are sent depends on that
                   executors implementation.
<strong>           Note:
</strong>          Currently, when using one of the built-in executors, this
                   reason is only used within status updates with task
                   state <code> TASK_RUNNING </code>.
</td></tr>
<tr><td><code>     REASON_SLAVE_REREGISTERED
</code></td><td>   The agent on which the task was running has reregistered
                   after being marked unreachable by the master.
<br/><strong>      Note:
</strong>          Due to garbage collection of the unreachable and gone agents
                   in the registry and master state Mesos also sends such status
                   updates for agents unknown to the master.
<strong>           Note:
</strong>          Status updates with this reason are modified copies re-sent
                   by the master which reflect the states of the tasks reported
                   by the agent upon its re-registration. See comments for
                   <code> REASON_RECONCILIATION </code>.
</td></tr></table>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Task Health Checking and Generalized Checks
layout: documentation</h2>
<h1 id="task-health-checking-and-generalized-checks"><a class="header" href="#task-health-checking-and-generalized-checks">Task Health Checking and Generalized Checks</a></h1>
<p>Sometimes applications crash, misbehave, or become unresponsive. To detect and
recover from such situations, some frameworks (e.g.,
<a href="https://github.com/mesosphere/marathon/blob/v1.3.6/docs/docs/health-checks.md">Marathon</a>,
<a href="https://aurora.apache.org/documentation/0.8.0/user-guide/#http-health-checking-and-graceful-shutdown">Apache Aurora</a>)
implement their own logic for checking the health of their tasks. This is
typically done by having the framework scheduler send a &quot;ping&quot; request, e.g.,
via HTTP, to the host where the task is running and arranging for the task or
executor to respond to the ping. Although this technique is extremely useful,
there are several disadvantages in the way it is usually implemented:</p>
<ul>
<li>Each Apache Mesos framework uses its own API and protocol.</li>
<li>Framework developers have to reimplement common functionality.</li>
<li>Health checks originating from a scheduler generate extra network traffic if
the task and the scheduler run on different nodes (which is usually the case);
moreover, network failures between the task and the scheduler may make the
latter think that the former is unhealthy, which might not be the case.</li>
<li>Implementing health checks in the framework scheduler can be a performance
bottleneck. If a framework is managing a large number of tasks, performing
health checks for every task can cause scheduler performance problems.</li>
</ul>
<p>To address the aforementioned problems, Mesos 1.2.0 introduced
<a href="health-checks.html#mesos-native-checking">the Mesos-native health check design</a>, defined
common API for <a href="health-checks.html#command-health-checks">command</a>,
<a href="health-checks.html#http-health-checks">HTTP(S)</a>, and <a href="health-checks.html#tcp-health-checks">TCP</a> health checks,
and provided reference implementations for all built-in executors.</p>
<p>Mesos 1.4.0 introduced <a href="health-checks.html#anatomy-of-a-check">a generalized check</a>, which
delegates interpretation of a check result to the framework. This might be
useful, for instance, to track tasks' internal state transitions reliably
without Mesos taking action on them.</p>
<p><strong>NOTE:</strong> Some functionality related to health checking was available prior to
1.2.0 release, however it was considered experimental.</p>
<p><strong>NOTE:</strong> Mesos monitors each process-based task, including Docker containers,
using an equivalent of a <code>waitpid()</code> system call. This technique allows
detecting and reporting process crashes, but is insufficient for cases when the
process is still running but is not responsive.</p>
<p>This document describes supported check and health check types, touches on
relevant implementation details, and mentions limitations and caveats.</p>
<p><a name="mesos-native-checking"></a></p>
<h2 id="mesos-native-task-checking"><a class="header" href="#mesos-native-task-checking">Mesos-native Task Checking</a></h2>
<p>In contrast to the state-of-the-art &quot;scheduler health check&quot; pattern mentioned
above, Mesos-native checks run on the agent node: it is the executor
which performs checks and not the scheduler. This improves scalability but means
that detecting network faults or task availability from the outside world
becomes a separate concern. For instance, if the task is running on a
partitioned agent, it will still be (health) checked and---if the health checks
fail---might be terminated. Needless to say that due to the network partition,
all this will happen without the framework scheduler being notified.</p>
<p>Mesos checks and health checks are described in
<a href="https://github.com/apache/mesos/blob/cdb90b91ce8ce02d6163e5e2ee5b46fb797b1dee/include/mesos/mesos.proto#L403-L485"><code>CheckInfo</code></a>
and <a href="https://github.com/apache/mesos/blob/cdb90b91ce8ce02d6163e5e2ee5b46fb797b1dee/include/mesos/mesos.proto#L488-L589"><code>HealthCheck</code></a>
protobufs respectively. Currently, only tasks can be (health) checked, not
arbitrary processes or executors, i.e., only the <code>TaskInfo</code> protobuf has the
optional <code>CheckInfo</code> and <code>HealthCheck</code> fields. However, it is worth noting that
all built-in executors map a task to a process.</p>
<p>Task status updates are leveraged to transfer the check and health check status
to the Mesos master and further to the framework's scheduler ensuring the
&quot;at-least-once&quot; delivery guarantee. To minimize performance overhead, those task
status updates are triggered if a certain condition is met, e.g., the value or
presence of a specific field in the check status changes.</p>
<p>When a built-in executor sends a task status update because the check or health
check status has changed, it sets <code>TaskStatus.reason</code> to
<code>REASON_TASK_CHECK_STATUS_UPDATED</code> or <code>REASON_TASK_HEALTH_CHECK_STATUS_UPDATED</code>
respectively. While sending such an update, the executor avoids shadowing other
data that might have been injected previously, e.g., a check update includes the
last known update from a health check.</p>
<p>It is the responsibility of the executor to interpret <code>CheckInfo</code> and
<code>HealthCheckInfo</code> and perform checks appropriately. All built-in executors
support health checking their tasks and all except the docker executor support
generalized checks (see <a href="health-checks.html#under-the-hood">implementation details</a> and
<a href="health-checks.html#current-limitations">limitations</a>).</p>
<p><strong>NOTE:</strong> It is up to the executor how---and whether at all---to honor the
<code>CheckInfo</code> and <code>HealthCheck</code> fields in <code>TaskInfo</code>. Implementations may vary
significantly depending on what entity <code>TaskInfo</code> represents. On this page only
the reference implementation for built-in executors is considered.</p>
<p>Custom executors can use <a href="health-checks.html#under-the-hood">the checker library</a>, the reference
implementation for health checking that all built-in executors rely on.</p>
<h3 id="on-the-differences-between-checks-and-health-checks"><a class="header" href="#on-the-differences-between-checks-and-health-checks">On the Differences Between Checks and Health Checks</a></h3>
<p>When humans read data from a sensor, they may interpret these data and act on
them. For example, if they check air temperature, they usually interpret
temperature readings and say whether it's cold or warm outside; they may also
act on the interpretation and decide to apply sunscreen or put on an extra
jacket.</p>
<p>Similar reasoning can be applied to checking task's state in Mesos:</p>
<ol>
<li>Perform a check.</li>
<li>Optionally interpret the result and, for example, declare the task either
healthy or unhealthy.</li>
<li>Optionally act on the interpretation by killing an unhealthy task.</li>
</ol>
<p>Mesos health checks do all of the above, 1+2+3: they run the check, declare the
task healthy or not, and kill it after <code>consecutive_failures</code> have occurred.
Though efficient and scalable, this strategy is inflexible for the needs of
frameworks which may want to run an arbitrary check without Mesos interpreting
the result in any way, for example, to transmit the task's internal state
transitions and make global decisions.</p>
<p>Conceptually, a health check is a check with an interpretation and a kill
policy. A check and a health check differ in how they are specified and
implemented:</p>
<ul>
<li>Built-in executors do not (and custom executors shall not) interpret the
result of a check. If they do, it should be a health check.</li>
<li>There is no concept of a check failure, hence grace period and consecutive
failures options are only available for health checks. Note that a check can
still time out (a health check interprets timeouts as failures), in this case
an empty result is sent to the scheduler.</li>
<li>Health checks do not propagate the result of the underlying check to the
scheduler, only its interpretation: healthy or unhealthy. Note that this may
change in the future.</li>
<li>Health check updates are deduplicated based on the interpretation and not the
result of the underlying check, i.e., given that only HTTP <code>4**</code> status codes
are considered failures, if the first HTTP check returns <code>200</code> and the second
<code>202</code>, only one status update after the first success is sent, while a check
would generate two status updates in this case.</li>
</ul>
<p><strong>NOTE:</strong> Docker executor currently supports health checks but not checks.</p>
<p><strong>NOTE:</strong> Slight changes in protobuf message naming and structure are due to
backward compatibility reasons; in the future the <code>HealthCheck</code> message will be
based on <code>CheckInfo</code>.</p>
<p><a name="anatomy-of-a-check"></a></p>
<h2 id="anatomy-of-a-check"><a class="header" href="#anatomy-of-a-check">Anatomy of a Check</a></h2>
<p>A <code>CheckStatusInfo</code> message is added to the task status update to convey the
check status. Currently, check status info is only added for <code>TASK_RUNNING</code>
status updates.</p>
<p>Built-in executors leverage task status updates to deliver check updates to the
scheduler. To minimize performance overhead, a check-related task status update
is triggered if and only if the value or presence of any field in
<code>CheckStatusInfo</code> changes. As the <code>CheckStatusInfo</code> message matures, in the
future we might deduplicate only on specific fields in <code>CheckStatusInfo</code> to make
sure that as few updates as possible are sent. Note that custom executors may
use a different strategy.</p>
<p>To support third party tooling that might not have access to the original
<code>TaskInfo</code> specification, <code>TaskStatus.check_status</code> generated by built-in
executors adheres to the following conventions:</p>
<ul>
<li>If the original <code>TaskInfo</code> has not specified a check,
<code>TaskStatus.check_status</code> is not present.</li>
<li>If the check has been specified, <code>TaskStatus.check_status.type</code> indicates the
check's type.</li>
<li>If the check result is not available for some reason (a check has not run yet
or a check has timed out), the corresponding result is empty, e.g.,
<code>TaskStatus.check_status.command</code> is present and empty.</li>
</ul>
<p><strong>NOTE:</strong> Frameworks that use custom executors are highly advised to follow the
same principles built-in executors use for consistency.</p>
<p><a name="command-checks"></a></p>
<h3 id="command-checks"><a class="header" href="#command-checks">Command Checks</a></h3>
<p>Command checks are described by the <code>CommandInfo</code> protobuf wrapped in the
<code>CheckInfo.Command</code> message; some fields are ignored though: <code>CommandInfo.user</code>
and <code>CommandInfo.uris</code>. A command check specifies an arbitrary command that is
used to check a particular condition of the task. The result of the check is the
exit code of the command.</p>
<p><strong>NOTE:</strong> Docker executor does not currently support checks. For all other
tasks, including Docker containers launched in the
<a href="mesos-containerizer.html">mesos containerizer</a>, the command will be executed from
the task's mount namespace.</p>
<p>To specify a command check, set <code>type</code> to <code>CheckInfo::COMMAND</code> and populate
<code>CheckInfo.Command.CommandInfo</code>, for example:</p>
<pre><code class="language-{.cpp}">TaskInfo task = [...];

CheckInfo check;
check.set_type(CheckInfo::COMMAND);
check.mutable_command()-&gt;mutable_command()-&gt;set_value(
    &quot;ls /checkfile &gt; /dev/null&quot;);

task.mutable_check()-&gt;CopyFrom(check);
</code></pre>
<p><a name="http-checks"></a></p>
<h3 id="http-checks"><a class="header" href="#http-checks">HTTP Checks</a></h3>
<p>HTTP checks are described by the <code>CheckInfo.Http</code> protobuf with <code>port</code> and
<code>path</code> fields. A <code>GET</code> request is sent to <code>http://&lt;host&gt;:port/path</code> using the
<code>curl</code> command. Note that <code>&lt;host&gt;</code> is currently not configurable and is set
automatically to <code>127.0.0.1</code> (see <a href="health-checks.html#current-limitations">limitations</a>), hence
the checked task must listen on the loopback interface along with any other
routeable interface it might be listening on. Field <code>port</code> must specify an
actual port the task is listening on, not a mapped one. The result of the check
is the HTTP status code of the response.</p>
<p>Built-in executors follow HTTP <code>3xx</code> redirects; custom executors may employ a
different strategy.</p>
<p>If necessary, executors enter the task's network namespace prior to launching
the <code>curl</code> command.</p>
<p><strong>NOTE:</strong> HTTPS checks are currently not supported.</p>
<p>To specify an HTTP check, set <code>type</code> to <code>CheckInfo::HTTP</code> and populate
<code>CheckInfo.Http</code>, for example:</p>
<pre><code class="language-{.cpp}">TaskInfo task = [...];

CheckInfo check;
check.set_type(CheckInfo::HTTP);
check.mutable_http()-&gt;set_port(8080);
check.mutable_http()-&gt;set_path(&quot;/health&quot;);

task.mutable_check()-&gt;CopyFrom(check);
</code></pre>
<p><a name="tcp-checks"></a></p>
<h3 id="tcp-checks"><a class="header" href="#tcp-checks">TCP Checks</a></h3>
<p>TCP checks are described by the <code>CheckInfo.Tcp</code> protobuf, which has a single
<code>port</code> field, which must specify an actual port the task is listening on, not a
mapped one. The task is probed using Mesos' <code>mesos-tcp-connect</code> command, which
tries to establish a TCP connection to <code>&lt;host&gt;:port</code>. Note that <code>&lt;host&gt;</code> is
currently not configurable and is set automatically to <code>127.0.0.1</code>
(see <a href="health-checks.html#current-limitations">limitations</a>), hence the checked task must listen on
the loopback interface along with any other routeable interface it might be
listening on. Field <code>port</code> must specify an actual port the task is listening on,
not a mapped one. The result of the check is the boolean value indicating
whether a TCP connection succeeded.</p>
<p>If necessary, executors enter the task's network namespace prior to launching
the <code>mesos-tcp-connect</code> command.</p>
<p>To specify a TCP check, set <code>type</code> to <code>CheckInfo::TCP</code> and populate
<code>CheckInfo.Tcp</code>, for example:</p>
<pre><code class="language-{.cpp}">TaskInfo task = [...];

CheckInfo check;
check.set_type(CheckInfo::TCP);
check.mutable_tcp()-&gt;set_port(8080);

task.mutable_check()-&gt;CopyFrom(check);
</code></pre>
<h3 id="common-options"><a class="header" href="#common-options">Common options</a></h3>
<p>The <code>CheckInfo</code> protobuf contains common options which regulate how a check must
be performed by an executor:</p>
<ul>
<li><code>delay_seconds</code> is the amount of time to wait until starting checking the
task.</li>
<li><code>interval_seconds</code> is the interval between check attempts.</li>
<li><code>timeout_seconds</code> is the amount of time to wait for the check to complete.
After this timeout, the check attempt is aborted and empty check update,
i.e., the absence of the check result, is reported.</li>
</ul>
<p><strong>NOTE:</strong> Since each time a check is performed a helper command is launched
(see <a href="health-checks.html#current-limitations">limitations</a>), setting <code>timeout_seconds</code> to a small
value, e.g., <code>&lt;5s</code>, may lead to intermittent failures.</p>
<p><strong>NOTE:</strong> Launching a check is not a free operation. To avoid unpredictable
spikes in agent's load, e.g., when most of the tasks run their checks
simultaneously, avoid setting <code>interval_seconds</code> to zero.</p>
<p>As an example, the code below specifies a task which is a Docker container with
a simple HTTP server listening on port <code>8080</code> and an HTTP check that should be
performed every <code>5</code> seconds starting from the task launch and response time
under <code>1</code> second.</p>
<pre><code class="language-{.cpp}">TaskInfo task = createTask(...);

// Use Netcat to emulate an HTTP server.
const string command =
    &quot;nc -lk -p 8080 -e echo -e \&quot;HTTP/1.1 200 OK\r\nContent-Length: 0\r\n\&quot;&quot;;
task.mutable_command()-&gt;set_value(command)

Image image;
image.set_type(Image::DOCKER);
image.mutable_docker()-&gt;set_name(&quot;alpine&quot;);

ContainerInfo* container = task.mutable_container();
container-&gt;set_type(ContainerInfo::MESOS);
container-&gt;mutable_mesos()-&gt;mutable_image()-&gt;CopyFrom(image);

// Set `delay_seconds` here because it takes
// some time to launch Netcat to serve requests.
CheckInfo check;
check.set_type(CheckInfo::HTTP);
check.mutable_http()-&gt;set_port(8080);
check.set_delay_seconds(15);
check.set_interval_seconds(5);
check.set_timeout_seconds(1);

task.mutable_check()-&gt;CopyFrom(check);
</code></pre>
<h2 id="anatomy-of-a-health-check"><a class="header" href="#anatomy-of-a-health-check">Anatomy of a Health Check</a></h2>
<p>The boolean <code>healthy</code> field is used to convey health status, which
<a href="health-checks.html#current-limitations">may be insufficient</a> in certain cases. This means a task
that has failed health checks will be <code>RUNNING</code> with <code>healthy</code> set to <code>false</code>.
Currently, the <code>healthy</code> field is only set for <code>TASK_RUNNING</code> status updates.</p>
<p>When a task turns unhealthy, a task status update message with the <code>healthy</code>
field set to <code>false</code> is sent to the Mesos master and then forwarded to a
scheduler. The executor is expected to kill the task after a number of
consecutive failures defined in the <code>consecutive_failures</code> field of the
<code>HealthCheck</code> protobuf.</p>
<p><strong>NOTE:</strong> While a scheduler currently cannot cancel a task kill due to failing
health checks, it may issue a <code>killTask</code> command itself. This may be helpful to
emulate a &quot;global&quot; policy for handling tasks with failing health checks (see
<a href="health-checks.html#current-limitations">limitations</a>). Alternatively, the scheduler might use
<a href="health-checks.html#anatomy-of-a-check">generalized checks</a> instead.</p>
<p>Built-in executors forward all unhealthy status updates, as well as the first
healthy update when a task turns healthy, i.e., when the task has started or
after one or more unhealthy updates have occurred. Note that custom executors
may use a different strategy.</p>
<p><a name="command-health-checks"></a></p>
<h3 id="command-health-checks"><a class="header" href="#command-health-checks">Command Health Checks</a></h3>
<p>Command health checks are described by the <code>CommandInfo</code> protobuf; some fields
are ignored though: <code>CommandInfo.user</code> and <code>CommandInfo.uris</code>. A command health
check specifies an arbitrary command that is used to validate the health of the
task. The executor launches the command and inspects its exit status: <code>0</code> is
treated as success, any other status as failure.</p>
<p><strong>NOTE:</strong> If a task is a Docker container launched by the docker executor, it
will be wrapped in <code>docker run</code>. For all other tasks, including Docker
containers launched in the <a href="mesos-containerizer.html">mesos containerizer</a>, the
command will be executed from the task's mount namespace.</p>
<p>To specify a command health check, set <code>type</code> to <code>HealthCheck::COMMAND</code> and
populate <code>CommandInfo</code>, for example:</p>
<pre><code class="language-{.cpp}">TaskInfo task = [...];

HealthCheck healthCheck;
healthCheck.set_type(HealthCheck::COMMAND);
healthCheck.mutable_command()-&gt;set_value(&quot;ls /checkfile &gt; /dev/null&quot;);

task.mutable_health_check()-&gt;CopyFrom(healthCheck);
</code></pre>
<p><a name="http-health-checks"></a></p>
<h3 id="https-health-checks"><a class="header" href="#https-health-checks">HTTP(S) Health Checks</a></h3>
<p>HTTP(S) health checks are described by the <code>HealthCheck.HTTPCheckInfo</code> protobuf
with <code>scheme</code>, <code>port</code>, <code>path</code>, and <code>statuses</code> fields. A <code>GET</code> request is sent to
<code>scheme://&lt;host&gt;:port/path</code> using the <code>curl</code> command. Note that <code>&lt;host&gt;</code> is
currently not configurable and is set automatically to <code>127.0.0.1</code> (see
<a href="health-checks.html#current-limitations">limitations</a>), hence the health checked task must listen
on the loopback interface along with any other routeable interface it might be
listening on. The <code>scheme</code> field supports <code>&quot;http&quot;</code> and <code>&quot;https&quot;</code> values only.
Field <code>port</code> must specify an actual port the task is listening on, not a mapped
one.</p>
<p>Built-in executors follow HTTP <code>3xx</code> redirects and treat status codes between
<code>200</code> and <code>399</code> as success; custom executors may employ a different strategy,
e.g., leveraging the <code>statuses</code> field.</p>
<p><strong>NOTE:</strong> Setting <code>HealthCheck.HTTPCheckInfo.statuses</code> has no effect on the
built-in executors.</p>
<p>If necessary, executors enter the task's network namespace prior to launching
the <code>curl</code> command.</p>
<p>To specify an HTTP health check, set <code>type</code> to <code>HealthCheck::HTTP</code> and populate
<code>HTTPCheckInfo</code>, for example:</p>
<pre><code class="language-{.cpp}">TaskInfo task = [...];

HealthCheck healthCheck;
healthCheck.set_type(HealthCheck::HTTP);
healthCheck.mutable_http()-&gt;set_port(8080);
healthCheck.mutable_http()-&gt;set_scheme(&quot;http&quot;);
healthCheck.mutable_http()-&gt;set_path(&quot;/health&quot;);

task.mutable_health_check()-&gt;CopyFrom(healthCheck);
</code></pre>
<p><a name="tcp-health-checks"></a></p>
<h3 id="tcp-health-checks"><a class="header" href="#tcp-health-checks">TCP Health Checks</a></h3>
<p>TCP health checks are described by the <code>HealthCheck.TCPCheckInfo</code> protobuf,
which has a single <code>port</code> field, which must specify an actual port the task is
listening on, not a mapped one. The task is probed using Mesos'
<code>mesos-tcp-connect</code> command, which tries to establish a TCP connection to
<code>&lt;host&gt;:port</code>. Note that <code>&lt;host&gt;</code> is currently not configurable and is set
automatically to <code>127.0.0.1</code> (see <a href="health-checks.html#current-limitations">limitations</a>), hence
the health checked task must listen on the loopback interface along with any
other routeable interface it might be listening on. Field <code>port</code> must specify an
actual port the task is listening on, not a mapped one.</p>
<p>The health check is considered successful if the connection can be established.</p>
<p>If necessary, executors enter the task's network namespace prior to launching
the <code>mesos-tcp-connect</code> command.</p>
<p>To specify a TCP health check, set <code>type</code> to <code>HealthCheck::TCP</code> and populate
<code>TCPCheckInfo</code>, for example:</p>
<pre><code class="language-{.cpp}">TaskInfo task = [...];

HealthCheck healthCheck;
healthCheck.set_type(HealthCheck::TCP);
healthCheck.mutable_tcp()-&gt;set_port(8080);

task.mutable_health_check()-&gt;CopyFrom(healthCheck);
</code></pre>
<h3 id="common-options-1"><a class="header" href="#common-options-1">Common options</a></h3>
<p>The <code>HealthCheck</code> protobuf contains common options which regulate how a health
check must be performed and interpreted by an executor:</p>
<ul>
<li><code>delay_seconds</code> is the amount of time to wait until starting health checking
the task.</li>
<li><code>interval_seconds</code> is the interval between health checks.</li>
<li><code>timeout_seconds</code> is the amount of time to wait for the health check to
complete. After this timeout, the health check is aborted and treated as a
failure.</li>
<li><code>consecutive_failures</code> is the number of consecutive failures until the task is
killed by the executor.</li>
<li><code>grace_period_seconds</code> is the amount of time after the task is launched during
which health check failures are ignored. Once a health check succeeds for the
first time, the grace period does not apply anymore. Note that it includes
<code>delay_seconds</code>, i.e., setting <code>grace_period_seconds</code> &lt; <code>delay_seconds</code> has
no effect.</li>
</ul>
<p><strong>NOTE:</strong> Since each time a health check is performed a helper command is
launched (see <a href="health-checks.html#current-limitations">limitations</a>), setting <code>timeout_seconds</code>
to a small value, e.g., <code>&lt;5s</code>, may lead to intermittent failures.</p>
<p>As an example, the code below specifies a task which is a Docker container with
a simple HTTP server listening on port <code>8080</code> and an HTTP health check that
should be performed every <code>5</code> seconds starting from the task launch and allows
consecutive failures during the first <code>15</code> seconds and response time under <code>1</code>
second.</p>
<pre><code class="language-{.cpp}">TaskInfo task = createTask(...);

// Use Netcat to emulate an HTTP server.
const string command =
    &quot;nc -lk -p 8080 -e echo -e \&quot;HTTP/1.1 200 OK\r\nContent-Length: 0\r\n\&quot;&quot;;
task.mutable_command()-&gt;set_value(command)

Image image;
image.set_type(Image::DOCKER);
image.mutable_docker()-&gt;set_name(&quot;alpine&quot;);

ContainerInfo* container = task.mutable_container();
container-&gt;set_type(ContainerInfo::MESOS);
container-&gt;mutable_mesos()-&gt;mutable_image()-&gt;CopyFrom(image);

// Set `grace_period_seconds` here because it takes
// some time to launch Netcat to serve requests.
HealthCheck healthCheck;
healthCheck.set_type(HealthCheck::HTTP);
healthCheck.mutable_http()-&gt;set_port(8080);
healthCheck.set_delay_seconds(0);
healthCheck.set_interval_seconds(5);
healthCheck.set_timeout_seconds(1);
healthCheck.set_grace_period_seconds(15);

task.mutable_health_check()-&gt;CopyFrom(healthCheck);
</code></pre>
<p><a name="under-the-hood"></a></p>
<h2 id="under-the-hood"><a class="header" href="#under-the-hood">Under the Hood</a></h2>
<p>All built-in executors rely on the checker library, which lives in
<a href="https://github.com/apache/mesos/tree/master/src/checks">&quot;src/checks&quot;</a>.
An executor creates an instance of the <code>Checker</code> or <code>HealthChecker</code> class per
task and passes the check or health check definition together with extra
parameters. In return, the library notifies the executor of changes in the
task's check or health status. For health checks, the definition is converted
to the check definition before performing the check, and the check result is
interpreted according to the health check definition.</p>
<p>The library depends on <code>curl</code> for HTTP(S) checks and <code>mesos-tcp-connect</code> for
TCP checks (the latter is a simple command bundled with Mesos).</p>
<p>One of the most non-trivial things the library takes care of is entering the
appropriate task's namespaces (<code>mnt</code>, <code>net</code>) on Linux agents. To perform a
command check, the checker must be in the same mount namespace as the checked
process; this is achieved by either calling <code>docker run</code> for the check command
in case of <a href="docker-containerizer.html">docker containerizer</a> or by explicitly
calling <code>setns()</code> for <code>mnt</code> namespace in case of <a href="mesos-containerizer.html">mesos containerizer</a>
(see <a href="containerizers.html">containerization in Mesos</a>). To perform an HTTP(S) or
TCP check, the most reliable solution is to share the same network namespace
with the checked process; in case of docker containerizer <code>setns()</code> for <code>net</code>
namespace is explicitly called, while mesos containerizer guarantees an executor
and its tasks are in the same network namespace.</p>
<p><strong>NOTE:</strong> Custom executors may or may not use this library. Please consult the
respective framework's documentation.</p>
<p>Regardless of executor, all checks and health checks consume resources from the
task's resource allocation. Hence it is a good idea to add some extra resources,
e.g., 0.05 cpu and 32MB mem, to the task definition if a Mesos-native check
and/or health check is specified.</p>
<p><strong>Windows Implementation</strong></p>
<p>On Windows, the implementation differs between the <a href="mesos-containerizer.html">mesos containerizer</a>
and <a href="docker-containerizer.html">docker containerizer</a>. The
<a href="mesos-containerizer.html">mesos containerizer</a> does not provide network or mount
namespace isolation, so <code>curl</code>, <code>mesos-tcp-connect</code> or the command health check
simply run as regular processes on the host. In constrast, the
<a href="docker-containerizer.html">docker containerizer</a> provides network and mount
isolation. For the command health check, the command enters the container's
namespace through <code>docker exec</code>. For the network health checks, the docker
executor launches a container with the <a href="https://hub.docker.com/r/mesos/windows-health-check"><code>mesos/windows-health-check</code></a>
image and enters the original container's network namespace through the
<code>--network=container:&lt;ID&gt;</code> parameter in <code>docker run</code>.</p>
<p><a name="current-limitations"></a></p>
<h2 id="current-limitations-and-caveats"><a class="header" href="#current-limitations-and-caveats">Current Limitations and Caveats</a></h2>
<ul>
<li>Docker executor does not support generalized checks (see
<a href="https://issues.apache.org/jira/browse/MESOS-7250">MESOS-7250</a>).</li>
<li>HTTPS checks are not supported, though HTTPS health checks are (see
<a href="https://issues.apache.org/jira/browse/MESOS-7356">MESOS-7356</a>).</li>
<li>Due to the short-polling nature of a check, some task state transitions may be
missed. For example, if the task transitions are <code>Init [111]</code> →
<code>Join [418]</code> → <code>Ready [200]</code>, the observed HTTP status codes in check
statuses may be <code>111</code> → <code>200</code>.</li>
<li>Due to its short-polling nature, a check whose state oscillates repeatedly
may lead to scalability issues due to a high volume of task status updates.</li>
<li>When a task becomes unhealthy, it is deemed to be killed after
<code>HealthCheck.consecutive_failures</code> failures. This decision is taken locally by
an executor, there is no way for a scheduler to intervene and react
differently. A workaround is to set <code>HealthCheck.consecutive_failures</code> to some
large value so that the scheduler can react. One possible solution is to
introduce a &quot;global&quot; policy for handling unhealthy tasks (see
<a href="https://issues.apache.org/jira/browse/MESOS-6171">MESOS-6171</a>).</li>
<li>HTTP(S) and TCP health checks use <code>127.0.0.1</code> as target IP. As a result, if
tasks want to support HTTP or TCP health checks, they should listen on the
loopback interface in addition to whatever interface they require (see
<a href="https://issues.apache.org/jira/browse/MESOS-6517">MESOS-6517</a>).</li>
<li>HTTP(S) health checks rely on the <code>curl</code> command. A health check is
considered failed if the required command is not available.</li>
<li>Windows HTTP(S) and TCP Docker health checks should ideally have the
<code>mesos/windows-health-check</code> image pulled beforehand. Otherwise, Docker will
attempt to pull the image during the health check, which will count towards
the health check timeout.</li>
<li>Only a single health check per task is allowed (see
<a href="https://issues.apache.org/jira/browse/MESOS-5962">MESOS-5962</a>).</li>
<li>Each time a health check runs, <a href="health-checks.html#under-the-hood">a helper command is launched</a>.
This introduces some run-time overhead (see
<a href="https://issues.apache.org/jira/browse/MESOS-6766">MESOS-6766</a>).</li>
<li>A task without a health check may be indistinguishable from a task with a
health check but still in a grace period. An extra state should be introduced
(see <a href="https://issues.apache.org/jira/browse/MESOS-6417">MESOS-6417</a>).</li>
<li>Task's health status cannot be assigned from outside, e.g., by an operator via
an endpoint.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Scheduler HTTP API
layout: documentation</h2>
<h1 id="scheduler-http-api"><a class="header" href="#scheduler-http-api">Scheduler HTTP API</a></h1>
<p>A Mesos scheduler can be built in two different ways:</p>
<ol>
<li>
<p>By using the <code>SchedulerDriver</code> C++ interface. The <code>SchedulerDriver</code> handles
the details of communicating with the Mesos master. Scheduler developers
implement custom scheduling logic by registering callbacks with the
<code>SchedulerDriver</code> for significant events, such as receiving a new resource offer
or a status update on a task. Because the <code>SchedulerDriver</code> interface is written
in C++, this typically requires that scheduler developers either use C++ or use
a C++ binding to their language of choice (e.g., JNI when using JVM-based
languages).</p>
</li>
<li>
<p>By using the new HTTP API. This allows Mesos schedulers to be developed
without using C++ or a native client library; instead, a custom scheduler
interacts with the Mesos master via HTTP requests, as described below. Although
it is theoretically possible to use the HTTP scheduler API &quot;directly&quot; (e.g., by
using a generic HTTP library), most scheduler developers should use a library for
their language of choice that manages the details of the HTTP API; see the
document on <a href="api-client-libraries.html">HTTP API client libraries</a> for a list.</p>
</li>
</ol>
<p>The v1 Scheduler HTTP API was introduced in Mesos 0.24.0. As of Mesos 1.0, it is
considered stable and is the recommended way to develop new Mesos schedulers.</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>The scheduler interacts with Mesos via the <a href="endpoints/master/api/v1/scheduler.html">/api/v1/scheduler</a> master endpoint. We refer to this endpoint with its suffix &quot;/scheduler&quot; in the rest of this document. This endpoint accepts HTTP POST requests with data encoded as JSON (Content-Type: application/json) or binary Protobuf (Content-Type: application/x-protobuf). The first request that a scheduler sends to &quot;/scheduler&quot; endpoint is called SUBSCRIBE and results in a streaming response (&quot;200 OK&quot; status code with Transfer-Encoding: chunked).</p>
<p><strong>Schedulers are expected to keep the subscription connection open as long as possible (barring errors in network, software, hardware, etc.) and incrementally process the response.</strong> HTTP client libraries that can only parse the response after the connection is closed cannot be used. For the encoding used, please refer to <strong>Events</strong> section below.</p>
<p>All subsequent (non-<code>SUBSCRIBE</code>) requests to the &quot;/scheduler&quot; endpoint (see details below in <strong>Calls</strong> section) must be sent using a different connection than the one used for subscription. Schedulers can submit requests using more than one different HTTP connection.</p>
<p>The master responds to HTTP POST requests that require asynchronous processing with status <strong>202 Accepted</strong> (or, for unsuccessful requests, with 4xx or 5xx status codes; details in later sections). The <strong>202 Accepted</strong> response means that a request has been accepted for processing, not that the processing of the request has been completed. The request might or might not be acted upon by Mesos (e.g., master fails during the processing of the request). Any asynchronous responses from these requests will be streamed on the long-lived subscription connection.</p>
<p>The master responds to HTTP POST requests that can be answered synchronously and immediately with status <strong>200 OK</strong> (or, for unsuccessful requests, with 4xx or 5xx status codes; details in later sections), possibly including a response body encoded in JSON or Protobuf. The encoding depends on the <strong>Accept</strong> header present in the request (the default encoding is JSON).</p>
<h2 id="calls"><a class="header" href="#calls">Calls</a></h2>
<p>The following calls are currently accepted by the master. The canonical source of this information is <a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/scheduler/scheduler.proto">scheduler.proto</a>. When sending JSON-encoded Calls, schedulers should encode raw bytes in Base64 and strings in UTF-8. All non-<code>SUBSCRIBE</code> calls should include the <code>Mesos-Stream-Id</code> header, explained in the <a href="scheduler-http-api.html#subscribe"><code>SUBSCRIBE</code></a> section. <code>SUBSCRIBE</code> calls should never include the <code>Mesos-Stream-Id</code> header.</p>
<p><a id="recordio-response-format"></a></p>
<h3 id="recordio-response-format"><a class="header" href="#recordio-response-format">RecordIO response format</a></h3>
<p>The response returned from the <code>SUBSCRIBE</code> call (see <a href="scheduler-http-api.html#subscribe">below</a>) is encoded in RecordIO format, which essentially prepends to a single record (either JSON or serialized Protobuf) its length in bytes, followed by a newline and then the data. See <a href="recordio.html">RecordIO Format</a> for details.</p>
<p><a id="subscribe"></a></p>
<h3 id="subscribe"><a class="header" href="#subscribe">SUBSCRIBE</a></h3>
<p>This is the first step in the communication process between the scheduler and the master. This is also to be considered as subscription to the &quot;/scheduler&quot; event stream.</p>
<p>To subscribe with the master, the scheduler sends an HTTP POST with a <code>SUBSCRIBE</code> message including the required FrameworkInfo, the list of initially suppressed roles and the initial offer constraints. The initially suppressed roles, as well as roles for which offer constraints are specified, must be contained in the set of roles in FrameworkInfo. Note that Mesos 1.11.0 simply ignores constraints for invalid roles, but this might change in the future.</p>
<p>Note that if &quot;subscribe.framework_info.id&quot; and &quot;FrameworkID&quot; are not set, the master considers the scheduler as a new one and subscribes it by assigning it a FrameworkID. The HTTP response is a stream in RecordIO format; the event stream begins with either a <code>SUBSCRIBED</code> event or an <code>ERROR</code> event (see details in <strong>Events</strong> section). The response also includes the <code>Mesos-Stream-Id</code> header, which is used by the master to uniquely identify the subscribed scheduler instance. This stream ID header should be included in all subsequent non-<code>SUBSCRIBE</code> calls sent over this subscription connection to the master. The value of <code>Mesos-Stream-Id</code> is guaranteed to be at most 128 bytes in length.</p>
<pre><code>SUBSCRIBE Request (JSON):

POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Accept: application/json
Connection: close

{
   &quot;type&quot;		: &quot;SUBSCRIBE&quot;,
   &quot;subscribe&quot;	: {
      &quot;framework_info&quot;	: {
        &quot;user&quot; :  &quot;foo&quot;,
        &quot;name&quot; :  &quot;Example HTTP Framework&quot;,
        &quot;roles&quot;: [&quot;test1&quot;, &quot;test2&quot;],
        &quot;capabilities&quot; : [{&quot;type&quot;: &quot;MULTI_ROLE&quot;}]
      },
      &quot;suppressed_roles&quot; : [&quot;test2&quot;],
      &quot;offer_constraints&quot; : {
        &quot;role_constraints&quot;: {
          &quot;test1&quot;: {
            &quot;groups&quot;: [{
              &quot;attribute_constraints&quot;: [{
                &quot;selector&quot;: {&quot;attribute_name&quot;: &quot;foo&quot;},
                &quot;predicate&quot;: {&quot;exists&quot;: {}}
              }]
            }]
          }
        }
      }
   }
}

SUBSCRIBE Response Event (JSON):
HTTP/1.1 200 OK

Content-Type: application/json
Transfer-Encoding: chunked
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

&lt;event length&gt;
{
 &quot;type&quot;			: &quot;SUBSCRIBED&quot;,
 &quot;subscribed&quot;	: {
     &quot;framework_id&quot;               : {&quot;value&quot;:&quot;12220-3440-12532-2345&quot;},
     &quot;heartbeat_interval_seconds&quot; : 15
  }
}
&lt;more events&gt;
</code></pre>
<p>Alternatively, if &quot;subscribe.framework_info.id&quot; and &quot;FrameworkID&quot; are set, the master considers this a request from an already subscribed scheduler reconnecting after a disconnection (e.g., due to master/scheduler failover or network disconnection) and responds
with a <code>SUBSCRIBED</code> event. For further details, see the <strong>Disconnections</strong> section below.</p>
<p>NOTE: In the old version of the API, (re-)registered callbacks also included MasterInfo, which contained information about the master the driver currently connected to. With the new API, since schedulers explicitly subscribe with the leading master (see details below in <strong>Master Detection</strong> section), it's not relevant anymore.</p>
<p>NOTE: By providing a different FrameworkInfo and/or set of suppressed roles
and/or offer constraints, a re-subscribing scheduler can change some of the
fields of FrameworkInfo, the set of suppressed roles and/or offer constraints.
Allowed changes and their effects are consistent with those that can be
performed via <code>UPDATE_FRAMEWORK</code> call (see below).</p>
<p>If subscription fails for whatever reason (e.g., invalid request), an HTTP 4xx response is returned with the error message as part of the body and the connection is closed.</p>
<p>A scheduler can make additional HTTP requests to the &quot;/scheduler&quot; endpoint only after it has opened a persistent connection to it by sending a <code>SUBSCRIBE</code> request and received a <code>SUBSCRIBED</code> response. Calls made without subscription will result in &quot;403 Forbidden&quot; instead of a &quot;202 Accepted&quot; response. A scheduler might also receive a &quot;400 Bad Request&quot; response if the HTTP request is malformed (e.g., malformed HTTP headers).</p>
<p>Note that the <code>Mesos-Stream-Id</code> header should <strong>never</strong> be included with a <code>SUBSCRIBE</code> call; the master will always provide a new unique stream ID for each subscription.</p>
<h3 id="teardown"><a class="header" href="#teardown">TEARDOWN</a></h3>
<p>Sent by the scheduler when it wants to tear itself down. When Mesos receives this request it will shut down all executors (and consequently kill tasks). It then removes the framework and closes all open connections from this scheduler to the Master.</p>
<pre><code>TEARDOWN Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;TEARDOWN&quot;
}

TEARDOWN Response:
HTTP/1.1 202 Accepted
</code></pre>
<h3 id="accept"><a class="header" href="#accept">ACCEPT</a></h3>
<p>Sent by the scheduler when it accepts offer(s) sent by the master. The <code>ACCEPT</code> request includes the type of operations (e.g., launch task, launch task group, reserve resources, create volumes) that the scheduler wants to perform on the offers. Note that until the scheduler replies (accepts or declines) to an offer, the offer's resources are considered allocated to the offer's role and to the framework. Also, any of the offer's resources not used in the <code>ACCEPT</code> call (e.g., to launch a task or task group) are considered declined and might be reoffered to other frameworks, meaning that they will not be reoffered to the scheduler for the amount of time defined by the filter. The same <code>OfferID</code> cannot be used in more than one <code>ACCEPT</code> call. These semantics might change when we add new features to Mesos (e.g., persistence, reservations, optimistic offers, resizeTask, etc.).</p>
<p>The scheduler API uses <code>Filters.refuse_seconds</code> to specify the duration for which resources are considered declined. If <code>filters</code> is not set, then the default value defined in <a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/mesos.proto">mesos.proto</a> will be used.</p>
<p>NOTE: Mesos will cap <code>Filters.refuse_seconds</code> at 31536000 seconds (365 days).</p>
<p>The master will send task status updates in response to <code>LAUNCH</code> and <code>LAUNCH_GROUP</code> operations. For other types of operations, if an operation ID is specified, the master will send operation status updates in response.</p>
<p>For more information on running workloads using this call, see the <a href="running-workloads.html">introduction to the <code>LAUNCH_GROUP</code> and <code>LAUNCH</code> operations</a>.</p>
<pre><code>ACCEPT Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;: {&quot;value&quot;: &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;: &quot;ACCEPT&quot;,
  &quot;accept&quot;: {
    &quot;offer_ids&quot;: [
      {&quot;value&quot;: &quot;12220-3440-12532-O12&quot;}
    ],
    &quot;operations&quot;: [
      {
        &quot;type&quot;: &quot;LAUNCH&quot;,
        &quot;launch&quot;: {
          &quot;task_infos&quot;: [
            {
              &quot;name&quot;: &quot;My Task&quot;,
              &quot;task_id&quot;: {&quot;value&quot;: &quot;12220-3440-12532-my-task&quot;},
              &quot;agent_id&quot;: {&quot;value&quot;: &quot;12220-3440-12532-S1233&quot;},
              &quot;executor&quot;: {
                &quot;command&quot;: {
                  &quot;shell&quot;: true,
                  &quot;value&quot;: &quot;sleep 1000&quot;
                },
                &quot;executor_id&quot;: {&quot;value&quot;: &quot;12214-23523-my-executor&quot;}
              },
              &quot;resources&quot;: [
                {
                  &quot;allocation_info&quot;: {&quot;role&quot;: &quot;engineering&quot;},
                  &quot;name&quot;: &quot;cpus&quot;,
                  &quot;role&quot;: &quot;*&quot;,
                  &quot;type&quot;: &quot;SCALAR&quot;,
                  &quot;scalar&quot;: {&quot;value&quot;: 1.0}
				        }, {
                  &quot;allocation_info&quot;: {&quot;role&quot;: &quot;engineering&quot;},
                  &quot;name&quot;: &quot;mem&quot;,
                  &quot;role&quot;: &quot;*&quot;,
                  &quot;type&quot;: &quot;SCALAR&quot;,
                  &quot;scalar&quot;: {&quot;value&quot;: 128.0}
				        }
              ],
              &quot;limits&quot;: {
                &quot;cpus&quot;: &quot;Infinity&quot;,
                &quot;mem&quot;: 512.0
              }
            }
          ]
        }
      }
    ],
    &quot;filters&quot;: {&quot;refuse_seconds&quot;: 5.0}
  }
}

ACCEPT Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="decline"><a class="header" href="#decline">DECLINE</a></h3>
<p>Sent by the scheduler to explicitly decline offer(s) received. Note that this is same as sending an <code>ACCEPT</code> call with no operations.</p>
<pre><code>DECLINE Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;DECLINE&quot;,
  &quot;decline&quot;			: {
    &quot;offer_ids&quot;	: [
                   {&quot;value&quot; : &quot;12220-3440-12532-O12&quot;},
                   {&quot;value&quot; : &quot;12220-3440-12532-O13&quot;}
                  ],
    &quot;filters&quot;	: {&quot;refuse_seconds&quot; : 5.0}
  }
}

DECLINE Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="revive"><a class="header" href="#revive">REVIVE</a></h3>
<p>Sent by the scheduler to perform two actions:</p>
<ol>
<li>Place the scheduler's role(s) in a non-<code>SUPPRESS</code>ed state in order to once again receive offers. No-op if the role is not suppressed.</li>
<li>Clears all filters for its role(s) that were previously set via <code>ACCEPT</code> and <code>DECLINE</code>.</li>
</ol>
<p>If no role is specified, the operation will apply to all of the scheduler's subscribed roles.</p>
<pre><code>REVIVE Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot; : {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;         : &quot;REVIVE&quot;,
  &quot;revive&quot;       : {&quot;role&quot;: &lt;one-of-the-subscribed-roles&gt;}
}

REVIVE Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="kill"><a class="header" href="#kill">KILL</a></h3>
<p>Sent by the scheduler to kill a specific task. If the scheduler has a custom executor, the kill is forwarded to the executor; it is up to the executor to kill the task and send a <code>TASK_KILLED</code> (or <code>TASK_FAILED</code>) update. If the task hasn't yet been delivered to the executor when Mesos master or agent receives the kill request, a <code>TASK_KILLED</code> is generated and the task launch is not forwarded to the executor. Note that if the task belongs to a task group, killing of one task results in all tasks in the task group being killed. Mesos releases the resources for a task once it receives a terminal update for the task. If the task is unknown to the master, a <code>TASK_LOST</code> will be generated.</p>
<pre><code>KILL Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;KILL&quot;,
  &quot;kill&quot;			: {
    &quot;task_id&quot;	:  {&quot;value&quot; : &quot;12220-3440-12532-my-task&quot;},
    &quot;agent_id&quot;	:  {&quot;value&quot; : &quot;12220-3440-12532-S1233&quot;}
  }
}

KILL Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="shutdown"><a class="header" href="#shutdown">SHUTDOWN</a></h3>
<p>Sent by the scheduler to shutdown a specific custom executor (NOTE: This is a new call that was not present in the old API). When an executor gets a shutdown event, it is expected to kill all its tasks (and send <code>TASK_KILLED</code> updates) and terminate. If an executor doesn't terminate within a certain timeout (configurable via the <code>--executor_shutdown_grace_period</code> agent flag), the agent will forcefully destroy the container (executor and its tasks) and transition its active tasks to <code>TASK_LOST</code>.</p>
<pre><code>SHUTDOWN Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;SHUTDOWN&quot;,
  &quot;shutdown&quot;		: {
    &quot;executor_id&quot;	:  {&quot;value&quot; : &quot;123450-2340-1232-my-executor&quot;},
    &quot;agent_id&quot;		:  {&quot;value&quot; : &quot;12220-3440-12532-S1233&quot;}
  }
}

SHUTDOWN Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="acknowledge"><a class="header" href="#acknowledge">ACKNOWLEDGE</a></h3>
<p>Sent by the scheduler to acknowledge a status update. Note that with the new API, schedulers are responsible for explicitly acknowledging the receipt of status updates that have <code>status.uuid</code> set. These status updates are retried until they are acknowledged by the scheduler. The scheduler must not acknowledge status updates that do not have <code>status.uuid</code> set, as they are not retried. The <code>uuid</code> field contains raw bytes encoded in Base64.</p>
<pre><code>ACKNOWLEDGE Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;ACKNOWLEDGE&quot;,
  &quot;acknowledge&quot;		: {
    &quot;agent_id&quot;	:  {&quot;value&quot; : &quot;12220-3440-12532-S1233&quot;},
    &quot;task_id&quot;	:  {&quot;value&quot; : &quot;12220-3440-12532-my-task&quot;},
    &quot;uuid&quot;		:  &quot;jhadf73jhakdlfha723adf&quot;
  }
}

ACKNOWLEDGE Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="acknowledge_operation_status"><a class="header" href="#acknowledge_operation_status">ACKNOWLEDGE_OPERATION_STATUS</a></h3>
<p>Sent by the scheduler to acknowledge an operation status update. Schedulers are responsible for explicitly acknowledging the receipt of status updates that have <code>status.uuid</code> set. These status updates are retried until they are acknowledged by the scheduler. The scheduler must not acknowledge status updates that do not have <code>status.uuid</code> set, as they are not retried. The <code>uuid</code> field contains raw bytes encoded in Base64.</p>
<pre><code>ACKNOWLEDGE_OPERATION_STATUS Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;: { &quot;value&quot;: &quot;12220-3440-12532-2345&quot; },
  &quot;type&quot;: &quot;ACKNOWLEDGE_OPERATION_STATUS&quot;,
  &quot;acknowledge_operation_status&quot;: {
    &quot;agent_id&quot;: { &quot;value&quot;: &quot;12220-3440-12532-S1233&quot; },
    &quot;resource_provider_id&quot;: { &quot;value&quot;: &quot;12220-3440-12532-rp&quot; },
    &quot;uuid&quot;: &quot;jhadf73jhakdlfha723adf&quot;,
    &quot;operation_id&quot;: &quot;73jhakdlfha723adf&quot;
  }
}

ACKNOWLEDGE_OPERATION_STATUS Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="reconcile"><a class="header" href="#reconcile">RECONCILE</a></h3>
<p>Sent by the scheduler to query the status of non-terminal tasks. This causes the master to send back <code>UPDATE</code> events for each task in the list. Tasks that are no longer known to Mesos will result in <code>TASK_LOST</code> updates. If the list of tasks is empty, master will send <code>UPDATE</code> events for all currently known tasks of the framework.</p>
<pre><code>RECONCILE Request (JSON):
POST /api/v1/scheduler   HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;RECONCILE&quot;,
  &quot;reconcile&quot;		: {
    &quot;tasks&quot;		: [
                   { &quot;task_id&quot;  : {&quot;value&quot; : &quot;312325&quot;},
                     &quot;agent_id&quot; : {&quot;value&quot; : &quot;123535&quot;}
                   }
                  ]
  }
}

RECONCILE Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="reconcile_operations"><a class="header" href="#reconcile_operations">RECONCILE_OPERATIONS</a></h3>
<p>Sent by the scheduler to query the status of non-terminal and terminal-but-unacknowledged operations. This causes the master to send back <code>UPDATE_OPERATION_STATUS</code> events for each operation in the list. If the list of operations is empty, the master will send events for all currently known operations of the framework.</p>
<pre><code>RECONCILE_OPERATIONS Request (JSON):
POST /api/v1/scheduler   HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Accept: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;: { &quot;value&quot;: &quot;12220-3440-12532-2345&quot; },
  &quot;type&quot;: &quot;RECONCILE_OPERATIONS&quot;,
  &quot;reconcile_operations&quot;: {
    &quot;operations&quot;: [
      {
        &quot;operation_id&quot;: { &quot;value&quot;: &quot;312325&quot; },
        &quot;agent_id&quot;: { &quot;value&quot;: &quot;123535&quot; },
        &quot;resource_provider_id&quot;: { &quot;value&quot;: &quot;927695&quot; }
      }
    ]
  }
}

RECONCILE_OPERATIONS Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="message"><a class="header" href="#message">MESSAGE</a></h3>
<p>Sent by the scheduler to send arbitrary binary data to the executor. Mesos neither interprets this data nor makes any guarantees about the delivery of this message to the executor. <code>data</code> is raw bytes encoded in Base64.</p>
<pre><code>MESSAGE Request (JSON):
POST /api/v1/scheduler   HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;MESSAGE&quot;,
  &quot;message&quot;			: {
    &quot;agent_id&quot;       : {&quot;value&quot; : &quot;12220-3440-12532-S1233&quot;},
    &quot;executor_id&quot;    : {&quot;value&quot; : &quot;my-framework-executor&quot;},
    &quot;data&quot;           : &quot;adaf838jahd748jnaldf&quot;
  }
}

MESSAGE Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="request"><a class="header" href="#request">REQUEST</a></h3>
<p>Sent by the scheduler to request resources from the master/allocator. The built-in hierarchical allocator simply ignores this request but other allocators (modules) can interpret this in a customizable fashion.</p>
<pre><code>Request (JSON):
POST /api/v1/scheduler   HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot;	: {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;			: &quot;REQUEST&quot;,
  &quot;requests&quot;		: [
      {
         &quot;agent_id&quot;       : {&quot;value&quot; : &quot;12220-3440-12532-S1233&quot;},
         &quot;resources&quot;      : {}
      }
  ]
}

REQUEST Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="suppress"><a class="header" href="#suppress">SUPPRESS</a></h3>
<p>Sent by the scheduler when it doesn't need offers for a given set of its roles. When Mesos master receives this request, it will stop sending offers for the given set of roles to the framework. As a special case, if roles are not specified, all subscribed roles of this framework are suppressed.</p>
<p>Note that master continues to send offers to other subscribed roles of this framework that are not suppressed. Also, status updates about tasks, executors and agents are not affected by this call. </p>
<p>If the scheduler wishes to receive offers for the suppressed roles again (e.g., it needs to schedule new workloads), it can send <code>REVIVE</code> call.</p>
<pre><code>SUPPRESS Request (JSON):
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Mesos-Stream-Id: 130ae4e3-6b13-4ef4-baa9-9f2e85c3e9af

{
  &quot;framework_id&quot; : {&quot;value&quot; : &quot;12220-3440-12532-2345&quot;},
  &quot;type&quot;         : &quot;SUPPRESS&quot;,
  &quot;suppress&quot;     : {&quot;roles&quot;: &lt;an-array-of-strings&gt;}
}

SUPPRESS Response:
HTTP/1.1 202 Accepted

</code></pre>
<h3 id="update_framework"><a class="header" href="#update_framework">UPDATE_FRAMEWORK</a></h3>
<p>Sent by the scheduler to change fields of its <code>FrameworkInfo</code> and/or the set of
suppressed roles and/or offer constraints. Allowed changes and their effects
are consistent with changing the same fields via re-subscribing.</p>
<h4 id="disallowed-updates"><a class="header" href="#disallowed-updates">Disallowed updates</a></h4>
<p>Updating the following <code>FrameworkInfo</code> fields is not allowed:</p>
<ul>
<li><code>principal</code> (mainly because &quot;changing a principal&quot; effectively means
a transfer of a framework by an original principal to the new one; secure
mechanism for such transfer is yet to be developed)</li>
<li><code>user</code></li>
<li><code>checkpoint</code></li>
</ul>
<p><code>UPDATE_FRAMEWORK</code> call trying to update any of these fields is not valid,
unlike an attempt to change <code>user</code>/<code>checkpoint</code> when resubscribing, in which
case the new value is ignored.</p>
<h4 id="updating-framework-roles"><a class="header" href="#updating-framework-roles">Updating framework roles</a></h4>
<p>Updating <code>framework_info.roles</code> and <code>suppressed_roles</code> is supported.
In a valid <code>UPDATE_FRAMEWORK</code> call, new suppressed roles must be a (potentially
empty) subset of new framework roles.</p>
<p>Updating roles has the following effects:</p>
<ul>
<li>After the call is processed, master will be sending offers to all
non-suppressed roles of the framework.</li>
<li>Offers to old framework roles removed by this call will be rescinded.</li>
<li>Offers to roles from suppressed set will NOT be rescinded.</li>
<li>For roles that were transitioned out of suppressed, offer filters (set by
ACCEPT/DECLINE) will be cleared.
will be cleared.</li>
<li>Other framework objects that use roles removed by this call (for example,
tasks) are not affected.</li>
</ul>
<h4 id="updating-offer-constraints"><a class="header" href="#updating-offer-constraints">Updating offer constraints</a></h4>
<p>For the <code>UPDATE_FRAMEWORK</code> call to be successfull, the <code>offer_constraints</code>
field, if present, must be internally valid (for the constraints validity
criteria, please refer to comments in
<a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/scheduler/scheduler.proto">scheduler.proto</a>)</p>
<p>As of 1.11.0, Mesos ignores offer constraints for roles other than valid roles
in <code>framework_info.roles</code>; future versions of Mesos are going to treat such
offer constraints as invalid.</p>
<p>Updated offer constraints have an immediate effect on offer generation after
update, but have no effect on already outstanding offers. Frameworks should not
expect that offers they receive right after the <code>UPDATE_FRAMEWORK</code> call
will satisfy the new constraints.</p>
<h4 id="updating-other-fields"><a class="header" href="#updating-other-fields">Updating other fields</a></h4>
<ul>
<li>Updating <code>name</code>, <code>hostname</code>, <code>webui_url</code> and <code>labels</code> is fully supported
by Mesos; these updates are simply propagated to Mesos API endpoints.</li>
<li>Updating <code>failover_timeout</code> and <code>offer_filters</code> is supported. Note that
there is no way to guarantee that offers issued when the old <code>offer_filters</code>
were in place will not be received by the framework after the master applies
the update.</li>
<li>Schedulers can add capabilities via updating <code>capabilities</code> field. The call
attempting to remove a capability is not considered invalid; however, there
is no guarantee that it is safe for the framework to remove the capability.
If you really need your framewok to be able to remove a capability, please
reach out to the Mesos dev/user list (dev@mesos.apache.org or
user@mesos.apache.org).
In future, to prevent accidental unsafe downgrade of frameworks, Mesos will
need to implement minimum capabilities for schedulers (similarly to minimum
master/agent capabilities, see
<a href="https://issues.apache.org/jira/browse/MESOS-8878">MESOS-8878</a>).</li>
</ul>
<pre><code>
UPDATE_FRAMEWORK Request (JSON):

POST /api/v1/scheduler  HTTP/1.1

Host: masterhost:5050
Content-Type: application/json
Accept: application/json
Connection: close

{
   &quot;type&quot;		: &quot;UPDATE_FRAMEWORK&quot;,
   &quot;update_framework&quot;	: {
      &quot;framework_info&quot;	: {
        &quot;user&quot; :  &quot;foo&quot;,
        &quot;name&quot; :  &quot;Example HTTP Framework&quot;,
        &quot;roles&quot;: [&quot;test1&quot;, &quot;test2&quot;],
        &quot;capabilities&quot; : [{&quot;type&quot;: &quot;MULTI_ROLE&quot;}]
      },
      &quot;suppressed_roles&quot; : [&quot;test2&quot;]
      &quot;offer_constraints&quot; : {
        &quot;role_constraints&quot;: {
          &quot;test1&quot;: {
            &quot;groups&quot;: [{
              &quot;attribute_constraints&quot;: [{
                &quot;selector&quot;: {&quot;attribute_name&quot;: &quot;foo&quot;},
                &quot;predicate&quot;: {&quot;exists&quot;: {}}
              }]
            }]
          }
        }
      }
  }
}

UPDATE_FRAMEWORK Response:
HTTP/1.1 200 OK
</code></pre>
<p>Response codes:</p>
<ul>
<li>&quot;200 OK&quot; after the update has been successfully applied by the master and
sent to the agents.</li>
<li>&quot;400 Bad request&quot; if the call was not valid or authorizing the call failed.</li>
<li>&quot;403 Forbidden&quot; if the principal was declined authorization to use the
provided FrameworkInfo. (Typical authorizer implementations will check
authorization to use specified roles.)</li>
</ul>
<p>No partial updates occur in error cases: either all fields are updated or none
of them.</p>
<p>NOTE: In Mesos 1.9, effects of changing roles or suppressed roles set via
UPDATE_FRAMEWORK could be potentially reordered with related effects of
<code>ACCEPT</code>/<code>DECLINE</code>/<code>SUPPRESS</code>/<code>REVIVE</code> or another <code>UPDATE_FRAMEWORK</code>;
to avoid such reordering, it was necessary to wait for UPDATE_FRAMEWORK response
before issuing the next call. This issue has been fixed in Mesos 1.10.0 (see
<a href="https://issues.apache.org/jira/browse/MESOS-10056">MESOS-10056</a>).</p>
<h2 id="events"><a class="header" href="#events">Events</a></h2>
<p>Schedulers are expected to keep a <strong>persistent</strong> connection to the &quot;/scheduler&quot; endpoint (even after getting a <code>SUBSCRIBED</code> HTTP Response event). This is indicated by the &quot;Connection: keep-alive&quot; and &quot;Transfer-Encoding: chunked&quot; headers with <em>no</em> &quot;Content-Length&quot; header set. All subsequent events that are relevant to this framework generated by Mesos are streamed on this connection. The master encodes each Event in RecordIO format, i.e., string representation of the length of the event in bytes followed by JSON or binary Protobuf (possibly compressed) encoded event. The length of an event is a 64-bit unsigned integer (encoded as a textual value) and will never be &quot;0&quot;. Also, note that the RecordIO encoding should be decoded by the scheduler whereas the underlying HTTP chunked encoding is typically invisible at the application (scheduler) layer. The type of content encoding used for the events will be determined by the accept header of the POST request (e.g., Accept: application/json).</p>
<p>The following events are currently sent by the master. The canonical source of this information is at <a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/scheduler/scheduler.proto">scheduler.proto</a>. Note that when sending JSON encoded events, master encodes raw bytes in Base64 and strings in UTF-8.</p>
<h3 id="subscribed"><a class="header" href="#subscribed">SUBSCRIBED</a></h3>
<p>The first event sent by the master when the scheduler sends a <code>SUBSCRIBE</code> request, if authorization / validation succeeds. See <code>SUBSCRIBE</code> in Calls section for the format.</p>
<h3 id="offers"><a class="header" href="#offers">OFFERS</a></h3>
<p>Sent by the master whenever there are new resources that can be offered to the framework. Each offer corresponds to a set of resources on an agent and is allocated to one of roles the framework is subscribed to. Until the scheduler 'Accept's or 'Decline's an offer the resources are considered allocated to the scheduler, unless the offer is otherwise rescinded, e.g., due to a lost agent or <code>--offer_timeout</code>.</p>
<pre><code>OFFERS Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;OFFERS&quot;,
  &quot;offers&quot;	: [
    {
      &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
      &quot;id&quot;             : {&quot;value&quot;: &quot;12214-23523-O235235&quot;},
      &quot;framework_id&quot;   : {&quot;value&quot;: &quot;12124-235325-32425&quot;},
      &quot;agent_id&quot;       : {&quot;value&quot;: &quot;12325-23523-S23523&quot;},
      &quot;hostname&quot;       : &quot;agent.host&quot;,
      &quot;resources&quot;      : [
                          {
                           &quot;allocation_info&quot;: { &quot;role&quot;: &quot;engineering&quot; },
                           &quot;name&quot;   : &quot;cpus&quot;,
                           &quot;type&quot;   : &quot;SCALAR&quot;,
                           &quot;scalar&quot; : {&quot;value&quot; : 2},
                           &quot;role&quot;   : &quot;*&quot;
                          }
                         ],
      &quot;attributes&quot;     : [
                          {
                           &quot;name&quot;   : &quot;os&quot;,
                           &quot;type&quot;   : &quot;TEXT&quot;,
                           &quot;text&quot;   : {&quot;value&quot; : &quot;ubuntu16.04&quot;}
                          }
                         ],
      &quot;executor_ids&quot;   : [
                          {&quot;value&quot; : &quot;12214-23523-my-executor&quot;}
                         ]
    }
  ]
}
</code></pre>
<h3 id="rescind"><a class="header" href="#rescind">RESCIND</a></h3>
<p>Sent by the master when a particular offer is no longer valid (e.g., the agent corresponding to the offer has been removed) and hence needs to be rescinded. Any future calls (<code>ACCEPT</code> / <code>DECLINE</code>) made by the scheduler regarding this offer will be invalid.</p>
<pre><code>RESCIND Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;RESCIND&quot;,
  &quot;rescind&quot;	: {
    &quot;offer_id&quot;	: { &quot;value&quot; : &quot;12214-23523-O235235&quot;}
  }
}
</code></pre>
<h3 id="update-1"><a class="header" href="#update-1">UPDATE</a></h3>
<p>Sent by the master whenever there is a status update that is generated by the executor, agent or master. Status updates should be used by executors to reliably communicate the status of the tasks that they manage. It is crucial that a terminal update (e.g., <code>TASK_FINISHED</code>, <code>TASK_KILLED</code>, <code>TASK_FAILED</code>) is sent by the executor as soon as the task terminates, in order for Mesos to release the resources allocated to the task. It is also the responsibility of the scheduler to explicitly acknowledge the receipt of status updates that are reliably retried. See <code>ACKNOWLEDGE</code> in the Calls section above for the semantics. Note that <code>uuid</code> and <code>data</code> are raw bytes encoded in Base64.</p>
<pre><code>UPDATE Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;UPDATE&quot;,
  &quot;update&quot;	: {
    &quot;status&quot;	: {
        &quot;task_id&quot;	: { &quot;value&quot; : &quot;12344-my-task&quot;},
        &quot;state&quot;		: &quot;TASK_RUNNING&quot;,
        &quot;source&quot;	: &quot;SOURCE_EXECUTOR&quot;,
        &quot;uuid&quot;		: &quot;adfadfadbhgvjayd23r2uahj&quot;,
        &quot;bytes&quot;		: &quot;uhdjfhuagdj63d7hadkf&quot;
      }
  }
}
</code></pre>
<h3 id="update_operation_status"><a class="header" href="#update_operation_status">UPDATE_OPERATION_STATUS</a></h3>
<p>Sent by the master whenever there is an update to the state of an operation for which the scheduler requested feedback by setting the operation's <code>id</code> field. It is the responsibility of the scheduler to explicitly acknowledge the receipt of any status updates which have their <code>uuid</code> field set, as this indicates that the update will be retried until acknowledgement is received. This ensures that such updates are delivered reliably. See <code>ACKNOWLEDGE_OPERATION_STATUS</code> in the Calls section above for the relevant acknowledgement semantics. Note that the <code>uuid</code> field contains raw bytes encoded in Base64.</p>
<pre><code>UPDATE_OPERATION_STATUS Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;UPDATE_OPERATION_STATUS&quot;,
  &quot;update_operation_status&quot;	: {
    &quot;status&quot;	: {
        &quot;operation_id&quot; : { &quot;value&quot; : &quot;operation-1234&quot;},
        &quot;state&quot;        : &quot;OPERATION_FAILED&quot;,
        &quot;uuid&quot;         : &quot;adfadfadbhgvjayd23r2uahj&quot;,
        &quot;agent_id&quot;     : { &quot;value&quot; : &quot;12214-23523-S235235&quot;},
        &quot;resource_provider_id&quot; : { &quot;value&quot; : &quot;83978-17885-1089645&quot;}
      }
  }
}
</code></pre>
<h3 id="message-1"><a class="header" href="#message-1">MESSAGE</a></h3>
<p>A custom message generated by the executor that is forwarded to the scheduler by the master. This message is not interpreted by Mesos and is only forwarded (without reliability guarantees) to the scheduler. It is up to the executor to retry if the message is dropped for any reason. The <code>data</code> field contains raw bytes encoded as Base64.</p>
<pre><code>MESSAGE Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;MESSAGE&quot;,
  &quot;message&quot;	: {
    &quot;agent_id&quot;		: { &quot;value&quot; : &quot;12214-23523-S235235&quot;},
    &quot;executor_id&quot;	: { &quot;value&quot; : &quot;12214-23523-my-executor&quot;},
    &quot;data&quot;			: &quot;adfadf3t2wa3353dfadf&quot;
  }
}
</code></pre>
<h3 id="failure"><a class="header" href="#failure">FAILURE</a></h3>
<p>Sent by the master when an agent is removed from the cluster (e.g., failed health checks) or when an executor is terminated. This event coincides with receipt of terminal <code>UPDATE</code> events for any active tasks belonging to the agent or executor and receipt of <code>RESCIND</code> events for any outstanding offers belonging to the agent. Note that there is no guaranteed order between the <code>FAILURE</code>, <code>UPDATE</code>, and <code>RESCIND</code> events.</p>
<pre><code>FAILURE Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;FAILURE&quot;,
  &quot;failure&quot;	: {
    &quot;agent_id&quot;		: { &quot;value&quot; : &quot;12214-23523-S235235&quot;},
    &quot;executor_id&quot;	: { &quot;value&quot; : &quot;12214-23523-my-executor&quot;},
    &quot;status&quot;		: 1
  }
}
</code></pre>
<h3 id="error"><a class="header" href="#error">ERROR</a></h3>
<p>Can be sent either:</p>
<ul>
<li>As the first event (in lieu of <code>SUBSCRIBED</code>) when the scheduler's <code>SUBSCRIBE</code> request is invalid (e.g. invalid <code>FrameworkInfo</code>) or unauthorized (e.g., a framework is not authorized to subscribe with some of the given <code>FrameworkInfo.roles</code>).</li>
<li>When an asynchronous error event is generated (e.g. the master detects a newer subscription from a failed over instance of the scheduler).</li>
</ul>
<p>It is recommended that the framework abort when it receives an error and retry subscription as necessary.</p>
<pre><code>ERROR Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;ERROR&quot;,
  &quot;message&quot;	: &quot;Framework is not authorized&quot;
}
</code></pre>
<h3 id="heartbeat"><a class="header" href="#heartbeat">HEARTBEAT</a></h3>
<p>This event is periodically sent by the master to inform the scheduler that a connection is alive. This also helps ensure that network intermediates do not close the persistent subscription connection due to lack of data flow. See the next section on how a scheduler can use this event to deal with network partitions.</p>
<pre><code>HEARTBEAT Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;	: &quot;HEARTBEAT&quot;
}
</code></pre>
<h2 id="disconnections"><a class="header" href="#disconnections">Disconnections</a></h2>
<p>Master considers a scheduler disconnected if the persistent subscription connection (opened via <code>SUBSCRIBE</code> request) to &quot;/scheduler&quot; breaks. The connection could break for several reasons, e.g., scheduler restart, scheduler failover, network error. Note that the master doesn't keep track of non-subscription connection(s) to
&quot;/scheduler&quot; because it is not expected to be a persistent connection.</p>
<p>If master realizes that the subscription connection is broken, it marks the scheduler as &quot;disconnected&quot; and starts a failover timeout (failover timeout is part of FrameworkInfo). It also drops any pending events in its queue. Additionally, it rejects subsequent non-subscribe HTTP requests to &quot;/scheduler&quot; with &quot;403 Forbidden&quot;, until the scheduler subscribes again with &quot;/scheduler&quot;. If the scheduler <em>does not</em> re-subscribe within the failover timeout, the master considers the scheduler gone forever and shuts down all its executors, thus killing all its tasks. Therefore, all production schedulers are recommended to use a high value (e.g., 4 weeks) for the failover timeout.</p>
<p>NOTE: To force shutdown of a framework before the failover timeout elapses (e.g., during framework development and testing), either the framework can send the <code>TEARDOWN</code> call (part of the Scheduler API) or an operator can use the <a href="endpoints/master/teardown.html">/teardown</a> master endpoint (part of the Operator API).</p>
<p>If the scheduler realizes that its subscription connection to &quot;/scheduler&quot; is broken or the master has changed (e.g., via ZooKeeper), it should resubscribe (using a backoff strategy). This is done by sending a <code>SUBSCRIBE</code> request (with framework ID set) on a <strong>new</strong> persistent connection to the &quot;/scheduler&quot; endpoint on the (possibly new) master. It should not send new non-subscribe HTTP requests to &quot;/scheduler&quot; unless it receives a <code>SUBSCRIBED</code> event; such requests will result in &quot;403 Forbidden&quot;.</p>
<p>If the master does not realize that the subscription connection is broken but the scheduler realizes it, the scheduler might open a new persistent connection to
&quot;/scheduler&quot; via <code>SUBSCRIBE</code>. In this case, the master closes the existing subscription connection and allows subscription on the new connection. The invariant here is that only one persistent subscription connection for a given framework ID is allowed on the master.</p>
<p>The master uses the <code>Mesos-Stream-Id</code> header to distinguish scheduler instances from one another. In the case of highly available schedulers with multiple instances, this can prevent unwanted behavior in certain failure scenarios. Each unique <code>Mesos-Stream-Id</code> is valid only for the life of a single subscription connection. Each response to a <code>SUBSCRIBE</code> request contains a <code>Mesos-Stream-Id</code>, and this ID must be included with all subsequent non-subscribe calls sent over that subscription connection. Whenever a new subscription connection is established, a new stream ID is generated and should be used for the life of that connection.</p>
<h3 id="network-partitions"><a class="header" href="#network-partitions">Network partitions</a></h3>
<p>In the case of a network partition, the subscription connection between the scheduler and master might not necessarily break. To be able to detect this scenario, master periodically (e.g., 15s) sends <code>HEARTBEAT</code> events (similar to Twitter's Streaming API). If a scheduler doesn't receive a bunch (e.g., 5) of these heartbeats within a time window, it should immediately disconnect and try to resubscribe. It is highly recommended for schedulers to use an exponential backoff strategy (e.g., up to a maximum of 15s) to avoid overwhelming the master while reconnecting. Schedulers can use a similar timeout (e.g., 75s) for receiving responses to any HTTP requests.</p>
<h2 id="master-detection"><a class="header" href="#master-detection">Master detection</a></h2>
<p>Mesos has a high-availability mode that uses multiple Mesos masters; one active master (called the leader or leading master) and several standbys in case it fails. The masters elect the leader, with ZooKeeper coordinating the election. For more details please refer to the <a href="high-availability.html">documentation</a>.</p>
<p>Schedulers are expected to make HTTP requests to the leading master. If requests are made to a non-leading master a &quot;HTTP 307 Temporary Redirect&quot; will be received with the &quot;Location&quot; header pointing to the leading master.</p>
<p>Example subscription workflow with redirection when the scheduler hits a non-leading master.</p>
<pre><code>Scheduler -&gt; Master
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost1:5050
Content-Type: application/json
Accept: application/json
Connection: keep-alive

{
  &quot;framework_info&quot;	: {
    &quot;user&quot; :  &quot;foo&quot;,
    &quot;name&quot; :  &quot;Example HTTP Framework&quot;
  },
  &quot;type&quot;			: &quot;SUBSCRIBE&quot;
}

Master -&gt; Scheduler
HTTP/1.1 307 Temporary Redirect
Location: masterhost2:5050


Scheduler -&gt; Master
POST /api/v1/scheduler  HTTP/1.1

Host: masterhost2:5050
Content-Type: application/json
Accept: application/json
Connection: keep-alive

{
  &quot;framework_info&quot;	: {
    &quot;user&quot; :  &quot;foo&quot;,
    &quot;name&quot; :  &quot;Example HTTP Framework&quot;
  },
  &quot;type&quot;			: &quot;SUBSCRIBE&quot;
}
</code></pre>
<p>If the scheduler knows the list of master's hostnames for a cluster, it could use this mechanism to find the leading master to subscribe with. Alternatively, the scheduler could use a library that detects the leading master given a ZooKeeper (or etcd) URL. For a C++ library that does ZooKeeper based master detection please look at <code>src/scheduler/scheduler.cpp</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><hr />
<h2>title: Apache Mesos - Executor HTTP API
layout: documentation</h2>
<h1 id="executor-http-api"><a class="header" href="#executor-http-api">Executor HTTP API</a></h1>
<p>A Mesos executor can be built in two different ways:</p>
<ol>
<li>
<p>By using the HTTP API. This allows Mesos executors to be developed without
using C++ or a native client library; instead, a custom executor interacts with
the Mesos agent via HTTP requests, as described below. Although it is
theoretically possible to use the HTTP executor API &quot;directly&quot; (e.g., by using a
generic HTTP library), most executor developers should use a library for their
language of choice that manages the details of the HTTP API; see the document on
<a href="api-client-libraries.html">HTTP API client libraries</a> for a list. This is the
recommended way to develop new Mesos executors.</p>
</li>
<li>
<p>By using the deprecated <code>ExecutorDriver</code> C++ interface. While this interface
is still supported, note that new features are usually not added to it. The
<code>ExecutorDriver</code> handles the details of communicating with the Mesos agent.
Executor developers implement custom executor logic by registering callbacks
with the <code>ExecutorDriver</code> for significant events, such as when a new task launch
request is received. Because the <code>ExecutorDriver</code> interface is written in C++,
this typically requires that executor developers either use C++ or use a C++
binding to their language of choice (e.g., JNI when using JVM-based languages).</p>
</li>
</ol>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>The executor interacts with Mesos via the [/api/v1/executor]
(endpoints/slave/api/v1/executor.md) agent endpoint. We refer to this endpoint
with its suffix &quot;/executor&quot; in the rest of this document. The endpoint accepts
HTTP POST requests with data encoded as JSON (Content-Type: application/json) or
binary Protobuf (Content-Type: application/x-protobuf). The first request that
the executor sends to the &quot;/executor&quot; endpoint is called <code>SUBSCRIBE</code> and results
in a streaming response (&quot;200 OK&quot; status code with Transfer-Encoding: chunked).</p>
<p><strong>Executors are expected to keep the subscription connection open as long as
possible (barring network errors, agent process restarts, software bugs, etc.)
and incrementally process the response.</strong> HTTP client libraries that can only
parse the response after the connection is closed cannot be used. For the
encoding used, please refer to <strong>Events</strong> section below.</p>
<p>All subsequent (non-<code>SUBSCRIBE</code>) requests to the &quot;/executor&quot; endpoint (see
details below in <strong>Calls</strong> section) must be sent using a different connection
than the one used for subscription. The agent responds to these HTTP POST
requests with &quot;202 Accepted&quot; status codes (or, for unsuccessful requests, with
4xx or 5xx status codes; details in later sections). The &quot;202 Accepted&quot; response
means that a request has been accepted for processing, not that the processing
of the request has been completed. The request might or might not be acted upon
by Mesos (e.g., agent fails during the processing of the request). Any
asynchronous responses from these requests will be streamed on the long-lived
subscription connection. Executors can submit requests using more than one
different HTTP connection.</p>
<p>The &quot;/executor&quot; endpoint is served at the Mesos agent's IP:port and in addition,
when the agent has the <code>http_executor_domain_sockets</code> flag set to <code>true</code>, the
executor endpoint is also served on a Unix domain socket, the location of which
can be found by the executor in the <code>MESOS_DOMAIN_SOCKET</code> environment variable.
Connecting to the domain socket is similar to connecting using a TCP socket, and
once the connection is established, data is sent and received in the same way.</p>
<h2 id="calls-1"><a class="header" href="#calls-1">Calls</a></h2>
<p>The following calls are currently accepted by the agent. The canonical source of
this information is <a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/executor/executor.proto">executor.proto</a>.
When sending JSON-encoded Calls, executors should encode raw bytes in Base64 and
strings in UTF-8.</p>
<h3 id="subscribe-1"><a class="header" href="#subscribe-1">SUBSCRIBE</a></h3>
<p>This is the first step in the communication process between the executor and
agent. This is also to be considered as subscription to the &quot;/executor&quot; events
stream.</p>
<p>To subscribe with the agent, the executor sends an HTTP POST with a <code>SUBSCRIBE</code>
message. The HTTP response is a stream in [RecordIO]
(scheduler-http-api.md#recordio-response-format) format; the event stream will
begin with a <code>SUBSCRIBED</code> event (see details in <strong>Events</strong> section).</p>
<p>Additionally, if the executor is connecting to the agent after a
<a href="executor-http-api.html#disconnections">disconnection</a>, it can also send a list of:</p>
<ul>
<li><strong>Unacknowledged Status Updates</strong>: The executor is expected to maintain a list
of status updates not acknowledged by the agent via the <code>ACKNOWLEDGE</code> events.</li>
<li><strong>Unacknowledged Tasks</strong>: The executor is expected to maintain a list of tasks
that have not been acknowledged by the agent. A task is considered
acknowledged if at least one of the status updates for this task is
acknowledged by the agent.</li>
</ul>
<pre><code>SUBSCRIBE Request (JSON):

POST /api/v1/executor  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json
Accept: application/json

{
  &quot;type&quot;: &quot;SUBSCRIBE&quot;,
  &quot;executor_id&quot;: {
    &quot;value&quot;: &quot;387aa966-8fc5-4428-a794-5a868a60d3eb&quot;
  },
  &quot;framework_id&quot;: {
    &quot;value&quot;: &quot;49154f1b-8cf6-4421-bf13-8bd11dccd1f1&quot;
  },
  &quot;subscribe&quot;: {
    &quot;unacknowledged_tasks&quot;: [
      {
        &quot;name&quot;: &quot;dummy-task&quot;,
        &quot;task_id&quot;: {
          &quot;value&quot;: &quot;d40f3f3e-bbe3-44af-a230-4cb1eae72f67&quot;
        },
        &quot;agent_id&quot;: {
          &quot;value&quot;: &quot;f1c9cdc5-195e-41a7-a0d7-adaa9af07f81&quot;
        },
        &quot;command&quot;: {
          &quot;value&quot;: &quot;ls&quot;,
          &quot;arguments&quot;: [
            &quot;-l&quot;,
            &quot;\/tmp&quot;
          ]
        }
      }
    ],
    &quot;unacknowledged_updates&quot;: [
      {
        &quot;framework_id&quot;: {
          &quot;value&quot;: &quot;49154f1b-8cf6-4421-bf13-8bd11dccd1f1&quot;
        },
        &quot;status&quot;: {
          &quot;source&quot;: &quot;SOURCE_EXECUTOR&quot;,
          &quot;task_id&quot;: {
            &quot;value&quot;: &quot;d40f3f3e-bbe3-44af-a230-4cb1eae72f67&quot;
          },
        &quot;state&quot;: &quot;TASK_RUNNING&quot;,
        &quot;uuid&quot;: &quot;ZDQwZjNmM2UtYmJlMy00NGFmLWEyMzAtNGNiMWVhZTcyZjY3Cg==&quot;
        }
      }
    ]
  }
}

SUBSCRIBE Response Event (JSON):
HTTP/1.1 200 OK

Content-Type: application/json
Transfer-Encoding: chunked

&lt;event-length&gt;
{
  &quot;type&quot;: &quot;SUBSCRIBED&quot;,
  &quot;subscribed&quot;: {
    &quot;executor_info&quot;: {
      &quot;executor_id&quot;: {
        &quot;value&quot;: &quot;387aa966-8fc5-4428-a794-5a868a60d3eb&quot;
      },
      &quot;command&quot;: {
        &quot;value&quot;: &quot;\/path\/to\/executor&quot;
      },
      &quot;framework_id&quot;: {
        &quot;value&quot;: &quot;49154f1b-8cf6-4421-bf13-8bd11dccd1f1&quot;
      }
    },
    &quot;framework_info&quot;: {
      &quot;user&quot;: &quot;foo&quot;,
      &quot;name&quot;: &quot;my_framework&quot;
    },
    &quot;agent_id&quot;: {
      &quot;value&quot;: &quot;f1c9cdc5-195e-41a7-a0d7-adaa9af07f81&quot;
    },
    &quot;agent_info&quot;: {
      &quot;host&quot;: &quot;agenthost&quot;,
      &quot;port&quot;: 5051
    }
  }
}
&lt;more events&gt;
</code></pre>
<p>NOTE: Once an executor is launched, the agent waits for a duration of <code>--executor_registration_timeout</code> (configurable at agent startup) for the executor to subscribe. If the executor fails to subscribe within this duration, the agent forcefully destroys the container executor is running in.</p>
<h3 id="update-2"><a class="header" href="#update-2">UPDATE</a></h3>
<p>Sent by the executor to reliably communicate the state of managed tasks. It is crucial that a terminal update (e.g., <code>TASK_FINISHED</code>, <code>TASK_KILLED</code> or <code>TASK_FAILED</code>) is sent to the agent as soon as the task terminates, in order to allow Mesos to release the resources allocated to the task.</p>
<p>The scheduler must explicitly respond to this call through an <code>ACKNOWLEDGE</code> message (see <code>ACKNOWLEDGED</code> in the Events section below for the semantics). The executor must maintain a list of unacknowledged updates. If for some reason, the executor is disconnected from the agent, these updates must be sent as part of <code>SUBSCRIBE</code> request in the <code>unacknowledged_updates</code> field.</p>
<pre><code>UPDATE Request (JSON):

POST /api/v1/executor  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json
Accept: application/json

{
  &quot;executor_id&quot;: {
    &quot;value&quot;: &quot;387aa966-8fc5-4428-a794-5a868a60d3eb&quot;
  },
  &quot;framework_id&quot;: {
    &quot;value&quot;: &quot;9aaa9d0d-e00d-444f-bfbd-23dd197939a0-0000&quot;
  },
  &quot;type&quot;: &quot;UPDATE&quot;,
  &quot;update&quot;: {
    &quot;status&quot;: {
      &quot;executor_id&quot;: {
        &quot;value&quot;: &quot;387aa966-8fc5-4428-a794-5a868a60d3eb&quot;
      },
      &quot;source&quot;: &quot;SOURCE_EXECUTOR&quot;,
      &quot;state&quot;: &quot;TASK_RUNNING&quot;,
      &quot;task_id&quot;: {
        &quot;value&quot;: &quot;66724cec-2609-4fa0-8d93-c5fb2099d0f8&quot;
      },
      &quot;uuid&quot;: &quot;ZDQwZjNmM2UtYmJlMy00NGFmLWEyMzAtNGNiMWVhZTcyZjY3Cg==&quot;
    }
  }
}

UPDATE Response:
HTTP/1.1 202 Accepted
</code></pre>
<h3 id="message-2"><a class="header" href="#message-2">MESSAGE</a></h3>
<p>Sent by the executor to send arbitrary binary data to the scheduler. Note that Mesos neither interprets this data nor makes any guarantees about the delivery of this message to the scheduler. The <code>data</code> field is raw bytes encoded in Base64.</p>
<pre><code>MESSAGE Request (JSON):

POST /api/v1/executor  HTTP/1.1

Host: agenthost:5051
Content-Type: application/json
Accept: application/json

{
  &quot;executor_id&quot;: {
    &quot;value&quot;: &quot;387aa966-8fc5-4428-a794-5a868a60d3eb&quot;
  },
  &quot;framework_id&quot;: {
    &quot;value&quot;: &quot;9aaa9d0d-e00d-444f-bfbd-23dd197939a0-0000&quot;
  },
  &quot;type&quot;: &quot;MESSAGE&quot;,
  &quot;message&quot;: {
    &quot;data&quot;: &quot;t+Wonz5fRFKMzCnEptlv5A==&quot;
  }
}

MESSAGE Response:
HTTP/1.1 202 Accepted
</code></pre>
<h2 id="events-1"><a class="header" href="#events-1">Events</a></h2>
<p>Executors are expected to keep a <strong>persistent</strong> connection to the &quot;/executor&quot; endpoint (even after getting a <code>SUBSCRIBED</code> HTTP Response event). This is indicated by the &quot;Connection: keep-alive&quot; and &quot;Transfer-Encoding: chunked&quot; headers with <em>no</em> &quot;Content-Length&quot; header set. All subsequent events that are relevant to this executor generated by Mesos are streamed on this connection. The agent encodes each Event in <a href="scheduler-http-api.html#recordio-response-format">RecordIO</a> format, i.e., string representation of length of the event in bytes followed by JSON or binary Protobuf  (possibly compressed) encoded event. The length of an event is a 64-bit unsigned integer (encoded as a textual value) and will never be &quot;0&quot;. Also, note that the <code>RecordIO</code> encoding should be decoded by the executor whereas the underlying HTTP chunked encoding is typically invisible at the application (executor) layer. The type of content encoding used for the events will be determined by the accept header of the POST request (e.g., &quot;Accept: application/json&quot;).</p>
<p>The following events are currently sent by the agent. The canonical source of this information is at <a href="https://github.com/apache/mesos/blob/master/include/mesos/v1/executor/executor.proto">executor.proto</a>. Note that when sending JSON-encoded events, agent encodes raw bytes in Base64 and strings in UTF-8.</p>
<h3 id="subscribed-1"><a class="header" href="#subscribed-1">SUBSCRIBED</a></h3>
<p>The first event sent by the agent when the executor sends a <code>SUBSCRIBE</code> request on the persistent connection. See <code>SUBSCRIBE</code> in Calls section for the format.</p>
<h3 id="launch"><a class="header" href="#launch">LAUNCH</a></h3>
<p>Sent by the agent whenever it needs to assign a new task to the executor. The executor is required to send an <code>UPDATE</code> message back to the agent indicating the success or failure of the task initialization.</p>
<p>The executor must maintain a list of unacknowledged tasks (see <code>SUBSCRIBE</code> in <code>Calls</code> section). If for some reason, the executor is disconnected from the agent, these tasks must be sent as part of <code>SUBSCRIBE</code> request in the <code>tasks</code> field.</p>
<pre><code>LAUNCH Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;: &quot;LAUNCH&quot;,
  &quot;launch&quot;: {
    &quot;framework_info&quot;: {
      &quot;id&quot;: {
        &quot;value&quot;: &quot;49154f1b-8cf6-4421-bf13-8bd11dccd1f1&quot;
      },
      &quot;user&quot;: &quot;foo&quot;,
      &quot;name&quot;: &quot;my_framework&quot;
    },
    &quot;task&quot;: {
      &quot;name&quot;: &quot;dummy-task&quot;,
      &quot;task_id&quot;: {
        &quot;value&quot;: &quot;d40f3f3e-bbe3-44af-a230-4cb1eae72f67&quot;
      },
      &quot;agent_id&quot;: {
        &quot;value&quot;: &quot;f1c9cdc5-195e-41a7-a0d7-adaa9af07f81&quot;
      },
      &quot;command&quot;: {
        &quot;value&quot;: &quot;sleep&quot;,
        &quot;arguments&quot;: [
          &quot;100&quot;
        ]
      }
    }
  }
}
</code></pre>
<h3 id="launch_group"><a class="header" href="#launch_group">LAUNCH_GROUP</a></h3>
<p>This <strong>experimental</strong> event was added in 1.1.0.</p>
<p>Sent by the agent whenever it needs to assign a new task group to the executor. The executor is required to send <code>UPDATE</code> messages back to the agent indicating the success or failure of each of the tasks in the group.</p>
<p>The executor must maintain a list of unacknowledged tasks (see <code>LAUNCH</code> section above).</p>
<pre><code>LAUNCH_GROUP Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot;: &quot;LAUNCH_GROUP&quot;,
  &quot;launch_group&quot;: {
    &quot;task_group&quot; : {
      &quot;tasks&quot; : [
        {
          &quot;name&quot;: &quot;dummy-task&quot;,
          &quot;task_id&quot;: {
            &quot;value&quot;: &quot;d40f3f3e-bbe3-44af-a230-4cb1eae72f67&quot;
          },
          &quot;agent_id&quot;: {
            &quot;value&quot;: &quot;f1c9cdc5-195e-41a7-a0d7-adaa9af07f81&quot;
          },
          &quot;command&quot;: {
            &quot;value&quot;: &quot;sleep&quot;,
            &quot;arguments&quot;: [
              &quot;100&quot;
            ]
          }
        }
      ]
    }
  }
}
</code></pre>
<h3 id="kill-1"><a class="header" href="#kill-1">KILL</a></h3>
<p>The <code>KILL</code> event is sent whenever the scheduler needs to stop execution of a specific task. The executor is required to send a terminal update (e.g., <code>TASK_FINISHED</code>, <code>TASK_KILLED</code> or <code>TASK_FAILED</code>) back to the agent once it has stopped/killed the task. Mesos will mark the task resources as freed once the terminal update is received.</p>
<pre><code>LAUNCH Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot; : &quot;KILL&quot;,
  &quot;kill&quot; : {
    &quot;task_id&quot; : {&quot;value&quot; : &quot;d40f3f3e-bbe3-44af-a230-4cb1eae72f67&quot;}
  }
}
</code></pre>
<h3 id="acknowledged"><a class="header" href="#acknowledged">ACKNOWLEDGED</a></h3>
<p>Sent by the agent in order to signal the executor that a status update was received as part of the reliable message passing mechanism. Acknowledged updates must not be retried.</p>
<pre><code>ACKNOWLEDGED Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot; : &quot;ACKNOWLEDGED&quot;,
  &quot;acknowledged&quot; : {
    &quot;task_id&quot; : {&quot;value&quot; : &quot;d40f3f3e-bbe3-44af-a230-4cb1eae72f67&quot;},
    &quot;uuid&quot; : &quot;ZDQwZjNmM2UtYmJlMy00NGFmLWEyMzAtNGNiMWVhZTcyZjY3Cg==&quot;
  }
}
</code></pre>
<h3 id="message-3"><a class="header" href="#message-3">MESSAGE</a></h3>
<p>Custom message generated by the scheduler and forwarded all the way to the executor. These messages are delivered &quot;as-is&quot; by Mesos and have no delivery guarantees. It is up to the scheduler to retry if a message is dropped for any reason. The <code>data</code> field contains raw bytes encoded as Base64.</p>
<pre><code>MESSAGE Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot; : &quot;MESSAGE&quot;,
  &quot;message&quot; : {
    &quot;data&quot; : &quot;c2FtcGxlIGRhdGE=&quot;
  }
}
</code></pre>
<h3 id="shutdown-1"><a class="header" href="#shutdown-1">SHUTDOWN</a></h3>
<p>Sent by the agent in order to shutdown the executor. Once an executor gets a <code>SHUTDOWN</code> event it is required to kill all its tasks, send <code>TASK_KILLED</code> updates and gracefully exit. If an executor doesn't terminate within a certain period <code>MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD</code> (an environment variable set by the agent upon executor startup), the agent will forcefully destroy the container where the executor is running. The agent would then send <code>TASK_LOST</code> updates for any remaining active tasks of this executor.</p>
<pre><code>SHUTDOWN Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot; : &quot;SHUTDOWN&quot;
}
</code></pre>
<h3 id="error-1"><a class="header" href="#error-1">ERROR</a></h3>
<p>Sent by the agent when an asynchronous error event is generated. It is recommended that the executor abort when it receives an error event and retry subscription.</p>
<pre><code>ERROR Event (JSON)

&lt;event-length&gt;
{
  &quot;type&quot; : &quot;ERROR&quot;,
  &quot;error&quot; : {
    &quot;message&quot; : &quot;Unrecoverable error&quot;
  }
}
</code></pre>
<h2 id="executor-environment-variables"><a class="header" href="#executor-environment-variables">Executor Environment Variables</a></h2>
<p>The following environment variables are set by the agent that can be used by the executor upon startup:</p>
<ul>
<li><code>MESOS_FRAMEWORK_ID</code>: <code>FrameworkID</code> of the scheduler needed as part of the <code>SUBSCRIBE</code> call.</li>
<li><code>MESOS_EXECUTOR_ID</code>: <code>ExecutorID</code> of the executor needed as part of the <code>SUBSCRIBE</code> call.</li>
<li><code>MESOS_DIRECTORY</code>: Path to the working directory for the executor on the host filesystem (deprecated).</li>
<li><code>MESOS_SANDBOX</code>: Path to the mapped sandbox inside of the container (determined by the agent flag <code>sandbox_directory</code>) for either mesos container with image or docker container. For the case of command task without image specified, it is the path to the sandbox on the host filesystem, which is identical to <code>MESOS_DIRECTORY</code>. <code>MESOS_DIRECTORY</code> is always the sandbox on the host filesystem.</li>
<li><code>MESOS_AGENT_ENDPOINT</code>: Agent endpoint (i.e., ip:port to be used by the executor to connect to the agent).</li>
<li><code>MESOS_CHECKPOINT</code>: If set to true, denotes that framework has checkpointing enabled.</li>
<li><code>MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD</code>: Amount of time the agent would wait for an executor to shut down (e.g., 60secs, 3mins etc.) after sending a <code>SHUTDOWN</code> event.</li>
<li><code>MESOS_EXECUTOR_AUTHENTICATION_TOKEN</code>: The token the executor should use to authenticate with the agent. When executor authentication is enabled, the agent generates a JSON web token (JWT) that the executor can use to authenticate with the agent's default JWT authenticator.</li>
</ul>
<p>If <code>MESOS_CHECKPOINT</code> is set (i.e., if framework checkpointing is enabled), the following additional variables are also set that can be used by the executor for retrying upon a disconnection with the agent:</p>
<ul>
<li><code>MESOS_RECOVERY_TIMEOUT</code>: The total duration that the executor should spend retrying before shutting itself down when it is disconnected from the agent (e.g., <code>15mins</code>, <code>5secs</code> etc.). This is configurable at agent startup via the flag <code>--recovery_timeout</code>.</li>
<li><code>MESOS_SUBSCRIPTION_BACKOFF_MAX</code>: The maximum backoff duration to be used by the executor between two retries when disconnected (e.g., <code>250ms</code>, <code>1mins</code> etc.). This is configurable at agent startup via the flag <code>--executor_reregistration_timeout</code>.</li>
</ul>
<p>NOTE: Additionally, the executor also inherits all the agent's environment variables.</p>
<p><a name="disconnections"></a></p>
<h2 id="disconnections-1"><a class="header" href="#disconnections-1">Disconnections</a></h2>
<p>An executor considers itself disconnected if the persistent subscription connection (opened via SUBSCRIBE request) to &quot;/executor&quot; breaks. The disconnection can happen due to an agent process failure etc.</p>
<p>Upon detecting a disconnection from the agent, the retry behavior depends on whether framework checkpointing is enabled:</p>
<ul>
<li>If framework checkpointing is disabled, the executor is not supposed to retry subscription and gracefully exit.</li>
<li>If framework checkpointing is enabled, the executor is supposed to retry subscription using a suitable <a href="executor-http-api.html#backoff-strategies">backoff strategy</a> for a duration of <code>MESOS_RECOVERY_TIMEOUT</code>. If it is not able to establish a subscription with the agent within this duration, it should gracefully exit.</li>
</ul>
<h2 id="agent-recovery-1"><a class="header" href="#agent-recovery-1">Agent Recovery</a></h2>
<p>Upon agent startup, an agent performs <a href="agent-recovery.html">recovery</a>. This allows the agent to recover status updates and reconnect with old executors. Currently, the agent supports the following recovery mechanisms specified via the <code>--recover</code> flag:</p>
<ul>
<li><strong>reconnect</strong> (default): This mode allows the agent to reconnect with any of it's old live executors provided the framework has enabled checkpointing. The recovery of the agent is only marked complete once all the disconnected executors have connected and hung executors have been destroyed. Hence, it is mandatory that every executor retries at least once within the interval (<code>MESOS_SUBSCRIPTION_BACKOFF_MAX</code>) to ensure it is not shutdown by the agent due to being hung/unresponsive.</li>
<li><strong>cleanup</strong>: This mode kills any old live executors and then exits the agent. This is usually done by operators when making a non-compatible agent/executor upgrade. Upon receiving a <code>SUBSCRIBE</code> request from the executor of a framework with checkpointing enabled, the agent would send it a <code>SHUTDOWN</code> event as soon as it reconnects. For hung executors, the agent would wait for a duration of <code>--executor_shutdown_grace_period</code> (configurable at agent startup) and then forcefully kill the container where the executor is running in.</li>
</ul>
<p><a name="backoff-strategies"></a></p>
<h2 id="backoff-strategies"><a class="header" href="#backoff-strategies">Backoff Strategies</a></h2>
<p>Executors are encouraged to retry subscription using a suitable backoff strategy like linear backoff, when they notice a disconnection with the agent. A disconnection typically happens when the agent process terminates (e.g., restarted for an upgrade). Each retry interval should be bounded by the value of <code>MESOS_SUBSCRIPTION_BACKOFF_MAX</code> which is set as an environment variable.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
